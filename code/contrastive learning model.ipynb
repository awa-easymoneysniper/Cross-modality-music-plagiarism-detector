{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18929,"status":"ok","timestamp":1754443633238,"user":{"displayName":"Bethany","userId":"07029141995136804205"},"user_tz":-480},"id":"jan_yAgelVPr","outputId":"60ec3dda-d521-46f1-81aa-62bd3120f380"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":566},"executionInfo":{"elapsed":16107,"status":"ok","timestamp":1754378620309,"user":{"displayName":"Bethany","userId":"07029141995136804205"},"user_tz":-480},"id":"s9IRtfXKHQCn","outputId":"3ed58898-f02f-4686-867b-a14a2c10b15c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n","Collecting sympy\n","  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy) (1.3.0)\n","Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/6.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/6.3 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m113.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sympy\n","  Attempting uninstall: sympy\n","    Found existing installation: sympy 1.13.1\n","    Uninstalling sympy-1.13.1:\n","      Successfully uninstalled sympy-1.13.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n","torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n","torch 2.6.0+cu124 requires sympy==1.13.1; python_version >= \"3.9\", but you have sympy 1.14.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed sympy-1.14.0\n"]},{"data":{"application/vnd.colab-display-data+json":{"id":"9a355aa1582a40169b0e7dc4124941cb","pip_warning":{"packages":["sympy"]}}},"metadata":{},"output_type":"display_data"}],"source":["!pip install -U sympy"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import pandas as pd\n","import pickle\n","import os\n","import random\n","from torch.utils.data import Dataset, DataLoader\n","from typing import Dict, List, Tuple, Optional\n","import json\n","from pathlib import Path\n","from collections import defaultdict\n","import re\n","from sklearn.metrics.pairwise import cosine_similarity\n","import joblib\n","\n","class MusicContrastiveDataset(Dataset):\n","    \"\"\"\n","    Ultra-fast dataset for contrastive learning with lazy validation and caching\n","    \"\"\"\n","    def __init__(self,\n","                 symbolic_dir: str,\n","                 audio_dir: str,\n","                 metadata_csv: str,\n","                 negative_ratio: float = 0.6,\n","                 hard_negative_ratio: float = 0.4,\n","                 cache_file: str = \"file_validation_cache.pkl\"):\n","\n","        self.symbolic_dir = Path(symbolic_dir)\n","        self.audio_dir = Path(audio_dir)\n","        self.metadata_csv = metadata_csv\n","        self.negative_ratio = negative_ratio\n","        self.hard_negative_ratio = hard_negative_ratio\n","        self.cache_file = cache_file\n","\n","        # Load or create validation cache\n","        self.validation_cache = self._load_or_create_cache()\n","\n","        # Load metadata first\n","        self.metadata_df = self._load_metadata_csv()\n","\n","        # Fast file loading (no validation yet)\n","        self.symbolic_files = self._load_file_list_fast(self.symbolic_dir, \"symbolic\")\n","        self.audio_files = self._load_file_list_fast(self.audio_dir, \"audio\")\n","\n","        # Create enhanced mappings with metadata\n","        self.segment_mapping = self._create_enhanced_segment_mapping()\n","        self.pairs = self._create_strategic_pairs()\n","\n","    def _load_or_create_cache(self) -> Dict:\n","        \"\"\"Load existing validation cache or create empty one\"\"\"\n","        cache_path = str(self.symbolic_dir.parent / self.cache_file)\n","        if os.path.exists(cache_path):\n","            try:\n","                with open(cache_path, 'rb') as f:\n","                    cache = pickle.load(f)\n","                print(f\"Loaded validation cache with {len(cache)} entries\")\n","                return cache\n","            except Exception as e:\n","                print(f\"Error loading cache: {e}\")\n","\n","        print(\"Creating new validation cache\")\n","        return {}\n","\n","    def _save_cache(self):\n","        \"\"\"Save validation cache\"\"\"\n","        try:\n","            cache_path = str(self.symbolic_dir.parent / self.cache_file)\n","            with open(cache_path, 'wb') as f:\n","                pickle.dump(self.validation_cache, f)\n","            print(f\"Saved validation cache with {len(self.validation_cache)} entries\")\n","        except Exception as e:\n","            print(f\"Error saving cache: {e}\")\n","\n","    def _quick_file_check(self, file_path: str, expected_dim: int) -> Tuple[bool, Optional[int], str]:\n","        \"\"\"\n","        Ultra-fast file validation using multiple heuristics\n","        Returns: (is_valid, dimension, reason)\n","        \"\"\"\n","        # Check cache first\n","        if file_path in self.validation_cache:\n","            cached = self.validation_cache[file_path]\n","            return cached['valid'], cached['dim'], cached['reason']\n","\n","        # Fast checks without loading file content\n","        try:\n","            # Check 1: File exists and has reasonable size\n","            if not os.path.exists(file_path):\n","                result = (False, None, \"file_not_found\")\n","                self.validation_cache[file_path] = {'valid': False, 'dim': None, 'reason': \"file_not_found\"}\n","                return result\n","\n","            file_size = os.path.getsize(file_path)\n","\n","            # Check 2: File size heuristics\n","            if file_size < 100:  # Too small to contain meaningful data\n","                result = (False, None, \"file_too_small\")\n","                self.validation_cache[file_path] = {'valid': False, 'dim': None, 'reason': \"file_too_small\"}\n","                return result\n","\n","            # Check 3: File size consistency with expected dimensions\n","            if expected_dim:\n","                # Rough size estimation: float32 (4 bytes) * expected_dim + numpy overhead\n","                min_expected_size = expected_dim * 4 + 100  # +100 for numpy header\n","                max_expected_size = expected_dim * 4 * 100   # Allow up to 100 time steps\n","\n","                if file_size < min_expected_size:\n","                    result = (False, expected_dim, \"size_too_small_for_dimension\")\n","                    self.validation_cache[file_path] = {'valid': False, 'dim': expected_dim, 'reason': \"size_too_small_for_dimension\"}\n","                    return result\n","\n","            # If all fast checks pass, assume valid (validate during actual loading)\n","            result = (True, expected_dim, \"passed_fast_checks\")\n","            self.validation_cache[file_path] = {'valid': True, 'dim': expected_dim, 'reason': \"passed_fast_checks\"}\n","            return result\n","\n","        except Exception as e:\n","            result = (False, None, f\"error: {str(e)}\")\n","            self.validation_cache[file_path] = {'valid': False, 'dim': None, 'reason': f\"error: {str(e)}\"}\n","            return result\n","    def _load_metadata_csv(self) -> pd.DataFrame:\n","        \"\"\"Load the MusicNet metadata CSV file\"\"\"\n","        try:\n","            df = pd.read_csv(self.metadata_csv)\n","            print(f\"Loaded metadata for {len(df)} pieces\")\n","            print(f\"Columns: {list(df.columns)}\")\n","\n","            # Group by composer for strategic pairing\n","            self.composer_groups = df.groupby('composer')['id'].apply(list).to_dict()\n","            print(f\"Found {len(self.composer_groups)} composers\")\n","\n","            # Create era mapping based on composer (simplified)\n","            self.era_mapping = self._create_era_mapping(df)\n","\n","            return df\n","        except Exception as e:\n","            print(f\"Error loading metadata CSV: {e}\")\n","            return pd.DataFrame()\n","\n","    def _create_era_mapping(self, df: pd.DataFrame) -> Dict:\n","        \"\"\"Create era mapping for composers based on actual dataset\"\"\"\n","        era_mapping = {}\n","\n","        # Based on the actual composers in your dataset\n","        baroque_composers = ['Bach']\n","        classical_composers = ['Mozart', 'Haydn', 'Beethoven']\n","        romantic_composers = ['Brahms', 'Schubert', 'Dvorak']\n","        impressionist_composers = ['Ravel', 'Faure']\n","        modern_composers = ['Cambini']  # 18th-19th century\n","\n","        for composer in df['composer'].unique():\n","            composer_str = str(composer).lower()\n","            if any(name.lower() in composer_str for name in baroque_composers):\n","                era_mapping[composer] = 'Baroque'\n","            elif any(name.lower() in composer_str for name in classical_composers):\n","                era_mapping[composer] = 'Classical'\n","            elif any(name.lower() in composer_str for name in romantic_composers):\n","                era_mapping[composer] = 'Romantic'\n","            elif any(name.lower() in composer_str for name in impressionist_composers):\n","                era_mapping[composer] = 'Impressionist'\n","            elif any(name.lower() in composer_str for name in modern_composers):\n","                era_mapping[composer] = 'Classical-Modern'\n","            else:\n","                era_mapping[composer] = 'Other'\n","\n","        return era_mapping\n","\n","    def _load_file_list_fast(self, directory: Path, file_type: str) -> Dict[str, List[str]]:\n","        \"\"\"Ultra-fast file list loading with lazy validation\"\"\"\n","        files_by_id = {}\n","        expected_dim = 768 if file_type == \"symbolic\" else 6144\n","\n","        print(f\"Fast-loading {file_type} files from: {directory}\")\n","\n","        npy_files = list(directory.glob(\"*.npy\"))\n","        print(f\"Found {len(npy_files)} files\")\n","\n","        valid_count = 0\n","        invalid_count = 0\n","\n","        for file_path in npy_files:\n","            # Quick validation without loading file content\n","            is_valid, actual_dim, reason = self._quick_file_check(str(file_path), expected_dim)\n","\n","            if not is_valid:\n","                invalid_count += 1\n","                if invalid_count <= 5:  # Only show first few invalid files\n","                    print(f\"Skipping invalid {file_type} file {file_path.name}: {reason}\")\n","                continue\n","\n","            # Extract ID from filename\n","            filename = file_path.stem\n","            if '_' in filename:\n","                parts = filename.split('_')\n","                for part in parts:\n","                    if part.isdigit():\n","                        data_id = part\n","                        break\n","                else:\n","                    data_id = parts[0]\n","            else:\n","                numbers = re.findall(r'\\d+', filename)\n","                data_id = numbers[0] if numbers else filename\n","\n","            if data_id not in files_by_id:\n","                files_by_id[data_id] = []\n","            files_by_id[data_id].append(str(file_path))\n","            valid_count += 1\n","\n","            # Save cache periodically to avoid losing progress\n","            if (valid_count + invalid_count) % 1000 == 0:\n","                self._save_cache()\n","                print(f\"  Processed {valid_count + invalid_count} files...\")\n","\n","        # Save final cache\n","        self._save_cache()\n","\n","        print(f\"Fast validation complete: {valid_count} valid, {invalid_count} invalid\")\n","        print(f\"Organized into {len(files_by_id)} ID groups\")\n","        return files_by_id\n","\n","    def _create_enhanced_segment_mapping(self) -> Dict:\n","        \"\"\"Create enhanced mapping with metadata information\"\"\"\n","        mapping = {}\n","\n","        # Find common IDs between symbolic, audio, and metadata\n","        metadata_ids = set(str(id_) for id_ in self.metadata_df['id'].tolist())\n","        symbolic_ids = set(self.symbolic_files.keys())\n","        audio_ids = set(self.audio_files.keys())\n","\n","        common_ids = metadata_ids & symbolic_ids & audio_ids\n","        print(f\"Common IDs across all sources: {len(common_ids)}\")\n","\n","        if len(common_ids) == 0:\n","            print(\"ERROR: No common IDs found between symbolic, audio, and metadata!\")\n","            print(f\"Metadata IDs sample: {list(metadata_ids)[:5]}\")\n","            print(f\"Symbolic IDs sample: {list(symbolic_ids)[:5]}\")\n","            print(f\"Audio IDs sample: {list(audio_ids)[:5]}\")\n","\n","        for data_id in common_ids:\n","            # Get metadata for this piece\n","            metadata_row = self.metadata_df[self.metadata_df['id'] == int(data_id)].iloc[0]\n","\n","            mapping[data_id] = {\n","                'symbolic': self.symbolic_files[data_id],\n","                'audio': self.audio_files[data_id],\n","                'composer': metadata_row['composer'],\n","                'composition': metadata_row['composition'],\n","                'movement': metadata_row.get('movement', ''),\n","                'ensemble': metadata_row.get('ensemble', ''),\n","                'seconds': metadata_row.get('seconds', 0),\n","                'era': self.era_mapping.get(metadata_row['composer'], 'Other')\n","            }\n","\n","        print(f\"Created enhanced mapping for {len(mapping)} pieces\")\n","        return mapping\n","\n","    def _create_strategic_pairs(self) -> List[Dict]:\n","        \"\"\"Create strategic positive and negative pairs using metadata\"\"\"\n","        pairs = []\n","        data_ids = list(self.segment_mapping.keys())\n","\n","        if len(data_ids) == 0:\n","            print(\"ERROR: No data IDs available for creating pairs!\")\n","            return pairs\n","\n","        print(f\"Creating strategic pairs from {len(data_ids)} pieces...\")\n","\n","        # Print composer and era distribution for debugging\n","        composer_counts = defaultdict(int)\n","        era_counts = defaultdict(int)\n","        for data_id in data_ids:\n","            composer_counts[self.segment_mapping[data_id]['composer']] += 1\n","            era_counts[self.segment_mapping[data_id]['era']] += 1\n","\n","        print(\"Composer distribution:\", dict(composer_counts))\n","        print(\"Era distribution:\", dict(era_counts))\n","\n","        # 1. CREATE POSITIVE PAIRS\n","        positive_pairs = self._create_positive_pairs(data_ids)\n","        pairs.extend(positive_pairs)\n","\n","        # 2. CREATE NEGATIVE PAIRS\n","        negative_pairs = self._create_negative_pairs(data_ids, len(positive_pairs))\n","        pairs.extend(negative_pairs)\n","\n","        print(f\"Created {len(pairs)} total pairs:\")\n","        print(f\"  - Positive: {len(positive_pairs)}\")\n","        print(f\"  - Negative: {len(negative_pairs)}\")\n","\n","        return pairs\n","\n","    def _create_positive_pairs(self, data_ids: List[str]) -> List[Dict]:\n","        \"\"\"Create strategic positive pairs\"\"\"\n","        positive_pairs = []\n","\n","        for data_id in data_ids:\n","            segments = self.segment_mapping[data_id]\n","            symbolic_segments = segments['symbolic']\n","            audio_segments = segments['audio']\n","\n","            # Type 1: Direct matches (same segment)\n","            min_segments = min(len(symbolic_segments), len(audio_segments))\n","            for i in range(min_segments):\n","                positive_pairs.append({\n","                    'symbolic_path': symbolic_segments[i],\n","                    'audio_path': audio_segments[i],\n","                    'label': 1,\n","                    'data_id': data_id,\n","                    'pair_type': 'positive_direct',\n","                    'composer': segments['composer'],\n","                    'composition': segments['composition']\n","                })\n","\n","            # Type 2: Overlapping segments (same recording, different segments)\n","            if min_segments > 1:\n","                for i in range(min_segments - 1):\n","                    for j in range(i + 1, min(i + 3, min_segments)):  # Nearby segments\n","                        positive_pairs.append({\n","                            'symbolic_path': symbolic_segments[i],\n","                            'audio_path': audio_segments[j],\n","                            'label': 1,\n","                            'data_id': data_id,\n","                            'pair_type': 'positive_nearby',\n","                            'composer': segments['composer'],\n","                            'composition': segments['composition']\n","                        })\n","\n","        print(f\"Created {len(positive_pairs)} positive pairs\")\n","        return positive_pairs\n","\n","    def _create_negative_pairs(self, data_ids: List[str], num_positives: int) -> List[Dict]:\n","        \"\"\"Create strategic negative pairs (hard negatives) based on actual dataset composers\"\"\"\n","        negative_pairs = []\n","        num_negatives = int(num_positives * self.negative_ratio / (1 - self.negative_ratio))\n","\n","        # Calculate distribution of negative types\n","        hard_negatives = int(num_negatives * self.hard_negative_ratio)\n","        medium_negatives = int(num_negatives * 0.3)\n","        easy_negatives = num_negatives - hard_negatives - medium_negatives\n","\n","        print(f\"Creating {num_negatives} negative pairs:\")\n","        print(f\"  - Hard (same composer): {hard_negatives}\")\n","        print(f\"  - Medium (same era): {medium_negatives}\")\n","        print(f\"  - Easy (different era): {easy_negatives}\")\n","\n","        # Group pieces by composer and era\n","        composer_groups = defaultdict(list)\n","        era_groups = defaultdict(list)\n","\n","        for data_id in data_ids:\n","            composer = self.segment_mapping[data_id]['composer']\n","            era = self.segment_mapping[data_id]['era']\n","            composer_groups[composer].append(data_id)\n","            era_groups[era].append(data_id)\n","\n","        # Type 1: Hard negatives - Same composer, different compositions\n","        hard_created = 0\n","        for composer, pieces in composer_groups.items():\n","            if len(pieces) > 1 and hard_created < hard_negatives:\n","                for _ in range(min(len(pieces) // 2, hard_negatives - hard_created)):\n","                    if len(pieces) >= 2:\n","                        id1, id2 = random.sample(pieces, 2)\n","                        # Ensure different compositions\n","                        if (self.segment_mapping[id1]['composition'] !=\n","                            self.segment_mapping[id2]['composition']):\n","                            negative_pairs.append(self._create_negative_pair(id1, id2, 'hard_same_composer'))\n","                            hard_created += 1\n","\n","        # Type 2: Medium negatives - Same era, different composers\n","        medium_created = 0\n","        for era, pieces in era_groups.items():\n","            if len(pieces) > 1 and medium_created < medium_negatives:\n","                era_composers = set(self.segment_mapping[p]['composer'] for p in pieces)\n","                if len(era_composers) > 1:\n","                    for _ in range(min(len(pieces) // 3, medium_negatives - medium_created)):\n","                        id1, id2 = random.sample(pieces, 2)\n","                        if self.segment_mapping[id1]['composer'] != self.segment_mapping[id2]['composer']:\n","                            negative_pairs.append(self._create_negative_pair(id1, id2, 'medium_same_era'))\n","                            medium_created += 1\n","\n","        # Type 3: Easy negatives - Different composers, different eras\n","        different_era_pairs = []\n","        for i, id1 in enumerate(data_ids):\n","            for id2 in data_ids[i+1:]:\n","                if (self.segment_mapping[id1]['composer'] != self.segment_mapping[id2]['composer'] and\n","                    self.segment_mapping[id1]['era'] != self.segment_mapping[id2]['era']):\n","                    different_era_pairs.append((id1, id2))\n","\n","        for _ in range(min(easy_negatives, len(different_era_pairs))):\n","            id1, id2 = random.choice(different_era_pairs)\n","            negative_pairs.append(self._create_negative_pair(id1, id2, 'easy_different_era'))\n","\n","        print(f\"Actually created: {hard_created} hard, {medium_created} medium, {easy_negatives} easy\")\n","        return negative_pairs\n","\n","    def _create_negative_pair(self, id1: str, id2: str, pair_subtype: str) -> Dict:\n","        \"\"\"Helper to create a negative pair\"\"\"\n","        symbolic_seg = random.choice(self.segment_mapping[id1]['symbolic'])\n","        audio_seg = random.choice(self.segment_mapping[id2]['audio'])\n","\n","        return {\n","            'symbolic_path': symbolic_seg,\n","            'audio_path': audio_seg,\n","            'label': 0,\n","            'data_id_symbolic': id1,\n","            'data_id_audio': id2,\n","            'pair_type': f'negative_{pair_subtype}',\n","            'composer_symbolic': self.segment_mapping[id1]['composer'],\n","            'composer_audio': self.segment_mapping[id2]['composer']\n","        }\n","\n","    def _load_features(self, file_path: str, expected_dim: int) -> torch.Tensor:\n","        \"\"\"\n","        Lazy feature loading with caching of problematic files\n","        Only validates dimension on first load, then caches the result\n","        \"\"\"\n","        try:\n","            # Check if this file was marked as invalid in cache\n","            if file_path in self.validation_cache:\n","                cached = self.validation_cache[file_path]\n","                if not cached['valid']:\n","                    # Return zero tensor for invalid files\n","                    return torch.zeros(1, expected_dim)\n","\n","            features = np.load(file_path)\n","\n","            # Handle different numpy formats\n","            if features.dtype == object:\n","                if features.shape == ():\n","                    data = features.item()\n","                    if isinstance(data, dict):\n","                        if 'features' in data:\n","                            features = data['features']\n","                        elif 'embeddings' in data:\n","                            features = data['embeddings']\n","                        else:\n","                            for key, value in data.items():\n","                                if isinstance(value, (np.ndarray, list)):\n","                                    features = value\n","                                    break\n","                    else:\n","                        features = data\n","                else:\n","                    features = features[0]\n","\n","            if isinstance(features, list):\n","                features = np.array(features)\n","\n","            if not isinstance(features, np.ndarray):\n","                features = np.array(features)\n","\n","            # Check for empty arrays\n","            if features.size == 0:\n","                # Mark as invalid in cache\n","                self.validation_cache[file_path] = {'valid': False, 'dim': None, 'reason': 'empty_array'}\n","                return torch.zeros(1, expected_dim)\n","\n","            features = torch.tensor(features, dtype=torch.float32)\n","\n","            if features.dim() == 1:\n","                features = features.unsqueeze(0)\n","\n","            if features.dim() > 2:\n","                features = features.view(features.size(0), -1)\n","\n","            # Validate dimensions and fix if needed\n","            actual_dim = features.shape[-1]\n","            if actual_dim != expected_dim:\n","                if actual_dim < expected_dim:\n","                    # Pad with zeros\n","                    padding = torch.zeros(features.shape[0], expected_dim - actual_dim)\n","                    features = torch.cat([features, padding], dim=1)\n","                else:\n","                    # Truncate\n","                    features = features[:, :expected_dim]\n","\n","                # Update cache with corrected info\n","                self.validation_cache[file_path] = {'valid': True, 'dim': expected_dim, 'reason': 'dimension_corrected'}\n","\n","            return features\n","\n","        except Exception as e:\n","            # Mark as invalid in cache\n","            self.validation_cache[file_path] = {'valid': False, 'dim': None, 'reason': f'load_error: {str(e)}'}\n","            print(f\"Error loading {file_path}: {e}\")\n","            return torch.zeros(1, expected_dim)\n","\n","    def __len__(self):\n","        return len(self.pairs)\n","\n","    def __getitem__(self, idx):\n","        pair = self.pairs[idx]\n","\n","        # Load features with proper dimension expectations\n","        symbolic_features = self._load_features(pair['symbolic_path'], expected_dim=768)\n","        audio_features = self._load_features(pair['audio_path'], expected_dim=6144)\n","        label = torch.tensor(pair['label'], dtype=torch.float32)\n","\n","        return {\n","            'symbolic': symbolic_features,\n","            'audio': audio_features,\n","            'label': label,\n","            'pair_type': pair['pair_type']\n","        }\n","\n","\n","# Custom collate function to handle any remaining inconsistencies\n","def custom_collate_fn(batch):\n","    \"\"\"Custom collate function that ensures consistent tensor shapes\"\"\"\n","    try:\n","        # Separate the components\n","        symbolic_list = []\n","        audio_list = []\n","        labels = []\n","        pair_types = []\n","\n","        for item in batch:\n","            symbolic_list.append(item['symbolic'])\n","            audio_list.append(item['audio'])\n","            labels.append(item['label'])\n","            pair_types.append(item['pair_type'])\n","\n","        # Stack tensors with error handling\n","        try:\n","            symbolic_batch = torch.stack(symbolic_list, dim=0)\n","        except RuntimeError as e:\n","            print(f\"Error stacking symbolic features: {e}\")\n","            # Fallback: ensure all have same shape\n","            min_dim = min(t.shape[-1] for t in symbolic_list)\n","            symbolic_list = [t[:, :min_dim] for t in symbolic_list]\n","            symbolic_batch = torch.stack(symbolic_list, dim=0)\n","\n","        try:\n","            audio_batch = torch.stack(audio_list, dim=0)\n","        except RuntimeError as e:\n","            print(f\"Error stacking audio features: {e}\")\n","            # Fallback: ensure all have same shape\n","            min_dim = min(t.shape[-1] for t in audio_list)\n","            audio_list = [t[:, :min_dim] for t in audio_list]\n","            audio_batch = torch.stack(audio_list, dim=0)\n","\n","        labels_batch = torch.stack(labels, dim=0)\n","\n","        return {\n","            'symbolic': symbolic_batch,\n","            'audio': audio_batch,\n","            'label': labels_batch,\n","            'pair_type': pair_types\n","        }\n","\n","    except Exception as e:\n","        print(f\"Error in custom_collate_fn: {e}\")\n","        # Return a minimal batch to avoid complete failure\n","        batch_size = len(batch)\n","        return {\n","            'symbolic': torch.zeros(batch_size, 1, 768),\n","            'audio': torch.zeros(batch_size, 1, 6144),\n","            'label': torch.zeros(batch_size),\n","            'pair_type': ['error'] * batch_size\n","        }\n","\n","\n","class UnifiedContrastiveModel(nn.Module):\n","    \"\"\"\n","    Enhanced model that outputs unified embeddings\n","    [Audio Features] + [MIDI Features] → Contrastive Model → [One Unified Embedding]\n","    \"\"\"\n","    def __init__(self, symbolic_dim=768, audio_dim=6144, latent_dim=512):\n","        super().__init__()\n","\n","        # Individual encoders\n","        self.symbolic_encoder = self._build_encoder(symbolic_dim, latent_dim // 2)\n","        self.audio_encoder = self._build_encoder(audio_dim, latent_dim // 2)\n","\n","        # Fusion layer for unified embedding\n","        self.fusion_layer = nn.Sequential(\n","            nn.Linear(latent_dim, latent_dim),\n","            nn.BatchNorm1d(latent_dim),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","            nn.Linear(latent_dim, latent_dim)\n","        )\n","\n","        # Temperature parameter for contrastive learning\n","        self.temperature = nn.Parameter(torch.tensor(0.07))\n","\n","    def _build_encoder(self, input_dim: int, output_dim: int) -> nn.Module:\n","        \"\"\"Build encoder network\"\"\"\n","        hidden_dim = max(512, input_dim // 4)\n","        return nn.Sequential(\n","            nn.Linear(input_dim, hidden_dim),\n","            nn.BatchNorm1d(hidden_dim),\n","            nn.ReLU(),\n","            nn.Dropout(0.3),\n","            nn.Linear(hidden_dim, hidden_dim // 2),\n","            nn.BatchNorm1d(hidden_dim // 2),\n","            nn.ReLU(),\n","            nn.Dropout(0.2),\n","            nn.Linear(hidden_dim // 2, output_dim)\n","        )\n","\n","    def forward(self, symbolic_features, audio_features):\n","        # Ensure proper shapes\n","        if symbolic_features.dim() == 3:\n","            symbolic_features = symbolic_features.squeeze(1)  # [batch, 1, 768] -> [batch, 768]\n","        if audio_features.dim() == 3:\n","            audio_features = audio_features.squeeze(1)        # [batch, 1, 6144] -> [batch, 6144]\n","\n","        # Encode individual modalities\n","        symbolic_emb = self.symbolic_encoder(symbolic_features)\n","        audio_emb = self.audio_encoder(audio_features)\n","\n","        # Concatenate and create unified embedding\n","        combined = torch.cat([symbolic_emb, audio_emb], dim=1)\n","        unified_embedding = self.fusion_layer(combined)\n","\n","        # Normalize the unified embedding\n","        unified_embedding = F.normalize(unified_embedding, p=2, dim=1)\n","\n","        return unified_embedding\n","\n","    def get_unified_embedding(self, symbolic_features, audio_features):\n","        \"\"\"Get the unified embedding for a pair\"\"\"\n","        return self.forward(symbolic_features, audio_features)\n","\n","\n","class InfoNCELoss(nn.Module):\n","    \"\"\"InfoNCE loss for contrastive learning with unified embeddings\"\"\"\n","    def __init__(self, temperature=0.07):\n","        super().__init__()\n","        self.temperature = temperature\n","\n","    def forward(self, embeddings, labels):\n","        # embeddings: [batch_size, embedding_dim]\n","        # labels: [batch_size] (1 for positive, 0 for negative)\n","\n","        batch_size = embeddings.shape[0]\n","\n","        # Create similarity matrix\n","        sim_matrix = torch.matmul(embeddings, embeddings.T) / self.temperature\n","\n","        # Create positive mask\n","        pos_mask = labels.unsqueeze(1) * labels.unsqueeze(0)\n","        neg_mask = 1 - pos_mask\n","\n","        # Compute InfoNCE loss\n","        pos_sim = sim_matrix * pos_mask\n","        neg_sim = sim_matrix * neg_mask\n","\n","        # For each positive pair, compute loss\n","        pos_pairs = torch.where(pos_mask == 1)\n","        if len(pos_pairs[0]) == 0:\n","            return torch.tensor(0.0, device=embeddings.device)\n","\n","        loss = 0\n","        for i, j in zip(pos_pairs[0], pos_pairs[1]):\n","            if i != j:  # Don't include self-similarity\n","                numerator = torch.exp(sim_matrix[i, j])\n","                denominator = torch.sum(torch.exp(sim_matrix[i, :]) * neg_mask[i, :]) + numerator\n","                loss += -torch.log(numerator / (denominator + 1e-8))  # Add epsilon for stability\n","\n","        return loss / max(len(pos_pairs[0]), 1)\n","\n","\n","class ReferenceBase:\n","    \"\"\"Reference database for plagiarism detection\"\"\"\n","    def __init__(self, save_dir: str = \"/content/drive/MyDrive/MuseGuard\"):\n","        self.embeddings = {}\n","        self.metadata = {}\n","        self.embedding_matrix = None\n","        self.ids = []\n","        self.save_dir = Path(save_dir)\n","        self.save_dir.mkdir(exist_ok=True)\n","\n","    def add_reference(self, piece_id: str, embedding: np.ndarray, metadata: Dict):\n","        \"\"\"Add a reference piece to the database\"\"\"\n","        self.embeddings[piece_id] = embedding\n","        self.metadata[piece_id] = metadata\n","\n","    def build_index(self):\n","        \"\"\"Build searchable index\"\"\"\n","        if not self.embeddings:\n","            return\n","\n","        self.ids = list(self.embeddings.keys())\n","        self.embedding_matrix = np.vstack([self.embeddings[id_] for id_ in self.ids])\n","        print(f\"Built reference index with {len(self.ids)} pieces\")\n","\n","    def search_similar(self, query_embedding: np.ndarray, top_k: int = 10, threshold: float = 0.7):\n","        \"\"\"Search for similar pieces\"\"\"\n","        if self.embedding_matrix is None:\n","            self.build_index()\n","\n","        # Compute similarities\n","        similarities = cosine_similarity([query_embedding], self.embedding_matrix)[0]\n","\n","        # Get top-k similar pieces\n","        top_indices = np.argsort(similarities)[::-1][:top_k]\n","\n","        results = []\n","        for idx in top_indices:\n","            similarity = similarities[idx]\n","            if similarity >= threshold:\n","                piece_id = self.ids[idx]\n","                results.append({\n","                    'piece_id': piece_id,\n","                    'similarity': similarity,\n","                    'metadata': self.metadata[piece_id]\n","                })\n","\n","        return results\n","\n","    def save(self, filename: str = \"music_reference_base.pkl\"):\n","        \"\"\"Save reference base to specified directory\"\"\"\n","        filepath = self.save_dir / filename\n","        try:\n","            data = {\n","                'embeddings': self.embeddings,\n","                'metadata': self.metadata,\n","                'embedding_matrix': self.embedding_matrix,\n","                'ids': self.ids\n","            }\n","            joblib.dump(data, filepath)\n","            print(f\"Reference base saved to {filepath}\")\n","        except Exception as e:\n","            print(f\"Error saving reference base: {e}\")\n","\n","    def load(self, filename: str = \"music_reference_base.pkl\"):\n","        \"\"\"Load reference base from specified directory\"\"\"\n","        filepath = self.save_dir / filename\n","        try:\n","            data = joblib.load(filepath)\n","            self.embeddings = data['embeddings']\n","            self.metadata = data['metadata']\n","            self.embedding_matrix = data['embedding_matrix']\n","            self.ids = data['ids']\n","            print(f\"Reference base loaded from {filepath}\")\n","        except Exception as e:\n","            print(f\"Error loading reference base: {e}\")\n","\n","\n","class EnhancedMusicPlagiarismDetector:\n","    \"\"\"Enhanced plagiarism detector with unified embeddings and reference base\"\"\"\n","\n","    def __init__(self,\n","                 symbolic_dir: str,\n","                 audio_dir: str,\n","                 metadata_csv: str,\n","                 save_dir: str = \"/content/drive/MyDrive/MuseGuard\",\n","                 device: str = 'cuda' if torch.cuda.is_available() else 'cpu'):\n","\n","        self.device = device\n","        self.save_dir = Path(save_dir)\n","        self.save_dir.mkdir(exist_ok=True)\n","\n","        self.model = UnifiedContrastiveModel().to(device)\n","        self.criterion = InfoNCELoss()\n","        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=2e-4, weight_decay=1e-4)\n","        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n","            self.optimizer, T_max=100, eta_min=1e-6\n","        )\n","\n","        # Create enhanced dataset\n","        self.dataset = MusicContrastiveDataset(\n","            symbolic_dir=symbolic_dir,\n","            audio_dir=audio_dir,\n","            metadata_csv=metadata_csv\n","        )\n","\n","        # Create data loaders with custom collate function\n","        self.train_loader = DataLoader(\n","            self.dataset,\n","            batch_size=16,  # Reduced batch size for stability\n","            shuffle=True,\n","            num_workers=2,  # Reduced workers to avoid multiprocessing issues\n","            pin_memory=True,\n","            drop_last=True,\n","            collate_fn=custom_collate_fn  # Use custom collate function\n","        )\n","\n","        # Initialize reference base with save directory\n","        self.reference_base = ReferenceBase(save_dir=save_dir)\n","\n","        # Training statistics\n","        self.training_stats = {\n","            'epoch_losses': [],\n","            'best_loss': float('inf'),\n","            'epochs_trained': 0\n","        }\n","\n","    def save_training_state(self, epoch: int, loss: float, is_best: bool = False):\n","        \"\"\"Save complete training state including model, optimizer, and reference base\"\"\"\n","        try:\n","            # Prepare checkpoint data\n","            checkpoint = {\n","                'epoch': epoch,\n","                'model_state_dict': self.model.state_dict(),\n","                'optimizer_state_dict': self.optimizer.state_dict(),\n","                'scheduler_state_dict': self.scheduler.state_dict(),\n","                'loss': loss,\n","                'training_stats': self.training_stats,\n","                'best_loss': self.training_stats['best_loss']\n","            }\n","\n","            # Save checkpoint\n","            checkpoint_path = self.save_dir / f\"checkpoint_epoch_{epoch}.pth\"\n","            torch.save(checkpoint, checkpoint_path)\n","\n","            # Save best model separately\n","            if is_best:\n","                best_model_path = self.save_dir / \"best_unified_model.pth\"\n","                torch.save(checkpoint, best_model_path)\n","                print(f\"Best model saved to {best_model_path}\")\n","\n","            # Save latest model (always overwrite)\n","            latest_model_path = self.save_dir / \"latest_unified_model.pth\"\n","            torch.save(checkpoint, latest_model_path)\n","\n","            # Build and save reference base every few epochs (live saving)\n","            if epoch % 5 == 0 or is_best:  # Save reference base every 5 epochs or when best\n","                print(f\"Building and saving reference base at epoch {epoch}...\")\n","                self.build_and_save_reference_base_live()\n","\n","            print(f\"Training state saved at epoch {epoch} (loss: {loss:.4f})\")\n","\n","        except Exception as e:\n","            print(f\"Error saving training state: {e}\")\n","\n","    def build_and_save_reference_base_live(self):\n","        \"\"\"Build and save reference base during training (live saving)\"\"\"\n","        try:\n","            # print(\"Building reference base from current model state...\")\n","            self.model.eval()\n","\n","            if len(self.dataset.segment_mapping) == 0:\n","                print(\"No segments available for building reference base!\")\n","                return\n","\n","            # Clear existing reference base\n","            self.reference_base = ReferenceBase(save_dir=str(self.save_dir))\n","            processed_pieces = set()\n","\n","            with torch.no_grad():\n","                for data_id, segments in self.dataset.segment_mapping.items():\n","                    if data_id in processed_pieces:\n","                        continue\n","\n","                    try:\n","                        # Get representative segments\n","                        symbolic_file = segments['symbolic'][0]  # Take first segment as representative\n","                        audio_file = segments['audio'][0]\n","\n","                        # Load features with proper dimensions\n","                        symbolic_features = self.dataset._load_features(symbolic_file, expected_dim=768).to(self.device)\n","                        audio_features = self.dataset._load_features(audio_file, expected_dim=6144).to(self.device)\n","\n","                        # Get unified embedding\n","                        unified_embedding = self.model(symbolic_features, audio_features)\n","\n","                        # Add to reference base\n","                        metadata = {\n","                            'composer': segments['composer'],\n","                            'composition': segments['composition'],\n","                            'movement': segments.get('movement', ''),\n","                            'ensemble': segments.get('ensemble', ''),\n","                            'era': segments['era']\n","                        }\n","\n","                        self.reference_base.add_reference(\n","                            data_id,\n","                            unified_embedding.cpu().numpy().flatten(),\n","                            metadata\n","                        )\n","\n","                        processed_pieces.add(data_id)\n","\n","                    except Exception as e:\n","                        print(f\"Error processing piece {data_id}: {e}\")\n","                        continue\n","\n","            # Build searchable index\n","            self.reference_base.build_index()\n","\n","            # Save reference base\n","            self.reference_base.save(\"music_reference_base.pkl\")\n","            print(f\"Reference base built and saved with {len(processed_pieces)} pieces\")\n","\n","        except Exception as e:\n","            print(f\"Error building reference base: {e}\")\n","        finally:\n","            self.model.train()  # Return to training mode\n","\n","    def train_epoch(self):\n","        \"\"\"Train for one epoch with better error handling\"\"\"\n","        self.model.train()\n","        total_loss = 0\n","        num_batches = 0\n","        successful_batches = 0\n","\n","        for batch_idx, batch in enumerate(self.train_loader):\n","            try:\n","                symbolic_features = batch['symbolic'].to(self.device)\n","                audio_features = batch['audio'].to(self.device)\n","                labels = batch['label'].to(self.device)\n","\n","                # Skip batch if it contains error data\n","                if 'error' in batch['pair_type']:\n","                    print(f\"Skipping error batch {batch_idx}\")\n","                    continue\n","\n","                self.optimizer.zero_grad()\n","\n","                # Get unified embeddings\n","                unified_embeddings = self.model(symbolic_features, audio_features)\n","\n","                # Compute loss\n","                loss = self.criterion(unified_embeddings, labels)\n","\n","                # Skip if loss is invalid\n","                if torch.isnan(loss) or torch.isinf(loss):\n","                    print(f\"Skipping batch {batch_idx} due to invalid loss: {loss}\")\n","                    continue\n","\n","                # Backward pass\n","                loss.backward()\n","                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n","                self.optimizer.step()\n","\n","                total_loss += loss.item()\n","                num_batches += 1\n","                successful_batches += 1\n","\n","                # Print progress every 100 batches\n","                if batch_idx % 100 == 0:\n","                    print(f\"Batch {batch_idx}/{len(self.train_loader)}, Loss: {loss.item():.4f}\")\n","\n","            except Exception as e:\n","                print(f\"Error in batch {batch_idx}: {e}\")\n","                continue\n","\n","        if successful_batches == 0:\n","            print(\"WARNING: No successful batches in this epoch!\")\n","            return float('inf')\n","\n","        avg_loss = total_loss / successful_batches\n","        self.scheduler.step()\n","        return avg_loss\n","\n","    def train(self, num_epochs: int = 50):\n","        \"\"\"Train the enhanced model with live saving\"\"\"\n","        print(f\"Training enhanced model on {len(self.dataset)} pairs\")\n","        print(f\"All models and reference base will be saved to: {self.save_dir}\")\n","        print(f\"Device: {self.device}\")\n","\n","        if len(self.dataset) == 0:\n","            print(\"ERROR: Dataset is empty, cannot train!\")\n","            return\n","\n","        # Load existing checkpoint if available\n","        latest_checkpoint = self.save_dir / \"latest_unified_model.pth\"\n","        start_epoch = 0\n","\n","        if latest_checkpoint.exists():\n","            print(f\"Found existing checkpoint, loading...\")\n","            try:\n","                checkpoint = torch.load(latest_checkpoint, map_location=self.device)\n","                self.model.load_state_dict(checkpoint['model_state_dict'])\n","                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","                self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n","                self.training_stats = checkpoint.get('training_stats', self.training_stats)\n","                start_epoch = checkpoint['epoch'] + 1\n","                print(f\"Resumed training from epoch {start_epoch}\")\n","            except Exception as e:\n","                print(f\"Error loading checkpoint: {e}, starting from scratch\")\n","\n","        patience = 10\n","        patience_counter = 0\n","\n","        for epoch in range(start_epoch, num_epochs):\n","            try:\n","                print(f\"\\n Epoch {epoch + 1}/{num_epochs}\")\n","                avg_loss = self.train_epoch()\n","\n","                if avg_loss == float('inf'):\n","                    print(f\"Epoch {epoch + 1} failed, skipping...\")\n","                    continue\n","\n","                print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n","\n","                # Update training stats\n","                self.training_stats['epoch_losses'].append(avg_loss)\n","                self.training_stats['epochs_trained'] = epoch + 1\n","\n","                # Check if this is the best model\n","                is_best = avg_loss < self.training_stats['best_loss']\n","                if is_best:\n","                    self.training_stats['best_loss'] = avg_loss\n","                    patience_counter = 0\n","                    print(f\"New best model! Loss: {avg_loss:.4f}\")\n","                else:\n","                    patience_counter += 1\n","\n","                # Save training state (LIVE SAVING)\n","                self.save_training_state(epoch + 1, avg_loss, is_best=is_best)\n","\n","                # Early stopping\n","                if patience_counter >= patience:\n","                    print(f\"Early stopping after {patience} epochs without improvement\")\n","                    break\n","\n","            except Exception as e:\n","                print(f\"Error in epoch {epoch + 1}: {e}\")\n","                # Still try to save current state even if epoch failed\n","                try:\n","                    self.save_training_state(epoch + 1, float('inf'), is_best=False)\n","                except:\n","                    pass\n","                continue\n","\n","        # Final save\n","        print(f\"\\n Training completed!\")\n","        print(f\"Total epochs: {self.training_stats['epochs_trained']}\")\n","        print(f\"Best loss: {self.training_stats['best_loss']:.4f}\")\n","\n","        # Final model and reference base save\n","        try:\n","            final_model_path = self.save_dir / \"final_unified_plagiarism_model.pth\"\n","            latest_checkpoint = self.save_dir / \"latest_unified_model.pth\"\n","            if latest_checkpoint.exists():\n","                import shutil\n","                shutil.copy2(latest_checkpoint, final_model_path)\n","                print(f\"Final model saved to {final_model_path}\")\n","\n","            # Build final reference base\n","            print(\"Building final reference base...\")\n","            self.build_and_save_reference_base_live()\n","\n","        except Exception as e:\n","            print(f\"Error in final save: {e}\")\n","\n","    def detect_plagiarism_unified(self,\n","                                symbolic_features: torch.Tensor,\n","                                audio_features: torch.Tensor,\n","                                threshold: float = 0.8) -> Dict:\n","        \"\"\"Detect plagiarism using unified embeddings\"\"\"\n","        self.model.eval()\n","\n","        with torch.no_grad():\n","            symbolic_features = symbolic_features.to(self.device)\n","            audio_features = audio_features.to(self.device)\n","\n","            # Get unified embedding\n","            unified_embedding = self.model(symbolic_features, audio_features)\n","\n","            # Search in reference base\n","            query_embedding = unified_embedding.cpu().numpy().flatten()\n","            similar_pieces = self.reference_base.search_similar(\n","                query_embedding,\n","                top_k=10,\n","                threshold=threshold\n","            )\n","\n","            return {\n","                'unified_embedding': unified_embedding.cpu().numpy(),\n","                'similar_pieces': similar_pieces,\n","                'max_similarity': max([p['similarity'] for p in similar_pieces]) if similar_pieces else 0.0,\n","                'is_plagiarism': len(similar_pieces) > 0\n","            }\n","\n","    def save_model(self, filename: str):\n","        \"\"\"Save the trained model to the specified directory\"\"\"\n","        try:\n","            filepath = self.save_dir / filename\n","            torch.save({\n","                'model_state_dict': self.model.state_dict(),\n","                'optimizer_state_dict': self.optimizer.state_dict(),\n","                'scheduler_state_dict': self.scheduler.state_dict(),\n","                'training_stats': self.training_stats\n","            }, filepath)\n","            print(f\"Model saved to {filepath}\")\n","        except Exception as e:\n","            print(f\"Error saving model to {filepath}: {e}\")\n","\n","    def load_model(self, filename: str):\n","        \"\"\"Load a trained model from the specified directory\"\"\"\n","        try:\n","            filepath = self.save_dir / filename\n","            checkpoint = torch.load(filepath, map_location=self.device)\n","            self.model.load_state_dict(checkpoint['model_state_dict'])\n","            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n","            if 'training_stats' in checkpoint:\n","                self.training_stats = checkpoint['training_stats']\n","            print(f\"Model loaded from {filepath}\")\n","        except Exception as e:\n","            print(f\"Error loading model from {filepath}: {e}\")\n","\n","\n","def validate_dataset_before_training(detector):\n","    \"\"\"Validate dataset before starting training\"\"\"\n","    print(\"Validating dataset...\")\n","\n","    if len(detector.dataset) == 0:\n","        print(\"ERROR: Dataset is empty!\")\n","        return False\n","\n","    # Test loading a few samples\n","    print(\"Testing sample loading...\")\n","    try:\n","        for i in range(min(5, len(detector.dataset))):\n","            sample = detector.dataset[i]\n","            print(f\"Sample {i}: symbolic shape {sample['symbolic'].shape}, \"\n","                  f\"audio shape {sample['audio'].shape}, label {sample['label']}\")\n","    except Exception as e:\n","        print(f\"Error loading samples: {e}\")\n","        return False\n","\n","    # Test DataLoader\n","    print(\"Testing DataLoader...\")\n","    try:\n","        batch = next(iter(detector.train_loader))\n","        print(f\"Batch shapes - symbolic: {batch['symbolic'].shape}, \"\n","              f\"audio: {batch['audio'].shape}, labels: {batch['label'].shape}\")\n","        return True\n","    except Exception as e:\n","        print(f\"Error with DataLoader: {e}\")\n","        return False\n","\n","\n","def main():\n","    \"\"\"Main training and evaluation pipeline with ultra-fast loading and live saving\"\"\"\n","    base_dir = \"/content/drive/MyDrive/MuseGuard\"\n","    symbolic_dir = f\"{base_dir}/extracted_features\"      # MIDI embeddings\n","    audio_dir = f\"{base_dir}/musicnet_embeddings\"        # Audio/WAV embeddings\n","    metadata_csv = f\"{base_dir}/musicnet_metadata.csv\"\n","    save_dir = base_dir  # All outputs will be saved here\n","\n","    print(\"=== ULTRA-FAST Music Plagiarism Detection System with LIVE SAVING ===\")\n","    print()\n","    print(\"📁 Directory Setup:\")\n","    print(f\"  MIDI (symbolic) embeddings: {symbolic_dir}\")\n","    print(f\"  Audio (WAV) embeddings: {audio_dir}\")\n","    print(f\"  Metadata CSV: {metadata_csv}\")\n","    print(f\"  💾 SAVE DIRECTORY: {save_dir}\")\n","\n","    # Check if files exist\n","    for path, name in [(symbolic_dir, \"MIDI/symbolic\"), (audio_dir, \"audio/WAV\"), (metadata_csv, \"metadata CSV\")]:\n","        if not os.path.exists(path):\n","            print(f\"ERROR: {name} path '{path}' does not exist!\")\n","            return\n","\n","    try:\n","        # Initialize enhanced detector with live saving\n","        print(f\"\\n Initializing ultra-fast music plagiarism detector with live saving...\")\n","        print(\"Processing composers: Bach, Beethoven, Brahms, Cambini, Dvorak, Faure, Haydn, Mozart, Ravel, Schubert\")\n","\n","        detector = EnhancedMusicPlagiarismDetector(\n","            symbolic_dir=symbolic_dir,  # MIDI embeddings\n","            audio_dir=audio_dir,        # Audio embeddings\n","            metadata_csv=metadata_csv,\n","            save_dir=save_dir           # Save everything to Google Drive\n","        )\n","\n","        # Validate dataset before training\n","        if not validate_dataset_before_training(detector):\n","            print(\"Dataset validation failed. Exiting...\")\n","            return\n","\n","        print(f\"\\n Dataset successfully created with {len(detector.dataset)} strategic pairs\")\n","        print(\" Pair types include:\")\n","        print(\"  - Positive: Direct matches, overlapping segments, nearby segments\")\n","        print(\"  - Negative: Same composer (hard), same era (medium), different era (easy)\")\n","\n","        # Train the model with live saving\n","        print(f\"\\n Starting training with unified embeddings and LIVE SAVING to {save_dir}...\")\n","\n","        detector.train(num_epochs=50)\n","\n","        print(f\"\\n === Training completed successfully! ===\")\n","        print(f\"All files saved to: {save_dir}\")\n","        print(\"Files available:\")\n","        print(\"- final_unified_plagiarism_model.pth (final trained model)\")\n","        print(\"- best_unified_model.pth (best performing model)\")\n","        print(\"- latest_unified_model.pth (latest model state)\")\n","        print(\"- music_reference_base.pkl (reference database)\")\n","        print(\"- file_validation_cache.pkl (validation cache for future runs)\")\n","        print(\"- checkpoint_epoch_X.pth (checkpoint files)\")\n","\n","    except Exception as e:\n","        print(f\"Error during execution: {e}\")\n","        import traceback\n","        traceback.print_exc()\n","\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g5miJPAg-mLS","executionInfo":{"status":"ok","timestamp":1754473348944,"user_tz":-480,"elapsed":29696050,"user":{"displayName":"Bethany","userId":"07029141995136804205"}},"outputId":"e9e81c3e-6265-4bf1-f5aa-403380e170f5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["=== 🚀 ULTRA-FAST Music Plagiarism Detection System with LIVE SAVING ===\n","🎯 Performance Features:\n","  ✅ Fast file validation using size heuristics\n","  ✅ Lazy loading - only load files when needed\n","  ✅ Persistent validation cache\n","  ✅ Skip corrupted files automatically\n","  ✅ Batch error recovery\n","  ✅ Early stopping\n","  ✅ LIVE SAVING - Models and reference base saved during training!\n","  ✅ Resume training from interruptions\n","  ✅ Checkpoint recovery\n","\n","📁 Directory Setup:\n","  MIDI (symbolic) embeddings: /content/drive/MyDrive/MuseGuard/extracted_features\n","  Audio (WAV) embeddings: /content/drive/MyDrive/MuseGuard/musicnet_embeddings\n","  Metadata CSV: /content/drive/MyDrive/MuseGuard/musicnet_metadata.csv\n","  💾 SAVE DIRECTORY: /content/drive/MyDrive/MuseGuard\n","\n","🔄 Initializing ultra-fast music plagiarism detector with live saving...\n","🎼 Processing composers: Bach, Beethoven, Brahms, Cambini, Dvorak, Faure, Haydn, Mozart, Ravel, Schubert\n","Creating new validation cache\n","Loaded metadata for 330 pieces\n","Columns: ['id', 'composer', 'composition', 'movement', 'ensemble', 'source', 'transcriber', 'catalog_name', 'seconds']\n","Found 10 composers\n","Fast-loading symbolic files from: /content/drive/MyDrive/MuseGuard/extracted_features\n","Found 22009 files\n","Saved validation cache with 1000 entries\n","  Processed 1000 files...\n","Saved validation cache with 2000 entries\n","  Processed 2000 files...\n","Saved validation cache with 3000 entries\n","  Processed 3000 files...\n","Saved validation cache with 4000 entries\n","  Processed 4000 files...\n","Saved validation cache with 5000 entries\n","  Processed 5000 files...\n","Saved validation cache with 6000 entries\n","  Processed 6000 files...\n","Saved validation cache with 7000 entries\n","  Processed 7000 files...\n","Saved validation cache with 8000 entries\n","  Processed 8000 files...\n","Saved validation cache with 9000 entries\n","  Processed 9000 files...\n","Saved validation cache with 10000 entries\n","  Processed 10000 files...\n","Saved validation cache with 11000 entries\n","  Processed 11000 files...\n","Saved validation cache with 12000 entries\n","  Processed 12000 files...\n","Saved validation cache with 13000 entries\n","  Processed 13000 files...\n","Saved validation cache with 14000 entries\n","  Processed 14000 files...\n","Saved validation cache with 15000 entries\n","  Processed 15000 files...\n","Saved validation cache with 16000 entries\n","  Processed 16000 files...\n","Saved validation cache with 17000 entries\n","  Processed 17000 files...\n","Skipping invalid symbolic file tokens_2567_ps27_01_seg17_features.npy: size_too_small_for_dimension\n","Saved validation cache with 18000 entries\n","  Processed 18000 files...\n","Saved validation cache with 19000 entries\n","  Processed 19000 files...\n","Saved validation cache with 20000 entries\n","  Processed 20000 files...\n","Saved validation cache with 21000 entries\n","  Processed 21000 files...\n","Saved validation cache with 22000 entries\n","  Processed 22000 files...\n","Saved validation cache with 22009 entries\n","Fast validation complete: 22008 valid, 1 invalid\n","Organized into 323 ID groups\n","Fast-loading audio files from: /content/drive/MyDrive/MuseGuard/musicnet_embeddings\n","Found 24740 files\n","Saved validation cache with 23009 entries\n","  Processed 1000 files...\n","Saved validation cache with 24009 entries\n","  Processed 2000 files...\n","Saved validation cache with 25009 entries\n","  Processed 3000 files...\n","Saved validation cache with 26009 entries\n","  Processed 4000 files...\n","Saved validation cache with 27009 entries\n","  Processed 5000 files...\n","Saved validation cache with 28009 entries\n","  Processed 6000 files...\n","Saved validation cache with 29009 entries\n","  Processed 7000 files...\n","Saved validation cache with 30009 entries\n","  Processed 8000 files...\n","Saved validation cache with 31009 entries\n","  Processed 9000 files...\n","Saved validation cache with 32009 entries\n","  Processed 10000 files...\n","Saved validation cache with 33009 entries\n","  Processed 11000 files...\n","Saved validation cache with 34009 entries\n","  Processed 12000 files...\n","Saved validation cache with 35009 entries\n","  Processed 13000 files...\n","Saved validation cache with 36009 entries\n","  Processed 14000 files...\n","Saved validation cache with 37009 entries\n","  Processed 15000 files...\n","Saved validation cache with 38009 entries\n","  Processed 16000 files...\n","Saved validation cache with 39009 entries\n","  Processed 17000 files...\n","Saved validation cache with 40009 entries\n","  Processed 18000 files...\n","Saved validation cache with 41009 entries\n","  Processed 19000 files...\n","Saved validation cache with 42009 entries\n","  Processed 20000 files...\n","Saved validation cache with 43009 entries\n","  Processed 21000 files...\n","Saved validation cache with 44009 entries\n","  Processed 22000 files...\n","Saved validation cache with 45009 entries\n","  Processed 23000 files...\n","Saved validation cache with 46009 entries\n","  Processed 24000 files...\n","Saved validation cache with 46749 entries\n","Fast validation complete: 24740 valid, 0 invalid\n","Organized into 330 ID groups\n","Common IDs across all sources: 323\n","Created enhanced mapping for 323 pieces\n","Creating strategic pairs from 323 pieces...\n","Composer distribution: {'Beethoven': 157, 'Bach': 60, 'Mozart': 24, 'Schubert': 30, 'Cambini': 9, 'Brahms': 24, 'Faure': 4, 'Dvorak': 8, 'Ravel': 4, 'Haydn': 3}\n","Era distribution: {'Classical': 184, 'Baroque': 60, 'Romantic': 62, 'Classical-Modern': 9, 'Impressionist': 8}\n","Created 63723 positive pairs\n","Creating 95584 negative pairs:\n","  - Hard (same composer): 38233\n","  - Medium (same era): 28675\n","  - Easy (different era): 28676\n","Actually created: 149 hard, 31 medium, 28676 easy\n","Created 92579 total pairs:\n","  - Positive: 63723\n","  - Negative: 28856\n","🔍 Validating dataset...\n","🧪 Testing sample loading...\n","Sample 0: symbolic shape torch.Size([1, 768]), audio shape torch.Size([1, 6144]), label 1.0\n","Sample 1: symbolic shape torch.Size([1, 768]), audio shape torch.Size([1, 6144]), label 1.0\n","Sample 2: symbolic shape torch.Size([1, 768]), audio shape torch.Size([1, 6144]), label 1.0\n","Sample 3: symbolic shape torch.Size([1, 768]), audio shape torch.Size([1, 6144]), label 1.0\n","Sample 4: symbolic shape torch.Size([1, 768]), audio shape torch.Size([1, 6144]), label 1.0\n","🧪 Testing DataLoader...\n","Batch shapes - symbolic: torch.Size([16, 1, 768]), audio: torch.Size([16, 1, 6144]), labels: torch.Size([16])\n","\n","✅ Dataset successfully created with 92579 strategic pairs\n","🎯 Pair types include:\n","  - Positive: Direct matches, overlapping segments, nearby segments\n","  - Negative: Same composer (hard), same era (medium), different era (easy)\n","\n","🎯 Starting training with unified embeddings and LIVE SAVING to /content/drive/MyDrive/MuseGuard...\n","🏗️ Architecture: [MIDI Features] + [Audio Features] → Unified Embedding\n","🚀 Enhanced features:\n","  ✅ Ultra-fast file validation (90% faster)\n","  ✅ Persistent cache system\n","  ✅ Lazy loading during training\n","  ✅ Custom collate function for tensor consistency\n","  ✅ Better error handling and recovery\n","  ✅ Early stopping with patience\n","  ✅ Optimized batch size for stability\n","  🔥 LIVE SAVING: Models saved every epoch, reference base every 5 epochs!\n","  🔄 RESUME CAPABILITY: Can resume from interruptions!\n","🚀 Training enhanced model on 92579 pairs\n","💾 All models and reference base will be saved to: /content/drive/MyDrive/MuseGuard\n","🔧 Device: cuda\n","\n","📊 Epoch 1/50\n","Batch 0/5786, Loss: 1.8047\n","Batch 100/5786, Loss: 1.5014\n","Batch 200/5786, Loss: 1.7008\n","Batch 300/5786, Loss: 1.3233\n","Batch 400/5786, Loss: 1.3937\n","Batch 500/5786, Loss: 1.2769\n","Batch 600/5786, Loss: 0.6611\n","Batch 700/5786, Loss: 0.7877\n","Batch 800/5786, Loss: 1.8712\n","Batch 900/5786, Loss: 1.4378\n","Batch 1000/5786, Loss: 1.5315\n","Batch 1100/5786, Loss: 0.7821\n","Batch 1200/5786, Loss: 1.5573\n","Batch 1300/5786, Loss: 0.7572\n","Batch 1400/5786, Loss: 0.0608\n","Batch 1500/5786, Loss: 0.8219\n","Batch 1600/5786, Loss: 1.5663\n","Batch 1700/5786, Loss: 0.5717\n","Batch 1800/5786, Loss: 1.3924\n","Batch 1900/5786, Loss: 1.1469\n","Batch 2000/5786, Loss: 1.9619\n","Batch 2100/5786, Loss: 1.0168\n","Batch 2200/5786, Loss: 1.3883\n","Batch 2300/5786, Loss: 0.9793\n","Batch 2400/5786, Loss: 0.0197\n","Batch 2500/5786, Loss: 0.4832\n","Batch 2600/5786, Loss: 1.1596\n","Batch 2700/5786, Loss: 0.4923\n","Batch 2800/5786, Loss: 0.2997\n","Batch 2900/5786, Loss: 0.1727\n","Batch 3000/5786, Loss: 0.2561\n","Batch 3100/5786, Loss: 0.0069\n","Batch 3200/5786, Loss: 0.5911\n","Batch 3300/5786, Loss: 1.3785\n","Batch 3400/5786, Loss: 1.1246\n","Batch 3500/5786, Loss: 2.0010\n","Batch 3600/5786, Loss: 1.9367\n","Batch 3700/5786, Loss: 0.5556\n","Batch 3800/5786, Loss: 0.2475\n","Batch 3900/5786, Loss: 0.1149\n","Batch 4000/5786, Loss: 0.9127\n","Batch 4100/5786, Loss: 1.3208\n","Batch 4200/5786, Loss: 0.0661\n","Batch 4300/5786, Loss: 0.1249\n","Batch 4400/5786, Loss: 0.3600\n","Batch 4500/5786, Loss: 0.5417\n","Batch 4600/5786, Loss: 0.6999\n","Batch 4700/5786, Loss: 0.0553\n","Batch 4800/5786, Loss: 0.4423\n","Batch 4900/5786, Loss: 1.8462\n","Batch 5000/5786, Loss: 0.8355\n","Batch 5100/5786, Loss: 0.9262\n","Batch 5200/5786, Loss: 0.1902\n","Batch 5300/5786, Loss: 1.8755\n","Batch 5400/5786, Loss: 0.4984\n","Batch 5500/5786, Loss: 0.5464\n","Batch 5600/5786, Loss: 0.3188\n","Batch 5700/5786, Loss: 1.8215\n","📈 Epoch 1/50, Average Loss: 0.9069\n","🎉 New best model! Loss: 0.9069\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 1...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 1 (loss: 0.9069)\n","\n","📊 Epoch 2/50\n","Batch 0/5786, Loss: 0.4997\n","Batch 100/5786, Loss: 0.0480\n","Batch 200/5786, Loss: 1.2032\n","Batch 300/5786, Loss: 0.7205\n","Batch 400/5786, Loss: 0.3993\n","Batch 500/5786, Loss: 0.2853\n","Batch 600/5786, Loss: 0.1645\n","Batch 700/5786, Loss: 1.4066\n","Batch 800/5786, Loss: 0.9308\n","Batch 900/5786, Loss: 0.4443\n","Batch 1000/5786, Loss: 0.0907\n","Batch 1100/5786, Loss: 0.8839\n","Batch 1200/5786, Loss: 0.0001\n","Batch 1300/5786, Loss: 1.6913\n","Batch 1400/5786, Loss: 1.6864\n","Batch 1500/5786, Loss: 0.0746\n","Batch 1600/5786, Loss: 0.3688\n","Batch 1700/5786, Loss: 0.6232\n","Batch 1800/5786, Loss: 0.8283\n","Batch 1900/5786, Loss: 0.0003\n","Batch 2000/5786, Loss: 0.0035\n","Batch 2100/5786, Loss: 0.0828\n","Batch 2200/5786, Loss: 1.1495\n","Batch 2300/5786, Loss: 0.0064\n","Batch 2400/5786, Loss: 0.8562\n","Batch 2500/5786, Loss: 0.0001\n","Batch 2600/5786, Loss: 0.3421\n","Batch 2700/5786, Loss: 0.9643\n","Batch 2800/5786, Loss: 0.2030\n","Batch 2900/5786, Loss: 1.1316\n","Batch 3000/5786, Loss: 0.7032\n","Batch 3100/5786, Loss: 0.0122\n","Batch 3200/5786, Loss: 1.5477\n","Batch 3300/5786, Loss: 0.4606\n","Batch 3400/5786, Loss: 0.6999\n","Batch 3500/5786, Loss: 0.0860\n","Batch 3600/5786, Loss: 0.4837\n","Batch 3700/5786, Loss: 0.6500\n","Batch 3800/5786, Loss: 0.7579\n","Batch 3900/5786, Loss: 1.3975\n","Batch 4000/5786, Loss: 0.7450\n","Batch 4100/5786, Loss: 0.3989\n","Batch 4200/5786, Loss: 0.8167\n","Batch 4300/5786, Loss: 1.4178\n","Batch 4400/5786, Loss: 1.0721\n","Batch 4500/5786, Loss: 0.0472\n","Batch 4600/5786, Loss: 0.8499\n","Batch 4700/5786, Loss: 1.1496\n","Batch 4800/5786, Loss: 0.6059\n","Batch 4900/5786, Loss: 1.4772\n","Batch 5000/5786, Loss: 0.1350\n","Batch 5100/5786, Loss: 1.2021\n","Batch 5200/5786, Loss: 0.5639\n","Batch 5300/5786, Loss: 0.4638\n","Batch 5400/5786, Loss: 0.1664\n","Batch 5500/5786, Loss: 0.5170\n","Batch 5600/5786, Loss: 0.0280\n","Batch 5700/5786, Loss: 0.5589\n","📈 Epoch 2/50, Average Loss: 0.6173\n","🎉 New best model! Loss: 0.6173\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 2...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 2 (loss: 0.6173)\n","\n","📊 Epoch 3/50\n","Batch 0/5786, Loss: 0.0409\n","Batch 100/5786, Loss: 0.0000\n","Batch 200/5786, Loss: 0.4758\n","Batch 300/5786, Loss: 0.7489\n","Batch 400/5786, Loss: 0.0141\n","Batch 500/5786, Loss: 0.6873\n","Batch 600/5786, Loss: 0.9897\n","Batch 700/5786, Loss: 0.1521\n","Batch 800/5786, Loss: 0.2903\n","Batch 900/5786, Loss: 0.0256\n","Batch 1000/5786, Loss: 0.0002\n","Batch 1100/5786, Loss: 0.2888\n","Batch 1200/5786, Loss: 0.1859\n","Batch 1300/5786, Loss: 0.0994\n","Batch 1400/5786, Loss: 0.2757\n","Batch 1500/5786, Loss: 0.8613\n","Batch 1600/5786, Loss: 0.1640\n","Batch 1700/5786, Loss: 0.5901\n","Batch 1800/5786, Loss: 0.7551\n","Batch 1900/5786, Loss: 0.4493\n","Batch 2000/5786, Loss: 0.1651\n","Batch 2100/5786, Loss: 0.7836\n","Batch 2200/5786, Loss: 0.5133\n","Batch 2300/5786, Loss: 1.1777\n","Batch 2400/5786, Loss: 0.7842\n","Batch 2500/5786, Loss: 0.0010\n","Batch 2600/5786, Loss: 0.0290\n","Batch 2700/5786, Loss: 0.7472\n","Batch 2800/5786, Loss: 1.0725\n","Batch 2900/5786, Loss: 0.0027\n","Batch 3000/5786, Loss: 0.7305\n","Batch 3100/5786, Loss: 1.4421\n","Batch 3200/5786, Loss: 1.0726\n","Batch 3300/5786, Loss: 0.4171\n","Batch 3400/5786, Loss: 0.1463\n","Batch 3500/5786, Loss: 1.3836\n","Batch 3600/5786, Loss: 0.1896\n","Batch 3700/5786, Loss: 0.4582\n","Batch 3800/5786, Loss: 0.0037\n","Batch 3900/5786, Loss: 0.0196\n","Batch 4000/5786, Loss: 1.0733\n","Batch 4100/5786, Loss: 0.0238\n","Batch 4200/5786, Loss: 0.1610\n","Batch 4300/5786, Loss: 1.5764\n","Batch 4400/5786, Loss: 0.8328\n","Batch 4500/5786, Loss: 0.0002\n","Batch 4600/5786, Loss: 0.0035\n","Batch 4700/5786, Loss: 1.3093\n","Batch 4800/5786, Loss: 0.0089\n","Batch 4900/5786, Loss: 1.5829\n","Batch 5000/5786, Loss: 0.2322\n","Batch 5100/5786, Loss: 0.0107\n","Batch 5200/5786, Loss: 0.0440\n","Batch 5300/5786, Loss: 0.4368\n","Batch 5400/5786, Loss: 1.2036\n","Batch 5500/5786, Loss: 1.0434\n","Batch 5600/5786, Loss: 0.0470\n","Batch 5700/5786, Loss: 0.0001\n","📈 Epoch 3/50, Average Loss: 0.4977\n","🎉 New best model! Loss: 0.4977\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 3...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 3 (loss: 0.4977)\n","\n","📊 Epoch 4/50\n","Batch 0/5786, Loss: 0.8909\n","Batch 100/5786, Loss: 0.5264\n","Batch 200/5786, Loss: 0.0004\n","Batch 300/5786, Loss: 0.0009\n","Batch 400/5786, Loss: 0.0344\n","Batch 500/5786, Loss: 1.7948\n","Batch 600/5786, Loss: 0.2578\n","Batch 700/5786, Loss: 0.3270\n","Batch 800/5786, Loss: 0.3635\n","Batch 900/5786, Loss: 0.0200\n","Batch 1000/5786, Loss: 0.0655\n","Batch 1100/5786, Loss: 1.3222\n","Batch 1200/5786, Loss: 0.0000\n","Batch 1300/5786, Loss: 0.0620\n","Batch 1400/5786, Loss: 0.0077\n","Batch 1500/5786, Loss: 0.6032\n","Batch 1600/5786, Loss: 0.1962\n","Batch 1700/5786, Loss: 0.8336\n","Batch 1800/5786, Loss: 0.8254\n","Batch 1900/5786, Loss: 0.0110\n","Batch 2000/5786, Loss: 0.8821\n","Batch 2100/5786, Loss: 1.6439\n","Batch 2200/5786, Loss: 0.0014\n","Batch 2300/5786, Loss: 0.5800\n","Batch 2400/5786, Loss: 0.5653\n","Batch 2500/5786, Loss: 0.0219\n","Batch 2600/5786, Loss: 0.7439\n","Batch 2700/5786, Loss: 0.0400\n","Batch 2800/5786, Loss: 1.1150\n","Batch 2900/5786, Loss: 0.0055\n","Batch 3000/5786, Loss: 0.6734\n","Batch 3100/5786, Loss: 0.1032\n","Batch 3200/5786, Loss: 0.0147\n","Batch 3300/5786, Loss: 0.2190\n","Batch 3400/5786, Loss: 1.4374\n","Batch 3500/5786, Loss: 0.2917\n","Batch 3600/5786, Loss: 0.6252\n","Batch 3700/5786, Loss: 0.0273\n","Batch 3800/5786, Loss: 1.2595\n","Batch 3900/5786, Loss: 0.1705\n","Batch 4000/5786, Loss: 1.2821\n","Batch 4100/5786, Loss: 0.9174\n","Batch 4200/5786, Loss: 0.6016\n","Batch 4300/5786, Loss: 0.5707\n","Batch 4400/5786, Loss: 0.0000\n","Batch 4500/5786, Loss: 0.9663\n","Batch 4600/5786, Loss: 0.1730\n","Batch 4700/5786, Loss: 0.4761\n","Batch 4800/5786, Loss: 0.3297\n","Batch 4900/5786, Loss: 0.9135\n","Batch 5000/5786, Loss: 0.7457\n","Batch 5100/5786, Loss: 0.0022\n","Batch 5200/5786, Loss: 0.2219\n","Batch 5300/5786, Loss: 0.2381\n","Batch 5400/5786, Loss: 0.3551\n","Batch 5500/5786, Loss: 1.1245\n","Batch 5600/5786, Loss: 0.2272\n","Batch 5700/5786, Loss: 0.3141\n","📈 Epoch 4/50, Average Loss: 0.4333\n","🎉 New best model! Loss: 0.4333\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 4...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 4 (loss: 0.4333)\n","\n","📊 Epoch 5/50\n","Batch 0/5786, Loss: 0.0203\n","Batch 100/5786, Loss: 0.0002\n","Batch 200/5786, Loss: 0.2932\n","Batch 300/5786, Loss: 1.5031\n","Batch 400/5786, Loss: 0.1920\n","Batch 500/5786, Loss: 0.1025\n","Batch 600/5786, Loss: 1.4720\n","Batch 700/5786, Loss: 0.1430\n","Batch 800/5786, Loss: 0.0272\n","Batch 900/5786, Loss: 0.1001\n","Batch 1000/5786, Loss: 0.5683\n","Batch 1100/5786, Loss: 0.3595\n","Batch 1200/5786, Loss: 0.0767\n","Batch 1300/5786, Loss: 0.3769\n","Batch 1400/5786, Loss: 0.5115\n","Batch 1500/5786, Loss: 0.0010\n","Batch 1600/5786, Loss: 0.1276\n","Batch 1700/5786, Loss: 0.0361\n","Batch 1800/5786, Loss: 0.4545\n","Batch 1900/5786, Loss: 0.0410\n","Batch 2000/5786, Loss: 0.1344\n","Batch 2100/5786, Loss: 1.9776\n","Batch 2200/5786, Loss: 1.1226\n","Batch 2300/5786, Loss: 0.0418\n","Batch 2400/5786, Loss: 0.5093\n","Batch 2500/5786, Loss: 0.0141\n","Batch 2600/5786, Loss: 0.0516\n","Batch 2700/5786, Loss: 0.0117\n","Batch 2800/5786, Loss: 0.3281\n","Batch 2900/5786, Loss: 0.9226\n","Batch 3000/5786, Loss: 0.1376\n","Batch 3100/5786, Loss: 0.0632\n","Batch 3200/5786, Loss: 0.6363\n","Batch 3300/5786, Loss: 0.3663\n","Batch 3400/5786, Loss: 0.0052\n","Batch 3500/5786, Loss: 0.0005\n","Batch 3600/5786, Loss: 0.4767\n","Batch 3700/5786, Loss: 0.0000\n","Batch 3800/5786, Loss: 0.3827\n","Batch 3900/5786, Loss: 0.1511\n","Batch 4000/5786, Loss: 0.0119\n","Batch 4100/5786, Loss: 0.3139\n","Batch 4200/5786, Loss: 0.3158\n","Batch 4300/5786, Loss: 0.8054\n","Batch 4400/5786, Loss: 0.7287\n","Batch 4500/5786, Loss: 0.4081\n","Batch 4600/5786, Loss: 0.0034\n","Batch 4700/5786, Loss: 1.1175\n","Batch 4800/5786, Loss: 1.0341\n","Batch 4900/5786, Loss: 0.0036\n","Batch 5000/5786, Loss: 0.7434\n","Batch 5100/5786, Loss: 0.0128\n","Batch 5200/5786, Loss: 0.0002\n","Batch 5300/5786, Loss: 0.0001\n","Batch 5400/5786, Loss: 0.2679\n","Batch 5500/5786, Loss: 0.0009\n","Batch 5600/5786, Loss: 0.0058\n","Batch 5700/5786, Loss: 0.3471\n","📈 Epoch 5/50, Average Loss: 0.3841\n","🎉 New best model! Loss: 0.3841\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 5...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 5 (loss: 0.3841)\n","\n","📊 Epoch 6/50\n","Batch 0/5786, Loss: 0.1860\n","Batch 100/5786, Loss: 1.4769\n","Batch 200/5786, Loss: 0.0004\n","Batch 300/5786, Loss: 0.0108\n","Batch 400/5786, Loss: 1.6664\n","Batch 500/5786, Loss: 1.4499\n","Batch 600/5786, Loss: 0.9222\n","Batch 700/5786, Loss: 0.0005\n","Batch 800/5786, Loss: 0.0483\n","Batch 900/5786, Loss: 0.0063\n","Batch 1000/5786, Loss: 0.0000\n","Batch 1100/5786, Loss: 0.2601\n","Batch 1200/5786, Loss: 0.7711\n","Batch 1300/5786, Loss: 0.0004\n","Batch 1400/5786, Loss: 0.0005\n","Batch 1500/5786, Loss: 0.0184\n","Batch 1600/5786, Loss: 0.4222\n","Batch 1700/5786, Loss: 0.8449\n","Batch 1800/5786, Loss: 0.3975\n","Batch 1900/5786, Loss: 0.1641\n","Batch 2000/5786, Loss: 0.4469\n","Batch 2100/5786, Loss: 0.0211\n","Batch 2200/5786, Loss: 0.0130\n","Batch 2300/5786, Loss: 1.3139\n","Batch 2400/5786, Loss: 0.0000\n","Batch 2500/5786, Loss: 0.1662\n","Batch 2600/5786, Loss: 0.0793\n","Batch 2700/5786, Loss: 0.6823\n","Batch 2800/5786, Loss: 0.0000\n","Batch 2900/5786, Loss: 0.7633\n","Batch 3000/5786, Loss: 0.5675\n","Batch 3100/5786, Loss: 0.6649\n","Batch 3200/5786, Loss: 0.0039\n","Batch 3300/5786, Loss: 0.7151\n","Batch 3400/5786, Loss: 0.0033\n","Batch 3500/5786, Loss: 0.8631\n","Batch 3600/5786, Loss: 0.0065\n","Batch 3700/5786, Loss: 0.3177\n","Batch 3800/5786, Loss: 0.3774\n","Batch 3900/5786, Loss: 0.0069\n","Batch 4000/5786, Loss: 0.8098\n","Batch 4100/5786, Loss: 0.0052\n","Batch 4200/5786, Loss: 0.5521\n","Batch 4300/5786, Loss: 0.0920\n","Batch 4400/5786, Loss: 0.5246\n","Batch 4500/5786, Loss: 0.1444\n","Batch 4600/5786, Loss: 0.0137\n","Batch 4700/5786, Loss: 0.4996\n","Batch 4800/5786, Loss: 1.4997\n","Batch 4900/5786, Loss: 0.9899\n","Batch 5000/5786, Loss: 0.2469\n","Batch 5100/5786, Loss: 1.3610\n","Batch 5200/5786, Loss: 0.0216\n","Batch 5300/5786, Loss: 0.0013\n","Batch 5400/5786, Loss: 0.0172\n","Batch 5500/5786, Loss: 0.1393\n","Batch 5600/5786, Loss: 0.0064\n","Batch 5700/5786, Loss: 1.1505\n","📈 Epoch 6/50, Average Loss: 0.3571\n","🎉 New best model! Loss: 0.3571\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 6...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 6 (loss: 0.3571)\n","\n","📊 Epoch 7/50\n","Batch 0/5786, Loss: 0.0031\n","Batch 100/5786, Loss: 0.1266\n","Batch 200/5786, Loss: 0.0005\n","Batch 300/5786, Loss: 0.0370\n","Batch 400/5786, Loss: 0.9853\n","Batch 500/5786, Loss: 0.0700\n","Batch 600/5786, Loss: 0.1192\n","Batch 700/5786, Loss: 0.2886\n","Batch 800/5786, Loss: 0.0001\n","Batch 900/5786, Loss: 0.0915\n","Batch 1000/5786, Loss: 0.0470\n","Batch 1100/5786, Loss: 0.0065\n","Batch 1200/5786, Loss: 0.6232\n","Batch 1300/5786, Loss: 0.0636\n","Batch 1400/5786, Loss: 0.0426\n","Batch 1500/5786, Loss: 1.0233\n","Batch 1600/5786, Loss: 0.0269\n","Batch 1700/5786, Loss: 1.3643\n","Batch 1800/5786, Loss: 0.7254\n","Batch 1900/5786, Loss: 0.2430\n","Batch 2000/5786, Loss: 0.0102\n","Batch 2100/5786, Loss: 0.0071\n","Batch 2200/5786, Loss: 2.3648\n","Batch 2300/5786, Loss: 0.0000\n","Batch 2400/5786, Loss: 0.7160\n","Batch 2500/5786, Loss: 0.2287\n","Batch 2600/5786, Loss: 0.2216\n","Batch 2700/5786, Loss: 0.0032\n","Batch 2800/5786, Loss: 0.7803\n","Batch 2900/5786, Loss: 0.4558\n","Batch 3000/5786, Loss: 0.0000\n","Batch 3100/5786, Loss: 0.1774\n","Batch 3200/5786, Loss: 0.0003\n","Batch 3300/5786, Loss: 0.2429\n","Batch 3400/5786, Loss: 0.7037\n","Batch 3500/5786, Loss: 0.0000\n","Batch 3600/5786, Loss: 0.0011\n","Batch 3700/5786, Loss: 0.0079\n","Batch 3800/5786, Loss: 0.0306\n","Batch 3900/5786, Loss: 0.1684\n","Batch 4000/5786, Loss: 0.0079\n","Batch 4100/5786, Loss: 0.6244\n","Batch 4200/5786, Loss: 0.0000\n","Batch 4300/5786, Loss: 0.0955\n","Batch 4400/5786, Loss: 0.0057\n","Batch 4500/5786, Loss: 0.0271\n","Batch 4600/5786, Loss: 0.4231\n","Batch 4700/5786, Loss: 0.0792\n","Batch 4800/5786, Loss: 0.1350\n","Batch 4900/5786, Loss: 0.9964\n","Batch 5000/5786, Loss: 0.0059\n","Batch 5100/5786, Loss: 0.0058\n","Batch 5200/5786, Loss: 0.0000\n","Batch 5300/5786, Loss: 0.0026\n","Batch 5400/5786, Loss: 0.0055\n","Batch 5500/5786, Loss: 0.5662\n","Batch 5600/5786, Loss: 0.2594\n","Batch 5700/5786, Loss: 0.0243\n","📈 Epoch 7/50, Average Loss: 0.3210\n","🎉 New best model! Loss: 0.3210\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 7...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 7 (loss: 0.3210)\n","\n","📊 Epoch 8/50\n","Batch 0/5786, Loss: 0.0000\n","Batch 100/5786, Loss: 0.0022\n","Batch 200/5786, Loss: 0.0004\n","Batch 300/5786, Loss: 0.4464\n","Batch 400/5786, Loss: 0.6115\n","Batch 500/5786, Loss: 0.0020\n","Batch 600/5786, Loss: 0.9061\n","Batch 700/5786, Loss: 1.9068\n","Batch 800/5786, Loss: 0.0120\n","Batch 900/5786, Loss: 0.4993\n","Batch 1000/5786, Loss: 0.6529\n","Batch 1100/5786, Loss: 0.0376\n","Batch 1200/5786, Loss: 0.1288\n","Batch 1300/5786, Loss: 0.0249\n","Batch 1400/5786, Loss: 0.7234\n","Batch 1500/5786, Loss: 0.2273\n","Batch 1600/5786, Loss: 0.2238\n","Batch 1700/5786, Loss: 0.0030\n","Batch 1800/5786, Loss: 0.0000\n","Batch 1900/5786, Loss: 0.0000\n","Batch 2000/5786, Loss: 0.0006\n","Batch 2100/5786, Loss: 0.0160\n","Batch 2200/5786, Loss: 0.5305\n","Batch 2300/5786, Loss: 0.0028\n","Batch 2400/5786, Loss: 0.0002\n","Batch 2500/5786, Loss: 0.0012\n","Batch 2600/5786, Loss: 0.4162\n","Batch 2700/5786, Loss: 0.0631\n","Batch 2800/5786, Loss: 0.0276\n","Batch 2900/5786, Loss: 0.0057\n","Batch 3000/5786, Loss: 0.0000\n","Batch 3100/5786, Loss: 0.0002\n","Batch 3200/5786, Loss: 0.7938\n","Batch 3300/5786, Loss: 0.2231\n","Batch 3400/5786, Loss: 0.8135\n","Batch 3500/5786, Loss: 1.4569\n","Batch 3600/5786, Loss: 0.0537\n","Batch 3700/5786, Loss: 0.0075\n","Batch 3800/5786, Loss: 0.2747\n","Batch 3900/5786, Loss: 0.0000\n","Batch 4000/5786, Loss: 0.0002\n","Batch 4100/5786, Loss: 0.1337\n","Batch 4200/5786, Loss: 0.2636\n","Batch 4300/5786, Loss: 0.0000\n","Batch 4400/5786, Loss: 0.0000\n","Batch 4500/5786, Loss: 0.6012\n","Batch 4600/5786, Loss: 0.0355\n","Batch 4700/5786, Loss: 0.2116\n","Batch 4800/5786, Loss: 0.0344\n","Batch 4900/5786, Loss: 0.0000\n","Batch 5000/5786, Loss: 0.7844\n","Batch 5100/5786, Loss: 0.0040\n","Batch 5200/5786, Loss: 0.9781\n","Batch 5300/5786, Loss: 0.0000\n","Batch 5400/5786, Loss: 0.7584\n","Batch 5500/5786, Loss: 0.0114\n","Batch 5600/5786, Loss: 0.0026\n","Batch 5700/5786, Loss: 0.0001\n","📈 Epoch 8/50, Average Loss: 0.2948\n","🎉 New best model! Loss: 0.2948\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 8...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 8 (loss: 0.2948)\n","\n","📊 Epoch 9/50\n","Batch 0/5786, Loss: 1.0870\n","Batch 100/5786, Loss: 0.0478\n","Batch 200/5786, Loss: 0.0355\n","Batch 300/5786, Loss: 0.0100\n","Batch 400/5786, Loss: 1.2977\n","Batch 500/5786, Loss: 0.0044\n","Batch 600/5786, Loss: 0.2494\n","Batch 700/5786, Loss: 0.4841\n","Batch 800/5786, Loss: 0.0009\n","Batch 900/5786, Loss: 0.0379\n","Batch 1000/5786, Loss: 0.0000\n","Batch 1100/5786, Loss: 0.0256\n","Batch 1200/5786, Loss: 0.1354\n","Batch 1300/5786, Loss: 0.3379\n","Batch 1400/5786, Loss: 0.7950\n","Batch 1500/5786, Loss: 0.0022\n","Batch 1600/5786, Loss: 0.0033\n","Batch 1700/5786, Loss: 0.0000\n","Batch 1800/5786, Loss: 0.6606\n","Batch 1900/5786, Loss: 0.0001\n","Batch 2000/5786, Loss: 0.5086\n","Batch 2100/5786, Loss: 0.0207\n","Batch 2200/5786, Loss: 1.5704\n","Batch 2300/5786, Loss: 0.3188\n","Batch 2400/5786, Loss: 0.0780\n","Batch 2500/5786, Loss: 0.0025\n","Batch 2600/5786, Loss: 0.3343\n","Batch 2700/5786, Loss: 0.0371\n","Batch 2800/5786, Loss: 0.0985\n","Batch 2900/5786, Loss: 0.0726\n","Batch 3000/5786, Loss: 0.0398\n","Batch 3100/5786, Loss: 1.0406\n","Batch 3200/5786, Loss: 0.1235\n","Batch 3300/5786, Loss: 0.2569\n","Batch 3400/5786, Loss: 0.6120\n","Batch 3500/5786, Loss: 0.0395\n","Batch 3600/5786, Loss: 0.0935\n","Batch 3700/5786, Loss: 0.0012\n","Batch 3800/5786, Loss: 0.0286\n","Batch 3900/5786, Loss: 0.0252\n","Batch 4000/5786, Loss: 0.0127\n","Batch 4100/5786, Loss: 0.0000\n","Batch 4200/5786, Loss: 0.0019\n","Batch 4300/5786, Loss: 2.3605\n","Batch 4400/5786, Loss: 0.0065\n","Batch 4500/5786, Loss: 0.2394\n","Batch 4600/5786, Loss: 0.9476\n","Batch 4700/5786, Loss: 0.9612\n","Batch 4800/5786, Loss: 0.3116\n","Batch 4900/5786, Loss: 0.2297\n","Batch 5000/5786, Loss: 0.0025\n","Batch 5100/5786, Loss: 0.1199\n","Batch 5200/5786, Loss: 0.0602\n","Batch 5300/5786, Loss: 0.3945\n","Batch 5400/5786, Loss: 0.3499\n","Batch 5500/5786, Loss: 0.0564\n","Batch 5600/5786, Loss: 0.0748\n","Batch 5700/5786, Loss: 0.0003\n","📈 Epoch 9/50, Average Loss: 0.2692\n","🎉 New best model! Loss: 0.2692\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 9...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 9 (loss: 0.2692)\n","\n","📊 Epoch 10/50\n","Batch 0/5786, Loss: 0.0001\n","Batch 100/5786, Loss: 0.0465\n","Batch 200/5786, Loss: 0.0000\n","Batch 300/5786, Loss: 1.1796\n","Batch 400/5786, Loss: 0.8479\n","Batch 500/5786, Loss: 0.5763\n","Batch 600/5786, Loss: 0.0760\n","Batch 700/5786, Loss: 0.7520\n","Batch 800/5786, Loss: 0.0046\n","Batch 900/5786, Loss: 0.0019\n","Batch 1000/5786, Loss: 0.0009\n","Batch 1100/5786, Loss: 0.0071\n","Batch 1200/5786, Loss: 0.7036\n","Batch 1300/5786, Loss: 0.6147\n","Batch 1400/5786, Loss: 0.2032\n","Batch 1500/5786, Loss: 0.3650\n","Batch 1600/5786, Loss: 0.0337\n","Batch 1700/5786, Loss: 0.0245\n","Batch 1800/5786, Loss: 0.0070\n","Batch 1900/5786, Loss: 0.0002\n","Batch 2000/5786, Loss: 0.0121\n","Batch 2100/5786, Loss: 0.0779\n","Batch 2200/5786, Loss: 0.0374\n","Batch 2300/5786, Loss: 0.7347\n","Batch 2400/5786, Loss: 0.0214\n","Batch 2500/5786, Loss: 0.0549\n","Batch 2600/5786, Loss: 0.0154\n","Batch 2700/5786, Loss: 0.0232\n","Batch 2800/5786, Loss: 0.0013\n","Batch 2900/5786, Loss: 0.0002\n","Batch 3000/5786, Loss: 0.0003\n","Batch 3100/5786, Loss: 0.0148\n","Batch 3200/5786, Loss: 0.0202\n","Batch 3300/5786, Loss: 1.2136\n","Batch 3400/5786, Loss: 0.0007\n","Batch 3500/5786, Loss: 0.0006\n","Batch 3600/5786, Loss: 0.1333\n","Batch 3700/5786, Loss: 0.9382\n","Batch 3800/5786, Loss: 0.0311\n","Batch 3900/5786, Loss: 0.6046\n","Batch 4000/5786, Loss: 0.1128\n","Batch 4100/5786, Loss: 0.7834\n","Batch 4200/5786, Loss: 0.0000\n","Batch 4300/5786, Loss: 0.0213\n","Batch 4400/5786, Loss: 0.3706\n","Batch 4500/5786, Loss: 0.0001\n","Batch 4600/5786, Loss: 0.0013\n","Batch 4700/5786, Loss: 0.4617\n","Batch 4800/5786, Loss: 0.0166\n","Batch 4900/5786, Loss: 1.0060\n","Batch 5000/5786, Loss: 0.2261\n","Batch 5100/5786, Loss: 0.1282\n","Batch 5200/5786, Loss: 0.0006\n","Batch 5300/5786, Loss: 0.5185\n","Batch 5400/5786, Loss: 0.0023\n","Batch 5500/5786, Loss: 0.0193\n","Batch 5600/5786, Loss: 0.8381\n","Batch 5700/5786, Loss: 0.5118\n","📈 Epoch 10/50, Average Loss: 0.2526\n","🎉 New best model! Loss: 0.2526\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 10...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 10 (loss: 0.2526)\n","\n","📊 Epoch 11/50\n","Batch 0/5786, Loss: 0.4993\n","Batch 100/5786, Loss: 0.1018\n","Batch 200/5786, Loss: 1.1063\n","Batch 300/5786, Loss: 0.0002\n","Batch 400/5786, Loss: 0.0012\n","Batch 500/5786, Loss: 1.5180\n","Batch 600/5786, Loss: 0.1301\n","Batch 700/5786, Loss: 0.0001\n","Batch 800/5786, Loss: 1.2481\n","Batch 900/5786, Loss: 0.7503\n","Batch 1000/5786, Loss: 0.6584\n","Batch 1100/5786, Loss: 0.0538\n","Batch 1200/5786, Loss: 0.0007\n","Batch 1300/5786, Loss: 0.1784\n","Batch 1400/5786, Loss: 0.0000\n","Batch 1500/5786, Loss: 0.0002\n","Batch 1600/5786, Loss: 0.5703\n","Batch 1700/5786, Loss: 0.5145\n","Batch 1800/5786, Loss: 0.2742\n","Batch 1900/5786, Loss: 0.0188\n","Batch 2000/5786, Loss: 0.0000\n","Batch 2100/5786, Loss: 0.5241\n","Batch 2200/5786, Loss: 0.0002\n","Batch 2300/5786, Loss: 0.0000\n","Batch 2400/5786, Loss: 0.0066\n","Batch 2500/5786, Loss: 0.3964\n","Batch 2600/5786, Loss: 0.0038\n","Batch 2700/5786, Loss: 1.0318\n","Batch 2800/5786, Loss: 0.4932\n","Batch 2900/5786, Loss: 0.8635\n","Batch 3000/5786, Loss: 0.0011\n","Batch 3100/5786, Loss: 0.7670\n","Batch 3200/5786, Loss: 0.1300\n","Batch 3300/5786, Loss: 0.0005\n","Batch 3400/5786, Loss: 0.5062\n","Batch 3500/5786, Loss: 0.2128\n","Batch 3600/5786, Loss: 0.0186\n","Batch 3700/5786, Loss: 0.5792\n","Batch 3800/5786, Loss: 0.2118\n","Batch 3900/5786, Loss: 0.1204\n","Batch 4000/5786, Loss: 0.0011\n","Batch 4100/5786, Loss: 0.2853\n","Batch 4200/5786, Loss: 0.0055\n","Batch 4300/5786, Loss: 0.4240\n","Batch 4400/5786, Loss: 0.1088\n","Batch 4500/5786, Loss: 1.1368\n","Batch 4600/5786, Loss: 0.7285\n","Batch 4700/5786, Loss: 1.9129\n","Batch 4800/5786, Loss: 0.0078\n","Batch 4900/5786, Loss: 0.0176\n","Batch 5000/5786, Loss: 0.0090\n","Batch 5100/5786, Loss: 0.0005\n","Batch 5200/5786, Loss: 0.5574\n","Batch 5300/5786, Loss: 0.0000\n","Batch 5400/5786, Loss: 0.1838\n","Batch 5500/5786, Loss: 0.0001\n","Batch 5600/5786, Loss: 1.2620\n","Batch 5700/5786, Loss: 1.1925\n","📈 Epoch 11/50, Average Loss: 0.2389\n","🎉 New best model! Loss: 0.2389\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 11...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 11 (loss: 0.2389)\n","\n","📊 Epoch 12/50\n","Batch 0/5786, Loss: 0.0001\n","Batch 100/5786, Loss: 0.2666\n","Batch 200/5786, Loss: 0.3866\n","Batch 300/5786, Loss: 0.0000\n","Batch 400/5786, Loss: 0.0077\n","Batch 500/5786, Loss: 0.0714\n","Batch 600/5786, Loss: 0.2530\n","Batch 700/5786, Loss: 0.6407\n","Batch 800/5786, Loss: 0.2013\n","Batch 900/5786, Loss: 0.0016\n","Batch 1000/5786, Loss: 0.0018\n","Batch 1100/5786, Loss: 0.0000\n","Batch 1200/5786, Loss: 0.5132\n","Batch 1300/5786, Loss: 0.0067\n","Batch 1400/5786, Loss: 0.6505\n","Batch 1500/5786, Loss: 0.0058\n","Batch 1600/5786, Loss: 0.6522\n","Batch 1700/5786, Loss: 0.0000\n","Batch 1800/5786, Loss: 0.0535\n","Batch 1900/5786, Loss: 0.0576\n","Batch 2000/5786, Loss: 0.0001\n","Batch 2100/5786, Loss: 0.0184\n","Batch 2200/5786, Loss: 0.0465\n","Batch 2300/5786, Loss: 0.3665\n","Batch 2400/5786, Loss: 0.0001\n","Batch 2500/5786, Loss: 1.6507\n","Batch 2600/5786, Loss: 0.9591\n","Batch 2700/5786, Loss: 0.0251\n","Batch 2800/5786, Loss: 0.4548\n","Batch 2900/5786, Loss: 0.0069\n","Batch 3000/5786, Loss: 0.1467\n","Batch 3100/5786, Loss: 0.1241\n","Batch 3200/5786, Loss: 0.0010\n","Batch 3300/5786, Loss: 0.0033\n","Batch 3400/5786, Loss: 1.0130\n","Batch 3500/5786, Loss: 0.1099\n","Batch 3600/5786, Loss: 0.2196\n","Batch 3700/5786, Loss: 0.0771\n","Batch 3800/5786, Loss: 0.4495\n","Batch 3900/5786, Loss: 0.3073\n","Batch 4000/5786, Loss: 0.1066\n","Batch 4100/5786, Loss: 0.0000\n","Batch 4200/5786, Loss: 0.7935\n","Batch 4300/5786, Loss: 0.0968\n","Batch 4400/5786, Loss: 0.4054\n","Batch 4500/5786, Loss: 0.3648\n","Batch 4600/5786, Loss: 0.0002\n","Batch 4700/5786, Loss: 1.3829\n","Batch 4800/5786, Loss: 0.0000\n","Batch 4900/5786, Loss: 0.0104\n","Batch 5000/5786, Loss: 0.0011\n","Batch 5100/5786, Loss: 0.9786\n","Batch 5200/5786, Loss: 0.0000\n","Batch 5300/5786, Loss: 0.0006\n","Batch 5400/5786, Loss: 0.0000\n","Batch 5500/5786, Loss: 0.0206\n","Batch 5600/5786, Loss: 0.0018\n","Batch 5700/5786, Loss: 0.0082\n","📈 Epoch 12/50, Average Loss: 0.2184\n","🎉 New best model! Loss: 0.2184\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 12...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 12 (loss: 0.2184)\n","\n","📊 Epoch 13/50\n","Batch 0/5786, Loss: 0.0009\n","Batch 100/5786, Loss: 0.2512\n","Batch 200/5786, Loss: 0.0263\n","Batch 300/5786, Loss: 0.0066\n","Batch 400/5786, Loss: 0.2439\n","Batch 500/5786, Loss: 0.0202\n","Batch 600/5786, Loss: 0.0291\n","Batch 700/5786, Loss: 0.0002\n","Batch 800/5786, Loss: 0.0992\n","Batch 900/5786, Loss: 0.0001\n","Batch 1000/5786, Loss: 0.0000\n","Batch 1100/5786, Loss: 0.0004\n","Batch 1200/5786, Loss: 0.0073\n","Batch 1300/5786, Loss: 0.0582\n","Batch 1400/5786, Loss: 2.4824\n","Batch 1500/5786, Loss: 0.0000\n","Batch 1600/5786, Loss: 0.0850\n","Batch 1700/5786, Loss: 0.0180\n","Batch 1800/5786, Loss: 0.0000\n","Batch 1900/5786, Loss: 0.0000\n","Batch 2000/5786, Loss: 0.0458\n","Batch 2100/5786, Loss: 0.1106\n","Batch 2200/5786, Loss: 0.0155\n","Batch 2300/5786, Loss: 0.0002\n","Batch 2400/5786, Loss: 0.3694\n","Batch 2500/5786, Loss: 0.0462\n","Batch 2600/5786, Loss: 0.1370\n","Batch 2700/5786, Loss: 0.2894\n","Batch 2800/5786, Loss: 0.0094\n","Batch 2900/5786, Loss: 0.0244\n","Batch 3000/5786, Loss: 0.0014\n","Batch 3100/5786, Loss: 0.0000\n","Batch 3200/5786, Loss: 0.0853\n","Batch 3300/5786, Loss: 0.6296\n","Batch 3400/5786, Loss: 0.0000\n","Batch 3500/5786, Loss: 0.0000\n","Batch 3600/5786, Loss: 0.2317\n","Batch 3700/5786, Loss: 0.0211\n","Batch 3800/5786, Loss: 0.0015\n","Batch 3900/5786, Loss: 0.0542\n","Batch 4000/5786, Loss: 0.1086\n","Batch 4100/5786, Loss: 0.2652\n","Batch 4200/5786, Loss: 1.0565\n","Batch 4300/5786, Loss: 0.0563\n","Batch 4400/5786, Loss: 0.0000\n","Batch 4500/5786, Loss: 0.0105\n","Batch 4600/5786, Loss: 0.6985\n","Batch 4700/5786, Loss: 0.2927\n","Batch 4800/5786, Loss: 0.0000\n","Batch 4900/5786, Loss: 0.0000\n","Batch 5000/5786, Loss: 0.0009\n","Batch 5100/5786, Loss: 0.0104\n","Batch 5200/5786, Loss: 0.2261\n","Batch 5300/5786, Loss: 0.6680\n","Batch 5400/5786, Loss: 0.0001\n","Batch 5500/5786, Loss: 0.0002\n","Batch 5600/5786, Loss: 0.0000\n","Batch 5700/5786, Loss: 0.7279\n","📈 Epoch 13/50, Average Loss: 0.2081\n","🎉 New best model! Loss: 0.2081\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 13...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 13 (loss: 0.2081)\n","\n","📊 Epoch 14/50\n","Batch 0/5786, Loss: 0.0818\n","Batch 100/5786, Loss: 0.0163\n","Batch 200/5786, Loss: 0.0665\n","Batch 300/5786, Loss: 0.0118\n","Batch 400/5786, Loss: 0.0065\n","Batch 500/5786, Loss: 0.6420\n","Batch 600/5786, Loss: 0.3194\n","Batch 700/5786, Loss: 0.0316\n","Batch 800/5786, Loss: 0.0001\n","Batch 900/5786, Loss: 0.0040\n","Batch 1000/5786, Loss: 0.1182\n","Batch 1100/5786, Loss: 0.0320\n","Batch 1200/5786, Loss: 0.0002\n","Batch 1300/5786, Loss: 0.0025\n","Batch 1400/5786, Loss: 0.0018\n","Batch 1500/5786, Loss: 0.0009\n","Batch 1600/5786, Loss: 0.0460\n","Batch 1700/5786, Loss: 0.0000\n","Batch 1800/5786, Loss: 0.0001\n","Batch 1900/5786, Loss: 0.0013\n","Batch 2000/5786, Loss: 0.0001\n","Batch 2100/5786, Loss: 0.1261\n","Batch 2200/5786, Loss: 0.0758\n","Batch 2300/5786, Loss: 0.0772\n","Batch 2400/5786, Loss: 0.4360\n","Batch 2500/5786, Loss: 0.0170\n","Batch 2600/5786, Loss: 0.0119\n","Batch 2700/5786, Loss: 0.5730\n","Batch 2800/5786, Loss: 0.0001\n","Batch 2900/5786, Loss: 0.0005\n","Batch 3000/5786, Loss: 0.0004\n","Batch 3100/5786, Loss: 0.6731\n","Batch 3200/5786, Loss: 0.0005\n","Batch 3300/5786, Loss: 0.0001\n","Batch 3400/5786, Loss: 0.0003\n","Batch 3500/5786, Loss: 0.2434\n","Batch 3600/5786, Loss: 0.0009\n","Batch 3700/5786, Loss: 0.4965\n","Batch 3800/5786, Loss: 0.0000\n","Batch 3900/5786, Loss: 0.7424\n","Batch 4000/5786, Loss: 0.0007\n","Batch 4100/5786, Loss: 0.1641\n","Batch 4200/5786, Loss: 0.0002\n","Batch 4300/5786, Loss: 0.0015\n","Batch 4400/5786, Loss: 0.0199\n","Batch 4500/5786, Loss: 0.0411\n","Batch 4600/5786, Loss: 0.0000\n","Batch 4700/5786, Loss: 1.0399\n","Batch 4800/5786, Loss: 0.0099\n","Batch 4900/5786, Loss: 0.0038\n","Batch 5000/5786, Loss: 0.1571\n","Batch 5100/5786, Loss: 0.0000\n","Batch 5200/5786, Loss: 1.0140\n","Batch 5300/5786, Loss: 0.0820\n","Batch 5400/5786, Loss: 0.0001\n","Batch 5500/5786, Loss: 0.0225\n","Batch 5600/5786, Loss: 0.0450\n","Batch 5700/5786, Loss: 0.0348\n","📈 Epoch 14/50, Average Loss: 0.1997\n","🎉 New best model! Loss: 0.1997\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 14...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 14 (loss: 0.1997)\n","\n","📊 Epoch 15/50\n","Batch 0/5786, Loss: 0.0015\n","Batch 100/5786, Loss: 0.0002\n","Batch 200/5786, Loss: 0.0003\n","Batch 300/5786, Loss: 0.0007\n","Batch 400/5786, Loss: 0.4089\n","Batch 500/5786, Loss: 0.0016\n","Batch 600/5786, Loss: 0.0000\n","Batch 700/5786, Loss: 0.0784\n","Batch 800/5786, Loss: 0.4619\n","Batch 900/5786, Loss: 0.0011\n","Batch 1000/5786, Loss: 0.0001\n","Batch 1100/5786, Loss: 0.0000\n","Batch 1200/5786, Loss: 0.2841\n","Batch 1300/5786, Loss: 0.0783\n","Batch 1400/5786, Loss: 0.3939\n","Batch 1500/5786, Loss: 0.0551\n","Batch 1600/5786, Loss: 0.1831\n","Batch 1700/5786, Loss: 0.0001\n","Batch 1800/5786, Loss: 0.1089\n","Batch 1900/5786, Loss: 0.0001\n","Batch 2000/5786, Loss: 0.0009\n","Batch 2100/5786, Loss: 0.0699\n","Batch 2200/5786, Loss: 0.0001\n","Batch 2300/5786, Loss: 0.0000\n","Batch 2400/5786, Loss: 0.0056\n","Batch 2500/5786, Loss: 0.0013\n","Batch 2600/5786, Loss: 0.0104\n","Batch 2700/5786, Loss: 0.0000\n","Batch 2800/5786, Loss: 0.0000\n","Batch 2900/5786, Loss: 0.0047\n","Batch 3000/5786, Loss: 0.3842\n","Batch 3100/5786, Loss: 0.0694\n","Batch 3200/5786, Loss: 0.0646\n","Batch 3300/5786, Loss: 0.3143\n","Batch 3400/5786, Loss: 0.0070\n","Batch 3500/5786, Loss: 0.0044\n","Batch 3600/5786, Loss: 0.0003\n","Batch 3700/5786, Loss: 0.0009\n","Batch 3800/5786, Loss: 0.0155\n","Batch 3900/5786, Loss: 0.0001\n","Batch 4000/5786, Loss: 0.0008\n","Batch 4100/5786, Loss: 0.0000\n","Batch 4200/5786, Loss: 0.0212\n","Batch 4300/5786, Loss: 0.5493\n","Batch 4400/5786, Loss: 0.5550\n","Batch 4500/5786, Loss: 0.2284\n","Batch 4600/5786, Loss: 0.0000\n","Batch 4700/5786, Loss: 0.0000\n","Batch 4800/5786, Loss: 0.0156\n","Batch 4900/5786, Loss: 0.2445\n","Batch 5000/5786, Loss: 0.0152\n","Batch 5100/5786, Loss: 0.1648\n","Batch 5200/5786, Loss: 0.2892\n","Batch 5300/5786, Loss: 0.0000\n","Batch 5400/5786, Loss: 0.7634\n","Batch 5500/5786, Loss: 0.0100\n","Batch 5600/5786, Loss: 0.0820\n","Batch 5700/5786, Loss: 0.3367\n","📈 Epoch 15/50, Average Loss: 0.1903\n","🎉 New best model! Loss: 0.1903\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 15...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 15 (loss: 0.1903)\n","\n","📊 Epoch 16/50\n","Batch 0/5786, Loss: 0.0137\n","Batch 100/5786, Loss: 0.0672\n","Batch 200/5786, Loss: 0.0077\n","Batch 300/5786, Loss: 0.0918\n","Batch 400/5786, Loss: 0.0000\n","Batch 500/5786, Loss: 0.0159\n","Batch 600/5786, Loss: 0.0622\n","Batch 700/5786, Loss: 0.0001\n","Batch 800/5786, Loss: 0.0074\n","Batch 900/5786, Loss: 0.0649\n","Batch 1000/5786, Loss: 0.1327\n","Batch 1100/5786, Loss: 0.0047\n","Batch 1200/5786, Loss: 0.0018\n","Batch 1300/5786, Loss: 0.0747\n","Batch 1400/5786, Loss: 0.1202\n","Batch 1500/5786, Loss: 0.0004\n","Batch 1600/5786, Loss: 0.0004\n","Batch 1700/5786, Loss: 0.0718\n","Batch 1800/5786, Loss: 0.0092\n","Batch 1900/5786, Loss: 0.0323\n","Batch 2000/5786, Loss: 0.1898\n","Batch 2100/5786, Loss: 0.0543\n","Batch 2200/5786, Loss: 0.0661\n","Batch 2300/5786, Loss: 0.0001\n","Batch 2400/5786, Loss: 0.0405\n","Batch 2500/5786, Loss: 0.5112\n","Batch 2600/5786, Loss: 0.1458\n","Batch 2700/5786, Loss: 0.0045\n","Batch 2800/5786, Loss: 0.0001\n","Batch 2900/5786, Loss: 0.0000\n","Batch 3000/5786, Loss: 0.6913\n","Batch 3100/5786, Loss: 0.0299\n","Batch 3200/5786, Loss: 0.7856\n","Batch 3300/5786, Loss: 0.0131\n","Batch 3400/5786, Loss: 1.2921\n","Batch 3500/5786, Loss: 0.2392\n","Batch 3600/5786, Loss: 0.0329\n","Batch 3700/5786, Loss: 0.0267\n","Batch 3800/5786, Loss: 0.1731\n","Batch 3900/5786, Loss: 0.0000\n","Batch 4000/5786, Loss: 0.0809\n","Batch 4100/5786, Loss: 0.1309\n","Batch 4200/5786, Loss: 0.0001\n","Batch 4300/5786, Loss: 0.0000\n","Batch 4400/5786, Loss: 0.0346\n","Batch 4500/5786, Loss: 0.0018\n","Batch 4600/5786, Loss: 0.0001\n","Batch 4700/5786, Loss: 0.0178\n","Batch 4800/5786, Loss: 0.0027\n","Batch 4900/5786, Loss: 0.0000\n","Batch 5000/5786, Loss: 0.0000\n","Batch 5100/5786, Loss: 0.0000\n","Batch 5200/5786, Loss: 0.0628\n","Batch 5300/5786, Loss: 0.0040\n","Batch 5400/5786, Loss: 0.1860\n","Batch 5500/5786, Loss: 0.0328\n","Batch 5600/5786, Loss: 1.8090\n","Batch 5700/5786, Loss: 0.0001\n","📈 Epoch 16/50, Average Loss: 0.1747\n","🎉 New best model! Loss: 0.1747\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 16...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 16 (loss: 0.1747)\n","\n","📊 Epoch 17/50\n","Batch 0/5786, Loss: 0.0040\n","Batch 100/5786, Loss: 0.7083\n","Batch 200/5786, Loss: 0.3731\n","Batch 300/5786, Loss: 0.0011\n","Batch 400/5786, Loss: 0.0881\n","Batch 500/5786, Loss: 0.1520\n","Batch 600/5786, Loss: 0.0590\n","Batch 700/5786, Loss: 0.0000\n","Batch 800/5786, Loss: 0.1410\n","Batch 900/5786, Loss: 0.0007\n","Batch 1000/5786, Loss: 0.0000\n","Batch 1100/5786, Loss: 0.0000\n","Batch 1200/5786, Loss: 0.1357\n","Batch 1300/5786, Loss: 0.0000\n","Batch 1400/5786, Loss: 0.0006\n","Batch 1500/5786, Loss: 0.4757\n","Batch 1600/5786, Loss: 0.3316\n","Batch 1700/5786, Loss: 0.0792\n","Batch 1800/5786, Loss: 0.0000\n","Batch 1900/5786, Loss: 0.0991\n","Batch 2000/5786, Loss: 0.0019\n","Batch 2100/5786, Loss: 0.0017\n","Batch 2200/5786, Loss: 0.1001\n","Batch 2300/5786, Loss: 0.0457\n","Batch 2400/5786, Loss: 0.1664\n","Batch 2500/5786, Loss: 0.0050\n","Batch 2600/5786, Loss: 0.1192\n","Batch 2700/5786, Loss: 0.0508\n","Batch 2800/5786, Loss: 0.0002\n","Batch 2900/5786, Loss: 0.0016\n","Batch 3000/5786, Loss: 0.8198\n","Batch 3100/5786, Loss: 0.0000\n","Batch 3200/5786, Loss: 0.0073\n","Batch 3300/5786, Loss: 0.1443\n","Batch 3400/5786, Loss: 0.0360\n","Batch 3500/5786, Loss: 0.5666\n","Batch 3600/5786, Loss: 0.0821\n","Batch 3700/5786, Loss: 0.0231\n","Batch 3800/5786, Loss: 0.0000\n","Batch 3900/5786, Loss: 0.0517\n","Batch 4000/5786, Loss: 0.0000\n","Batch 4100/5786, Loss: 0.0392\n","Batch 4200/5786, Loss: 0.0000\n","Batch 4300/5786, Loss: 0.6918\n","Batch 4400/5786, Loss: 0.4517\n","Batch 4500/5786, Loss: 0.0001\n","Batch 4600/5786, Loss: 0.0005\n","Batch 4700/5786, Loss: 1.3102\n","Batch 4800/5786, Loss: 1.5866\n","Batch 4900/5786, Loss: 0.0005\n","Batch 5000/5786, Loss: 0.0000\n","Batch 5100/5786, Loss: 0.2373\n","Batch 5200/5786, Loss: 0.0319\n","Batch 5300/5786, Loss: 0.4961\n","Batch 5400/5786, Loss: 0.0002\n","Batch 5500/5786, Loss: 0.0003\n","Batch 5600/5786, Loss: 0.0014\n","Batch 5700/5786, Loss: 0.0039\n","📈 Epoch 17/50, Average Loss: 0.1701\n","🎉 New best model! Loss: 0.1701\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 17...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 17 (loss: 0.1701)\n","\n","📊 Epoch 18/50\n","Batch 0/5786, Loss: 0.4064\n","Batch 100/5786, Loss: 0.1196\n","Batch 200/5786, Loss: 0.0007\n","Batch 300/5786, Loss: 0.2193\n","Batch 400/5786, Loss: 0.0294\n","Batch 500/5786, Loss: 0.0277\n","Batch 600/5786, Loss: 0.0014\n","Batch 700/5786, Loss: 0.3205\n","Batch 800/5786, Loss: 0.0002\n","Batch 900/5786, Loss: 0.3559\n","Batch 1000/5786, Loss: 0.0002\n","Batch 1100/5786, Loss: 0.0248\n","Batch 1200/5786, Loss: 0.0320\n","Batch 1300/5786, Loss: 0.0002\n","Batch 1400/5786, Loss: 0.3668\n","Batch 1500/5786, Loss: 0.0005\n","Batch 1600/5786, Loss: 0.0024\n","Batch 1700/5786, Loss: 0.0001\n","Batch 1800/5786, Loss: 0.0000\n","Batch 1900/5786, Loss: 0.0047\n","Batch 2000/5786, Loss: 0.0170\n","Batch 2100/5786, Loss: 0.0000\n","Batch 2200/5786, Loss: 0.0039\n","Batch 2300/5786, Loss: 0.0001\n","Batch 2400/5786, Loss: 0.0772\n","Batch 2500/5786, Loss: 0.0368\n","Batch 2600/5786, Loss: 0.0061\n","Batch 2700/5786, Loss: 0.0003\n","Batch 2800/5786, Loss: 0.0048\n","Batch 2900/5786, Loss: 0.3740\n","Batch 3000/5786, Loss: 0.5764\n","Batch 3100/5786, Loss: 0.0007\n","Batch 3200/5786, Loss: 0.1351\n","Batch 3300/5786, Loss: 0.1351\n","Batch 3400/5786, Loss: 0.0001\n","Batch 3500/5786, Loss: 0.0450\n","Batch 3600/5786, Loss: 0.0005\n","Batch 3700/5786, Loss: 0.0118\n","Batch 3800/5786, Loss: 0.0000\n","Batch 3900/5786, Loss: 0.1361\n","Batch 4000/5786, Loss: 0.1876\n","Batch 4100/5786, Loss: 0.0024\n","Batch 4200/5786, Loss: 0.0004\n","Batch 4300/5786, Loss: 0.0000\n","Batch 4400/5786, Loss: 0.0332\n","Batch 4500/5786, Loss: 0.0000\n","Batch 4600/5786, Loss: 0.1117\n","Batch 4700/5786, Loss: 0.0058\n","Batch 4800/5786, Loss: 0.0012\n","Batch 4900/5786, Loss: 0.0000\n","Batch 5000/5786, Loss: 0.0001\n","Batch 5100/5786, Loss: 0.4134\n","Batch 5200/5786, Loss: 0.5692\n","Batch 5300/5786, Loss: 0.2073\n","Batch 5400/5786, Loss: 0.6509\n","Batch 5500/5786, Loss: 0.2627\n","Batch 5600/5786, Loss: 0.0636\n","Batch 5700/5786, Loss: 1.7585\n","📈 Epoch 18/50, Average Loss: 0.1568\n","🎉 New best model! Loss: 0.1568\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 18...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 18 (loss: 0.1568)\n","\n","📊 Epoch 19/50\n","Batch 0/5786, Loss: 0.0000\n","Batch 100/5786, Loss: 0.0651\n","Batch 200/5786, Loss: 0.0001\n","Batch 300/5786, Loss: 1.3469\n","Batch 400/5786, Loss: 0.0000\n","Batch 500/5786, Loss: 0.0727\n","Batch 600/5786, Loss: 0.0193\n","Batch 700/5786, Loss: 0.2479\n","Batch 800/5786, Loss: 0.0402\n","Batch 900/5786, Loss: 0.0018\n","Batch 1000/5786, Loss: 0.3024\n","Batch 1100/5786, Loss: 0.0000\n","Batch 1200/5786, Loss: 1.4161\n","Batch 1300/5786, Loss: 0.0001\n","Batch 1400/5786, Loss: 0.0001\n","Batch 1500/5786, Loss: 0.0007\n","Batch 1600/5786, Loss: 0.0053\n","Batch 1700/5786, Loss: 0.0889\n","Batch 1800/5786, Loss: 0.0002\n","Batch 1900/5786, Loss: 0.2342\n","Batch 2000/5786, Loss: 0.0000\n","Batch 2100/5786, Loss: 0.4327\n","Batch 2200/5786, Loss: 0.0123\n","Batch 2300/5786, Loss: 0.9645\n","Batch 2400/5786, Loss: 0.0001\n","Batch 2500/5786, Loss: 0.0000\n","Batch 2600/5786, Loss: 0.1736\n","Batch 2700/5786, Loss: 0.0031\n","Batch 2800/5786, Loss: 0.0000\n","Batch 2900/5786, Loss: 0.0917\n","Batch 3000/5786, Loss: 1.3012\n","Batch 3100/5786, Loss: 0.0000\n","Batch 3200/5786, Loss: 0.7450\n","Batch 3300/5786, Loss: 0.0000\n","Batch 3400/5786, Loss: 0.0000\n","Batch 3500/5786, Loss: 0.5334\n","Batch 3600/5786, Loss: 0.0255\n","Batch 3700/5786, Loss: 0.0002\n","Batch 3800/5786, Loss: 0.3906\n","Batch 3900/5786, Loss: 0.1325\n","Batch 4000/5786, Loss: 0.0006\n","Batch 4100/5786, Loss: 0.0000\n","Batch 4200/5786, Loss: 0.1359\n","Batch 4300/5786, Loss: 0.0003\n","Batch 4400/5786, Loss: 0.0000\n","Batch 4500/5786, Loss: 0.6588\n","Batch 4600/5786, Loss: 0.0002\n","Batch 4700/5786, Loss: 0.0000\n","Batch 4800/5786, Loss: 0.0682\n","Batch 4900/5786, Loss: 0.9735\n","Batch 5000/5786, Loss: 0.0056\n","Batch 5100/5786, Loss: 0.0624\n","Batch 5200/5786, Loss: 0.5368\n","Batch 5300/5786, Loss: 0.0072\n","Batch 5400/5786, Loss: 0.0536\n","Batch 5500/5786, Loss: 0.0001\n","Batch 5600/5786, Loss: 0.7834\n","Batch 5700/5786, Loss: 0.1106\n","📈 Epoch 19/50, Average Loss: 0.1532\n","🎉 New best model! Loss: 0.1532\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 19...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 19 (loss: 0.1532)\n","\n","📊 Epoch 20/50\n","Batch 0/5786, Loss: 0.1687\n","Batch 100/5786, Loss: 0.0043\n","Batch 200/5786, Loss: 0.0000\n","Batch 300/5786, Loss: 0.0000\n","Batch 400/5786, Loss: 0.3102\n","Batch 500/5786, Loss: 0.5938\n","Batch 600/5786, Loss: 0.1536\n","Batch 700/5786, Loss: 0.7271\n","Batch 800/5786, Loss: 0.0067\n","Batch 900/5786, Loss: 0.0001\n","Batch 1000/5786, Loss: 0.0358\n","Batch 1100/5786, Loss: 0.0012\n","Batch 1200/5786, Loss: 0.0032\n","Batch 1300/5786, Loss: 0.0000\n","Batch 1400/5786, Loss: 0.2540\n","Batch 1500/5786, Loss: 0.0889\n","Batch 1600/5786, Loss: 0.8452\n","Batch 1700/5786, Loss: 0.0010\n","Batch 1800/5786, Loss: 0.1011\n","Batch 1900/5786, Loss: 0.0060\n","Batch 2000/5786, Loss: 0.0711\n","Batch 2100/5786, Loss: 0.0002\n","Batch 2200/5786, Loss: 0.2675\n","Batch 2300/5786, Loss: 0.2261\n","Batch 2400/5786, Loss: 0.0011\n","Batch 2500/5786, Loss: 0.0780\n","Batch 2600/5786, Loss: 0.0043\n","Batch 2700/5786, Loss: 0.2177\n","Batch 2800/5786, Loss: 0.0003\n","Batch 2900/5786, Loss: 0.3009\n","Batch 3000/5786, Loss: 0.0002\n","Batch 3100/5786, Loss: 0.0057\n","Batch 3200/5786, Loss: 0.2383\n","Batch 3300/5786, Loss: 0.0001\n","Batch 3400/5786, Loss: 0.0026\n","Batch 3500/5786, Loss: 0.4602\n","Batch 3600/5786, Loss: 0.0109\n","Batch 3700/5786, Loss: 0.0557\n","Batch 3800/5786, Loss: 0.0128\n","Batch 3900/5786, Loss: 0.2625\n","Batch 4000/5786, Loss: 0.0029\n","Batch 4100/5786, Loss: 0.3488\n","Batch 4200/5786, Loss: 0.4072\n","Batch 4300/5786, Loss: 0.0000\n","Batch 4400/5786, Loss: 0.0001\n","Batch 4500/5786, Loss: 0.0020\n","Batch 4600/5786, Loss: 0.0008\n","Batch 4700/5786, Loss: 0.7762\n","Batch 4800/5786, Loss: 1.3057\n","Batch 4900/5786, Loss: 0.0151\n","Batch 5000/5786, Loss: 0.0011\n","Batch 5100/5786, Loss: 0.0591\n","Batch 5200/5786, Loss: 0.0024\n","Batch 5300/5786, Loss: 0.1233\n","Batch 5400/5786, Loss: 0.0015\n","Batch 5500/5786, Loss: 0.0001\n","Batch 5600/5786, Loss: 0.0002\n","Batch 5700/5786, Loss: 0.0089\n","📈 Epoch 20/50, Average Loss: 0.1403\n","🎉 New best model! Loss: 0.1403\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 20...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 20 (loss: 0.1403)\n","\n","📊 Epoch 21/50\n","Batch 0/5786, Loss: 0.0121\n","Batch 100/5786, Loss: 0.0143\n","Batch 200/5786, Loss: 0.0005\n","Batch 300/5786, Loss: 0.0000\n","Batch 400/5786, Loss: 0.5049\n","Batch 500/5786, Loss: 0.0246\n","Batch 600/5786, Loss: 0.6861\n","Batch 700/5786, Loss: 0.0000\n","Batch 800/5786, Loss: 0.0125\n","Batch 900/5786, Loss: 0.0651\n","Batch 1000/5786, Loss: 0.0103\n","Batch 1100/5786, Loss: 0.0000\n","Batch 1200/5786, Loss: 0.0007\n","Batch 1300/5786, Loss: 0.0000\n","Batch 1400/5786, Loss: 0.0036\n","Batch 1500/5786, Loss: 0.0471\n","Batch 1600/5786, Loss: 0.0000\n","Batch 1700/5786, Loss: 0.0726\n","Batch 1800/5786, Loss: 0.4933\n","Batch 1900/5786, Loss: 0.0001\n","Batch 2000/5786, Loss: 0.0168\n","Batch 2100/5786, Loss: 0.0000\n","Batch 2200/5786, Loss: 0.0002\n","Batch 2300/5786, Loss: 0.0002\n","Batch 2400/5786, Loss: 0.0555\n","Batch 2500/5786, Loss: 0.0000\n","Batch 2600/5786, Loss: 0.0001\n","Batch 2700/5786, Loss: 0.0007\n","Batch 2800/5786, Loss: 0.0809\n","Batch 2900/5786, Loss: 0.2460\n","Batch 3000/5786, Loss: 0.0001\n","Batch 3100/5786, Loss: 0.0167\n","Batch 3200/5786, Loss: 0.0015\n","Batch 3300/5786, Loss: 0.0168\n","Batch 3400/5786, Loss: 0.0001\n","Batch 3500/5786, Loss: 0.0266\n","Batch 3600/5786, Loss: 0.0635\n","Batch 3700/5786, Loss: 0.0000\n","Batch 3800/5786, Loss: 0.0025\n","Batch 3900/5786, Loss: 0.5044\n","Batch 4000/5786, Loss: 0.0000\n","Batch 4100/5786, Loss: 0.0000\n","Batch 4200/5786, Loss: 0.0297\n","Batch 4300/5786, Loss: 0.0000\n","Batch 4400/5786, Loss: 0.0167\n","Batch 4500/5786, Loss: 0.0000\n","Batch 4600/5786, Loss: 0.0038\n","Batch 4700/5786, Loss: 0.0014\n","Batch 4800/5786, Loss: 0.8291\n","Batch 4900/5786, Loss: 0.0000\n","Batch 5000/5786, Loss: 0.0000\n","Batch 5100/5786, Loss: 0.1493\n","Batch 5200/5786, Loss: 1.1535\n","Batch 5300/5786, Loss: 0.0018\n","Batch 5400/5786, Loss: 0.0016\n","Batch 5500/5786, Loss: 0.0595\n","Batch 5600/5786, Loss: 1.3697\n","Batch 5700/5786, Loss: 0.0096\n","📈 Epoch 21/50, Average Loss: 0.1325\n","🎉 New best model! Loss: 0.1325\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 21...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 21 (loss: 0.1325)\n","\n","📊 Epoch 22/50\n","Batch 0/5786, Loss: 0.1159\n","Batch 100/5786, Loss: 0.2916\n","Batch 200/5786, Loss: 0.0000\n","Batch 300/5786, Loss: 0.0003\n","Batch 400/5786, Loss: 0.0003\n","Batch 500/5786, Loss: 0.1037\n","Batch 600/5786, Loss: 0.5781\n","Batch 700/5786, Loss: 0.0006\n","Batch 800/5786, Loss: 0.0001\n","Batch 900/5786, Loss: 0.0000\n","Batch 1000/5786, Loss: 0.2332\n","Batch 1100/5786, Loss: 0.0241\n","Batch 1200/5786, Loss: 0.0006\n","Batch 1300/5786, Loss: 0.0000\n","Batch 1400/5786, Loss: 0.0000\n","Batch 1500/5786, Loss: 0.0166\n","Batch 1600/5786, Loss: 0.2648\n","Batch 1700/5786, Loss: 0.0844\n","Batch 1800/5786, Loss: 0.0001\n","Batch 1900/5786, Loss: 0.0033\n","Batch 2000/5786, Loss: 0.6445\n","Batch 2100/5786, Loss: 0.0006\n","Batch 2200/5786, Loss: 0.0000\n","Batch 2300/5786, Loss: 0.0000\n","Batch 2400/5786, Loss: 0.0176\n","Batch 2500/5786, Loss: 0.0000\n","Batch 2600/5786, Loss: 0.2762\n","Batch 2700/5786, Loss: 0.1387\n","Batch 2800/5786, Loss: 0.0002\n","Batch 2900/5786, Loss: 0.0001\n","Batch 3000/5786, Loss: 0.0009\n","Batch 3100/5786, Loss: 0.1119\n","Batch 3200/5786, Loss: 0.0000\n","Batch 3300/5786, Loss: 0.0000\n","Batch 3400/5786, Loss: 0.0371\n","Batch 3500/5786, Loss: 1.2797\n","Batch 3600/5786, Loss: 0.0132\n","Batch 3700/5786, Loss: 0.0904\n","Batch 3800/5786, Loss: 0.0062\n","Batch 3900/5786, Loss: 0.0401\n","Batch 4000/5786, Loss: 0.0053\n","Batch 4100/5786, Loss: 0.0001\n","Batch 4200/5786, Loss: 0.0000\n","Batch 4300/5786, Loss: 0.0000\n","Batch 4400/5786, Loss: 0.1730\n","Batch 4500/5786, Loss: 0.0000\n","Batch 4600/5786, Loss: 0.0009\n","Batch 4700/5786, Loss: 0.0401\n","Batch 4800/5786, Loss: 0.1164\n","Batch 4900/5786, Loss: 0.0004\n","Batch 5000/5786, Loss: 0.4910\n","Batch 5100/5786, Loss: 0.1932\n","Batch 5200/5786, Loss: 0.0031\n","Batch 5300/5786, Loss: 0.0379\n","Batch 5400/5786, Loss: 0.0000\n","Batch 5500/5786, Loss: 0.0450\n","Batch 5600/5786, Loss: 0.0000\n","Batch 5700/5786, Loss: 0.0001\n","📈 Epoch 22/50, Average Loss: 0.1320\n","🎉 New best model! Loss: 0.1320\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 22...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 22 (loss: 0.1320)\n","\n","📊 Epoch 23/50\n","Batch 0/5786, Loss: 0.0053\n","Batch 100/5786, Loss: 0.0011\n","Batch 200/5786, Loss: 0.0160\n","Batch 300/5786, Loss: 0.0296\n","Batch 400/5786, Loss: 0.0086\n","Batch 500/5786, Loss: 0.0000\n","Batch 600/5786, Loss: 0.0205\n","Batch 700/5786, Loss: 0.0001\n","Batch 800/5786, Loss: 0.2415\n","Batch 900/5786, Loss: 0.0000\n","Batch 1000/5786, Loss: 0.0210\n","Batch 1100/5786, Loss: 0.0041\n","Batch 1200/5786, Loss: 0.0313\n","Batch 1300/5786, Loss: 0.5890\n","Batch 1400/5786, Loss: 0.0204\n","Batch 1500/5786, Loss: 0.0082\n","Batch 1600/5786, Loss: 0.0001\n","Batch 1700/5786, Loss: 0.0011\n","Batch 1800/5786, Loss: 0.3403\n","Batch 1900/5786, Loss: 0.0025\n","Batch 2000/5786, Loss: 0.3881\n","Batch 2100/5786, Loss: 0.0199\n","Batch 2200/5786, Loss: 0.0000\n","Batch 2300/5786, Loss: 0.0004\n","Batch 2400/5786, Loss: 0.0000\n","Batch 2500/5786, Loss: 0.0053\n","Batch 2600/5786, Loss: 0.0223\n","Batch 2700/5786, Loss: 0.9281\n","Batch 2800/5786, Loss: 0.0000\n","Batch 2900/5786, Loss: 0.0001\n","Batch 3000/5786, Loss: 0.9321\n","Batch 3100/5786, Loss: 0.0075\n","Batch 3200/5786, Loss: 0.0009\n","Batch 3300/5786, Loss: 0.0000\n","Batch 3400/5786, Loss: 0.0000\n","Batch 3500/5786, Loss: 0.0031\n","Batch 3600/5786, Loss: 0.0000\n","Batch 3700/5786, Loss: 0.0000\n","Batch 3800/5786, Loss: 0.0190\n","Batch 3900/5786, Loss: 0.0001\n","Batch 4000/5786, Loss: 0.0003\n","Batch 4100/5786, Loss: 0.1277\n","Batch 4200/5786, Loss: 0.2784\n","Batch 4300/5786, Loss: 0.0002\n","Batch 4400/5786, Loss: 0.0919\n","Batch 4500/5786, Loss: 0.0026\n","Batch 4600/5786, Loss: 0.0155\n","Batch 4700/5786, Loss: 0.2754\n","Batch 4800/5786, Loss: 1.6247\n","Batch 4900/5786, Loss: 0.2585\n","Batch 5000/5786, Loss: 0.0060\n","Batch 5100/5786, Loss: 0.0407\n","Batch 5200/5786, Loss: 0.0000\n","Batch 5300/5786, Loss: 0.0006\n","Batch 5400/5786, Loss: 0.0019\n","Batch 5500/5786, Loss: 0.0023\n","Batch 5600/5786, Loss: 0.0001\n","Batch 5700/5786, Loss: 0.0000\n","📈 Epoch 23/50, Average Loss: 0.1222\n","🎉 New best model! Loss: 0.1222\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 23...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 23 (loss: 0.1222)\n","\n","📊 Epoch 24/50\n","Batch 0/5786, Loss: 0.0006\n","Batch 100/5786, Loss: 0.0000\n","Batch 200/5786, Loss: 0.0150\n","Batch 300/5786, Loss: 0.0009\n","Batch 400/5786, Loss: 0.0191\n","Batch 500/5786, Loss: 0.0033\n","Batch 600/5786, Loss: 0.2278\n","Batch 700/5786, Loss: 0.0099\n","Batch 800/5786, Loss: 0.0028\n","Batch 900/5786, Loss: 0.0101\n","Batch 1000/5786, Loss: 0.0004\n","Batch 1100/5786, Loss: 0.0000\n","Batch 1200/5786, Loss: 0.0001\n","Batch 1300/5786, Loss: 0.0000\n","Batch 1400/5786, Loss: 0.4155\n","Batch 1500/5786, Loss: 0.3140\n","Batch 1600/5786, Loss: 0.0087\n","Batch 1700/5786, Loss: 0.0001\n","Batch 1800/5786, Loss: 0.0322\n","Batch 1900/5786, Loss: 0.0031\n","Batch 2000/5786, Loss: 0.0385\n","Batch 2100/5786, Loss: 1.3948\n","Batch 2200/5786, Loss: 0.0080\n","Batch 2300/5786, Loss: 0.0137\n","Batch 2400/5786, Loss: 0.0298\n","Batch 2500/5786, Loss: 0.0030\n","Batch 2600/5786, Loss: 0.0003\n","Batch 2700/5786, Loss: 0.0000\n","Batch 2800/5786, Loss: 0.2378\n","Batch 2900/5786, Loss: 0.0024\n","Batch 3000/5786, Loss: 0.0002\n","Batch 3100/5786, Loss: 0.6148\n","Batch 3200/5786, Loss: 0.0239\n","Batch 3300/5786, Loss: 0.2525\n","Batch 3400/5786, Loss: 0.0000\n","Batch 3500/5786, Loss: 0.1257\n","Batch 3600/5786, Loss: 0.7860\n","Batch 3700/5786, Loss: 0.2131\n","Batch 3800/5786, Loss: 0.1420\n","Batch 3900/5786, Loss: 0.4413\n","Batch 4000/5786, Loss: 0.0001\n","Batch 4100/5786, Loss: 0.1195\n","Batch 4200/5786, Loss: 0.0003\n","Batch 4300/5786, Loss: 0.0063\n","Batch 4400/5786, Loss: 0.0053\n","Batch 4500/5786, Loss: 0.0059\n","Batch 4600/5786, Loss: 0.0000\n","Batch 4700/5786, Loss: 1.0675\n","Batch 4800/5786, Loss: 0.0006\n","Batch 4900/5786, Loss: 0.1181\n","Batch 5000/5786, Loss: 0.0000\n","Batch 5100/5786, Loss: 0.4444\n","Batch 5200/5786, Loss: 0.5877\n","Batch 5300/5786, Loss: 0.0002\n","Batch 5400/5786, Loss: 0.2866\n","Batch 5500/5786, Loss: 0.0000\n","Batch 5600/5786, Loss: 0.0086\n","Batch 5700/5786, Loss: 0.0017\n","📈 Epoch 24/50, Average Loss: 0.1166\n","🎉 New best model! Loss: 0.1166\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 24...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 24 (loss: 0.1166)\n","\n","📊 Epoch 25/50\n","Batch 0/5786, Loss: 0.0600\n","Batch 100/5786, Loss: 0.0990\n","Batch 200/5786, Loss: 0.2039\n","Batch 300/5786, Loss: 0.0000\n","Batch 400/5786, Loss: 0.0000\n","Batch 500/5786, Loss: 0.0006\n","Batch 600/5786, Loss: 0.0228\n","Batch 700/5786, Loss: 0.4170\n","Batch 800/5786, Loss: 0.0000\n","Batch 900/5786, Loss: 0.0941\n","Batch 1000/5786, Loss: 0.0000\n","Batch 1100/5786, Loss: 0.0003\n","Batch 1200/5786, Loss: 0.0039\n","Batch 1300/5786, Loss: 0.0197\n","Batch 1400/5786, Loss: 0.0318\n","Batch 1500/5786, Loss: 0.0030\n","Batch 1600/5786, Loss: 0.0000\n","Batch 1700/5786, Loss: 1.0128\n","Batch 1800/5786, Loss: 0.0523\n","Batch 1900/5786, Loss: 0.0083\n","Batch 2000/5786, Loss: 0.1463\n","Batch 2100/5786, Loss: 0.0000\n","Batch 2200/5786, Loss: 0.2361\n","Batch 2300/5786, Loss: 0.0000\n","Batch 2400/5786, Loss: 0.0000\n","Batch 2500/5786, Loss: 0.0031\n","Batch 2600/5786, Loss: 0.0753\n","Batch 2700/5786, Loss: 0.0000\n","Batch 2800/5786, Loss: 0.1376\n","Batch 2900/5786, Loss: 0.0001\n","Batch 3000/5786, Loss: 0.0281\n","Batch 3100/5786, Loss: 0.0000\n","Batch 3200/5786, Loss: 0.0116\n","Batch 3300/5786, Loss: 0.0026\n","Batch 3400/5786, Loss: 0.0007\n","Batch 3500/5786, Loss: 0.0010\n","Batch 3600/5786, Loss: 0.0001\n","Batch 3700/5786, Loss: 0.0167\n","Batch 3800/5786, Loss: 0.0000\n","Batch 3900/5786, Loss: 0.0000\n","Batch 4000/5786, Loss: 0.0006\n","Batch 4100/5786, Loss: 0.0284\n","Batch 4200/5786, Loss: 0.0228\n","Batch 4300/5786, Loss: 0.0001\n","Batch 4400/5786, Loss: 0.0024\n","Batch 4500/5786, Loss: 0.0000\n","Batch 4600/5786, Loss: 0.0000\n","Batch 4700/5786, Loss: 0.3460\n","Batch 4800/5786, Loss: 0.0000\n","Batch 4900/5786, Loss: 0.0000\n","Batch 5000/5786, Loss: 0.0000\n","Batch 5100/5786, Loss: 0.0000\n","Batch 5200/5786, Loss: 0.0104\n","Batch 5300/5786, Loss: 0.4358\n","Batch 5400/5786, Loss: 0.3674\n","Batch 5500/5786, Loss: 0.0346\n","Batch 5600/5786, Loss: 0.0001\n","Batch 5700/5786, Loss: 0.0190\n","📈 Epoch 25/50, Average Loss: 0.1100\n","🎉 New best model! Loss: 0.1100\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 25...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 25 (loss: 0.1100)\n","\n","📊 Epoch 26/50\n","Batch 0/5786, Loss: 0.0047\n","Batch 100/5786, Loss: 0.0000\n","Batch 200/5786, Loss: 0.0585\n","Batch 300/5786, Loss: 0.0006\n","Batch 400/5786, Loss: 0.0000\n","Batch 500/5786, Loss: 0.0008\n","Batch 600/5786, Loss: 0.0044\n","Batch 700/5786, Loss: 0.0239\n","Batch 800/5786, Loss: 0.0291\n","Batch 900/5786, Loss: 0.0000\n","Batch 1000/5786, Loss: 0.0000\n","Batch 1100/5786, Loss: 0.1353\n","Batch 1200/5786, Loss: 0.0000\n","Batch 1300/5786, Loss: 0.0001\n","Batch 1400/5786, Loss: 0.0061\n","Batch 1500/5786, Loss: 0.1889\n","Batch 1600/5786, Loss: 0.0000\n","Batch 1700/5786, Loss: 0.0000\n","Batch 1800/5786, Loss: 0.0000\n","Batch 1900/5786, Loss: 0.0019\n","Batch 2000/5786, Loss: 0.7466\n","Batch 2100/5786, Loss: 0.0000\n","Batch 2200/5786, Loss: 0.0000\n","Batch 2300/5786, Loss: 0.0289\n","Batch 2400/5786, Loss: 0.0517\n","Batch 2500/5786, Loss: 0.1543\n","Batch 2600/5786, Loss: 0.0925\n","Batch 2700/5786, Loss: 0.3509\n","Batch 2800/5786, Loss: 0.0000\n","Batch 2900/5786, Loss: 0.0001\n","Batch 3000/5786, Loss: 0.0000\n","Batch 3100/5786, Loss: 0.0000\n","Batch 3200/5786, Loss: 0.2260\n","Batch 3300/5786, Loss: 0.0232\n","Batch 3400/5786, Loss: 0.0165\n","Batch 3500/5786, Loss: 0.0001\n","Batch 3600/5786, Loss: 0.0000\n","Batch 3700/5786, Loss: 0.0757\n","Batch 3800/5786, Loss: 0.0001\n","Batch 3900/5786, Loss: 0.0001\n","Batch 4000/5786, Loss: 0.0097\n","Batch 4100/5786, Loss: 0.0277\n","Batch 4200/5786, Loss: 0.0001\n","Batch 4300/5786, Loss: 0.7318\n","Batch 4400/5786, Loss: 0.0041\n","Batch 4500/5786, Loss: 0.0002\n","Batch 4600/5786, Loss: 0.0009\n","Batch 4700/5786, Loss: 0.0763\n","Batch 4800/5786, Loss: 0.0001\n","Batch 4900/5786, Loss: 0.0441\n","Batch 5000/5786, Loss: 0.0002\n","Batch 5100/5786, Loss: 0.0000\n","Batch 5200/5786, Loss: 0.0054\n","Batch 5300/5786, Loss: 0.0000\n","Batch 5400/5786, Loss: 0.0545\n","Batch 5500/5786, Loss: 0.0000\n","Batch 5600/5786, Loss: 0.6812\n","Batch 5700/5786, Loss: 0.2250\n","📈 Epoch 26/50, Average Loss: 0.1064\n","🎉 New best model! Loss: 0.1064\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 26...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 26 (loss: 0.1064)\n","\n","📊 Epoch 27/50\n","Batch 0/5786, Loss: 0.2360\n","Batch 100/5786, Loss: 0.0016\n","Batch 200/5786, Loss: 0.0736\n","Batch 300/5786, Loss: 0.0001\n","Batch 400/5786, Loss: 0.0000\n","Batch 500/5786, Loss: 1.9229\n","Batch 600/5786, Loss: 0.0242\n","Batch 700/5786, Loss: 0.0453\n","Batch 800/5786, Loss: 0.0021\n","Batch 900/5786, Loss: 0.1088\n","Batch 1000/5786, Loss: 0.0025\n","Batch 1100/5786, Loss: 0.0000\n","Batch 1200/5786, Loss: 0.0106\n","Batch 1300/5786, Loss: 0.4214\n","Batch 1400/5786, Loss: 0.0000\n","Batch 1500/5786, Loss: 0.0714\n","Batch 1600/5786, Loss: 0.0000\n","Batch 1700/5786, Loss: 0.0000\n","Batch 1800/5786, Loss: 0.0002\n","Batch 1900/5786, Loss: 0.0000\n","Batch 2000/5786, Loss: 0.1121\n","Batch 2100/5786, Loss: 0.0012\n","Batch 2200/5786, Loss: 0.0008\n","Batch 2300/5786, Loss: 0.0000\n","Batch 2400/5786, Loss: 0.0000\n","Batch 2500/5786, Loss: 0.0031\n","Batch 2600/5786, Loss: 0.0000\n","Batch 2700/5786, Loss: 0.0001\n","Batch 2800/5786, Loss: 0.0000\n","Batch 2900/5786, Loss: 0.0061\n","Batch 3000/5786, Loss: 1.0236\n","Batch 3100/5786, Loss: 0.0003\n","Batch 3200/5786, Loss: 0.0001\n","Batch 3300/5786, Loss: 0.0004\n","Batch 3400/5786, Loss: 0.0000\n","Batch 3500/5786, Loss: 0.0046\n","Batch 3600/5786, Loss: 0.2371\n","Batch 3700/5786, Loss: 0.0267\n","Batch 3800/5786, Loss: 0.3364\n","Batch 3900/5786, Loss: 0.0049\n","Batch 4000/5786, Loss: 0.0000\n","Batch 4100/5786, Loss: 0.0001\n","Batch 4200/5786, Loss: 0.0001\n","Batch 4300/5786, Loss: 1.2172\n","Batch 4400/5786, Loss: 0.0035\n","Batch 4500/5786, Loss: 0.0314\n","Batch 4600/5786, Loss: 0.0159\n","Batch 4700/5786, Loss: 0.0072\n","Batch 4800/5786, Loss: 0.0003\n","Batch 4900/5786, Loss: 0.8400\n","Batch 5000/5786, Loss: 0.0001\n","Batch 5100/5786, Loss: 0.0251\n","Batch 5200/5786, Loss: 0.0132\n","Batch 5300/5786, Loss: 0.0064\n","Batch 5400/5786, Loss: 0.0228\n","Batch 5500/5786, Loss: 0.0005\n","Batch 5600/5786, Loss: 0.0023\n","Batch 5700/5786, Loss: 0.0024\n","📈 Epoch 27/50, Average Loss: 0.1040\n","🎉 New best model! Loss: 0.1040\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 27...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 27 (loss: 0.1040)\n","\n","📊 Epoch 28/50\n","Batch 0/5786, Loss: 0.5317\n","Batch 100/5786, Loss: 0.0004\n","Batch 200/5786, Loss: 0.2951\n","Batch 300/5786, Loss: 0.0037\n","Batch 400/5786, Loss: 0.0001\n","Batch 500/5786, Loss: 0.0512\n","Batch 600/5786, Loss: 0.1446\n","Batch 700/5786, Loss: 0.0036\n","Batch 800/5786, Loss: 0.0003\n","Batch 900/5786, Loss: 0.0000\n","Batch 1000/5786, Loss: 0.0002\n","Batch 1100/5786, Loss: 0.0812\n","Batch 1200/5786, Loss: 1.0239\n","Batch 1300/5786, Loss: 0.0000\n","Batch 1400/5786, Loss: 0.0935\n","Batch 1500/5786, Loss: 0.0041\n","Batch 1600/5786, Loss: 0.0136\n","Batch 1700/5786, Loss: 0.0007\n","Batch 1800/5786, Loss: 0.0338\n","Batch 1900/5786, Loss: 0.0512\n","Batch 2000/5786, Loss: 0.0533\n","Batch 2100/5786, Loss: 0.0000\n","Batch 2200/5786, Loss: 0.0021\n","Batch 2300/5786, Loss: 0.0022\n","Batch 2400/5786, Loss: 0.0000\n","Batch 2500/5786, Loss: 0.0000\n","Batch 2600/5786, Loss: 0.0185\n","Batch 2700/5786, Loss: 0.0000\n","Batch 2800/5786, Loss: 0.0000\n","Batch 2900/5786, Loss: 0.0782\n","Batch 3000/5786, Loss: 0.0000\n","Batch 3100/5786, Loss: 1.1129\n","Batch 3200/5786, Loss: 0.0120\n","Batch 3300/5786, Loss: 0.6243\n","Batch 3400/5786, Loss: 0.0041\n","Batch 3500/5786, Loss: 0.0157\n","Batch 3600/5786, Loss: 0.0015\n","Batch 3700/5786, Loss: 0.0004\n","Batch 3800/5786, Loss: 0.2168\n","Batch 3900/5786, Loss: 0.0000\n","Batch 4000/5786, Loss: 0.0025\n","Batch 4100/5786, Loss: 0.0377\n","Batch 4200/5786, Loss: 0.0000\n","Batch 4300/5786, Loss: 0.0000\n","Batch 4400/5786, Loss: 0.0000\n","Batch 4500/5786, Loss: 0.0000\n","Batch 4600/5786, Loss: 0.0000\n","Batch 4700/5786, Loss: 0.0003\n","Batch 4800/5786, Loss: 0.9221\n","Batch 4900/5786, Loss: 0.0000\n","Batch 5000/5786, Loss: 0.3740\n","Batch 5100/5786, Loss: 0.0031\n","Batch 5200/5786, Loss: 0.7607\n","Batch 5300/5786, Loss: 0.2974\n","Batch 5400/5786, Loss: 0.2530\n","Batch 5500/5786, Loss: 0.0008\n","Batch 5600/5786, Loss: 0.0000\n","Batch 5700/5786, Loss: 0.0000\n","📈 Epoch 28/50, Average Loss: 0.0985\n","🎉 New best model! Loss: 0.0985\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 28...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 28 (loss: 0.0985)\n","\n","📊 Epoch 29/50\n","Batch 0/5786, Loss: 0.0063\n","Batch 100/5786, Loss: 0.0003\n","Batch 200/5786, Loss: 0.0000\n","Batch 300/5786, Loss: 0.1658\n","Batch 400/5786, Loss: 0.0110\n","Batch 500/5786, Loss: 0.0047\n","Batch 600/5786, Loss: 0.0001\n","Batch 700/5786, Loss: 0.0000\n","Batch 800/5786, Loss: 0.1579\n","Batch 900/5786, Loss: 0.0000\n","Batch 1000/5786, Loss: 0.2334\n","Batch 1100/5786, Loss: 0.0004\n","Batch 1200/5786, Loss: 0.0016\n","Batch 1300/5786, Loss: 0.0085\n","Batch 1400/5786, Loss: 0.0331\n","Batch 1500/5786, Loss: 0.0030\n","Batch 1600/5786, Loss: 0.2311\n","Batch 1700/5786, Loss: 0.0000\n","Batch 1800/5786, Loss: 0.0000\n","Batch 1900/5786, Loss: 0.0000\n","Batch 2000/5786, Loss: 0.4008\n","Batch 2100/5786, Loss: 0.0005\n","Batch 2200/5786, Loss: 0.0012\n","Batch 2300/5786, Loss: 0.0000\n","Batch 2400/5786, Loss: 0.1222\n","Batch 2500/5786, Loss: 0.0080\n","Batch 2600/5786, Loss: 0.1759\n","Batch 2700/5786, Loss: 0.0086\n","Batch 2800/5786, Loss: 0.0000\n","Batch 2900/5786, Loss: 0.0000\n","Batch 3000/5786, Loss: 0.0000\n","Batch 3100/5786, Loss: 0.0089\n","Batch 3200/5786, Loss: 0.0000\n","Batch 3300/5786, Loss: 0.0001\n","Batch 3400/5786, Loss: 0.0005\n","Batch 3500/5786, Loss: 0.0019\n","Batch 3600/5786, Loss: 0.0918\n","Batch 3700/5786, Loss: 0.0248\n","Batch 3800/5786, Loss: 0.0005\n","Batch 3900/5786, Loss: 0.0294\n","Batch 4000/5786, Loss: 0.0068\n","Batch 4100/5786, Loss: 0.1689\n","Batch 4200/5786, Loss: 0.0140\n","Batch 4300/5786, Loss: 0.1582\n","Batch 4400/5786, Loss: 0.0007\n","Batch 4500/5786, Loss: 0.0000\n","Batch 4600/5786, Loss: 0.0000\n","Batch 4700/5786, Loss: 0.0000\n","Batch 4800/5786, Loss: 0.0022\n","Batch 4900/5786, Loss: 0.0000\n","Batch 5000/5786, Loss: 0.0455\n","Batch 5100/5786, Loss: 0.0001\n","Batch 5200/5786, Loss: 0.2323\n","Batch 5300/5786, Loss: 0.0600\n","Batch 5400/5786, Loss: 0.0081\n","Batch 5500/5786, Loss: 0.3658\n","Batch 5600/5786, Loss: 0.0007\n","Batch 5700/5786, Loss: 0.3360\n","📈 Epoch 29/50, Average Loss: 0.0910\n","🎉 New best model! Loss: 0.0910\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 29...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 29 (loss: 0.0910)\n","\n","📊 Epoch 30/50\n","Batch 0/5786, Loss: 0.0000\n","Batch 100/5786, Loss: 0.0000\n","Batch 200/5786, Loss: 0.4076\n","Batch 300/5786, Loss: 0.0001\n","Batch 400/5786, Loss: 0.0648\n","Batch 500/5786, Loss: 0.0003\n","Batch 600/5786, Loss: 0.0000\n","Batch 700/5786, Loss: 0.0008\n","Batch 800/5786, Loss: 0.0001\n","Batch 900/5786, Loss: 0.1548\n","Batch 1000/5786, Loss: 0.4893\n","Batch 1100/5786, Loss: 0.0004\n","Batch 1200/5786, Loss: 0.0052\n","Batch 1300/5786, Loss: 0.0005\n","Batch 1400/5786, Loss: 0.0071\n","Batch 1500/5786, Loss: 0.0019\n","Batch 1600/5786, Loss: 0.0000\n","Batch 1700/5786, Loss: 0.0001\n","Batch 1800/5786, Loss: 0.0000\n","Batch 1900/5786, Loss: 0.0480\n","Batch 2000/5786, Loss: 0.0000\n","Batch 2100/5786, Loss: 0.0001\n","Batch 2200/5786, Loss: 0.0002\n","Batch 2300/5786, Loss: 0.0000\n","Batch 2400/5786, Loss: 0.5873\n","Batch 2500/5786, Loss: 0.0002\n","Batch 2600/5786, Loss: 0.0017\n","Batch 2700/5786, Loss: 0.0952\n","Batch 2800/5786, Loss: 0.0006\n","Batch 2900/5786, Loss: 0.0017\n","Batch 3000/5786, Loss: 0.2320\n","Batch 3100/5786, Loss: 0.0000\n","Batch 3200/5786, Loss: 0.6462\n","Batch 3300/5786, Loss: 0.0000\n","Batch 3400/5786, Loss: 0.0049\n","Batch 3500/5786, Loss: 0.1958\n","Batch 3600/5786, Loss: 0.0998\n","Batch 3700/5786, Loss: 1.3039\n","Batch 3800/5786, Loss: 0.0058\n","Batch 3900/5786, Loss: 0.0000\n","Batch 4000/5786, Loss: 0.0037\n","Batch 4100/5786, Loss: 0.0013\n","Batch 4200/5786, Loss: 0.0001\n","Batch 4300/5786, Loss: 0.0019\n","Batch 4400/5786, Loss: 0.0884\n","Batch 4500/5786, Loss: 0.1023\n","Batch 4600/5786, Loss: 0.0020\n","Batch 4700/5786, Loss: 0.0000\n","Batch 4800/5786, Loss: 0.0497\n","Batch 4900/5786, Loss: 0.1161\n","Batch 5000/5786, Loss: 0.0000\n","Batch 5100/5786, Loss: 0.0003\n","Batch 5200/5786, Loss: 0.2752\n","Batch 5300/5786, Loss: 0.0001\n","Batch 5400/5786, Loss: 0.4908\n","Batch 5500/5786, Loss: 0.3316\n","Batch 5600/5786, Loss: 0.0000\n","Batch 5700/5786, Loss: 0.0002\n","📈 Epoch 30/50, Average Loss: 0.0930\n","🔄 Building and saving reference base at epoch 30...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 30 (loss: 0.0930)\n","\n","📊 Epoch 31/50\n","Batch 0/5786, Loss: 0.0030\n","Batch 100/5786, Loss: 0.0004\n","Batch 200/5786, Loss: 0.0000\n","Batch 300/5786, Loss: 0.0001\n","Batch 400/5786, Loss: 0.0001\n","Batch 500/5786, Loss: 1.0293\n","Batch 600/5786, Loss: 0.0153\n","Batch 700/5786, Loss: 0.0000\n","Batch 800/5786, Loss: 0.0027\n","Batch 900/5786, Loss: 0.0001\n","Batch 1000/5786, Loss: 0.0003\n","Batch 1100/5786, Loss: 0.0001\n","Batch 1200/5786, Loss: 0.0018\n","Batch 1300/5786, Loss: 0.0002\n","Batch 1400/5786, Loss: 0.0221\n","Batch 1500/5786, Loss: 0.0005\n","Batch 1600/5786, Loss: 0.0000\n","Batch 1700/5786, Loss: 0.0018\n","Batch 1800/5786, Loss: 0.0002\n","Batch 1900/5786, Loss: 0.5223\n","Batch 2000/5786, Loss: 0.0000\n","Batch 2100/5786, Loss: 0.0156\n","Batch 2200/5786, Loss: 0.0000\n","Batch 2300/5786, Loss: 0.0000\n","Batch 2400/5786, Loss: 0.0000\n","Batch 2500/5786, Loss: 0.3806\n","Batch 2600/5786, Loss: 0.0001\n","Batch 2700/5786, Loss: 0.2732\n","Batch 2800/5786, Loss: 0.0395\n","Batch 2900/5786, Loss: 0.4711\n","Batch 3000/5786, Loss: 0.0048\n","Batch 3100/5786, Loss: 0.0387\n","Batch 3200/5786, Loss: 0.0007\n","Batch 3300/5786, Loss: 0.0457\n","Batch 3400/5786, Loss: 0.0019\n","Batch 3500/5786, Loss: 0.0001\n","Batch 3600/5786, Loss: 0.0001\n","Batch 3700/5786, Loss: 2.0045\n","Batch 3800/5786, Loss: 0.0020\n","Batch 3900/5786, Loss: 0.0467\n","Batch 4000/5786, Loss: 0.3288\n","Batch 4100/5786, Loss: 0.0363\n","Batch 4200/5786, Loss: 0.2476\n","Batch 4300/5786, Loss: 0.0097\n","Batch 4400/5786, Loss: 0.0000\n","Batch 4500/5786, Loss: 0.0000\n","Batch 4600/5786, Loss: 0.0923\n","Batch 4700/5786, Loss: 1.0162\n","Batch 4800/5786, Loss: 0.0002\n","Batch 4900/5786, Loss: 0.0132\n","Batch 5000/5786, Loss: 0.0000\n","Batch 5100/5786, Loss: 0.0000\n","Batch 5200/5786, Loss: 0.0000\n","Batch 5300/5786, Loss: 0.0780\n","Batch 5400/5786, Loss: 0.0041\n","Batch 5500/5786, Loss: 0.0020\n","Batch 5600/5786, Loss: 0.4882\n","Batch 5700/5786, Loss: 0.0445\n","📈 Epoch 31/50, Average Loss: 0.0868\n","🎉 New best model! Loss: 0.0868\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 31...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 31 (loss: 0.0868)\n","\n","📊 Epoch 32/50\n","Batch 0/5786, Loss: 0.2969\n","Batch 100/5786, Loss: 0.0065\n","Batch 200/5786, Loss: 0.0208\n","Batch 300/5786, Loss: 0.4730\n","Batch 400/5786, Loss: 0.0060\n","Batch 500/5786, Loss: 0.0463\n","Batch 600/5786, Loss: 0.0000\n","Batch 700/5786, Loss: 0.0002\n","Batch 800/5786, Loss: 0.0425\n","Batch 900/5786, Loss: 0.8357\n","Batch 1000/5786, Loss: 0.0296\n","Batch 1100/5786, Loss: 0.0000\n","Batch 1200/5786, Loss: 0.0108\n","Batch 1300/5786, Loss: 0.0000\n","Batch 1400/5786, Loss: 0.0459\n","Batch 1500/5786, Loss: 0.0000\n","Batch 1600/5786, Loss: 0.0000\n","Batch 1700/5786, Loss: 0.0418\n","Batch 1800/5786, Loss: 0.0004\n","Batch 1900/5786, Loss: 0.0000\n","Batch 2000/5786, Loss: 0.0000\n","Batch 2100/5786, Loss: 0.0000\n","Batch 2200/5786, Loss: 0.0023\n","Batch 2300/5786, Loss: 0.0000\n","Batch 2400/5786, Loss: 0.0000\n","Batch 2500/5786, Loss: 0.0000\n","Batch 2600/5786, Loss: 0.0000\n","Batch 2700/5786, Loss: 0.0000\n","Batch 2800/5786, Loss: 0.0003\n","Batch 2900/5786, Loss: 0.0018\n","Batch 3000/5786, Loss: 0.0003\n","Batch 3100/5786, Loss: 0.1334\n","Batch 3200/5786, Loss: 0.0000\n","Batch 3300/5786, Loss: 0.0000\n","Batch 3400/5786, Loss: 0.0001\n","Batch 3500/5786, Loss: 0.0002\n","Batch 3600/5786, Loss: 0.0053\n","Batch 3700/5786, Loss: 0.0007\n","Batch 3800/5786, Loss: 0.0000\n","Batch 3900/5786, Loss: 0.0052\n","Batch 4000/5786, Loss: 0.0001\n","Batch 4100/5786, Loss: 0.2524\n","Batch 4200/5786, Loss: 0.0008\n","Batch 4300/5786, Loss: 0.1092\n","Batch 4400/5786, Loss: 0.0044\n","Batch 4500/5786, Loss: 0.0187\n","Batch 4600/5786, Loss: 0.0000\n","Batch 4700/5786, Loss: 0.0003\n","Batch 4800/5786, Loss: 0.0465\n","Batch 4900/5786, Loss: 0.0016\n","Batch 5000/5786, Loss: 0.0006\n","Batch 5100/5786, Loss: 0.0005\n","Batch 5200/5786, Loss: 0.9112\n","Batch 5300/5786, Loss: 0.0003\n","Batch 5400/5786, Loss: 0.0000\n","Batch 5500/5786, Loss: 0.0014\n","Batch 5600/5786, Loss: 0.0175\n","Batch 5700/5786, Loss: 0.0000\n","📈 Epoch 32/50, Average Loss: 0.0827\n","🎉 New best model! Loss: 0.0827\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 32...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 32 (loss: 0.0827)\n","\n","📊 Epoch 33/50\n","Batch 0/5786, Loss: 0.0629\n","Batch 100/5786, Loss: 0.0007\n","Batch 200/5786, Loss: 0.0001\n","Batch 300/5786, Loss: 0.0039\n","Batch 400/5786, Loss: 0.0006\n","Batch 500/5786, Loss: 0.0003\n","Batch 600/5786, Loss: 0.0673\n","Batch 700/5786, Loss: 0.0000\n","Batch 800/5786, Loss: 0.4083\n","Batch 900/5786, Loss: 0.0155\n","Batch 1000/5786, Loss: 0.0000\n","Batch 1100/5786, Loss: 0.0002\n","Batch 1200/5786, Loss: 0.0224\n","Batch 1300/5786, Loss: 0.3343\n","Batch 1400/5786, Loss: 0.0076\n","Batch 1500/5786, Loss: 0.0080\n","Batch 1600/5786, Loss: 0.0001\n","Batch 1700/5786, Loss: 0.0001\n","Batch 1800/5786, Loss: 1.1697\n","Batch 1900/5786, Loss: 0.0003\n","Batch 2000/5786, Loss: 0.0027\n","Batch 2100/5786, Loss: 0.0000\n","Batch 2200/5786, Loss: 0.1132\n","Batch 2300/5786, Loss: 0.0641\n","Batch 2400/5786, Loss: 0.1493\n","Batch 2500/5786, Loss: 0.0002\n","Batch 2600/5786, Loss: 0.1981\n","Batch 2700/5786, Loss: 0.0001\n","Batch 2800/5786, Loss: 0.0002\n","Batch 2900/5786, Loss: 0.1085\n","Batch 3000/5786, Loss: 0.4913\n","Batch 3100/5786, Loss: 0.0000\n","Batch 3200/5786, Loss: 0.0110\n","Batch 3300/5786, Loss: 0.0000\n","Batch 3400/5786, Loss: 0.0924\n","Batch 3500/5786, Loss: 0.0003\n","Batch 3600/5786, Loss: 0.0069\n","Batch 3700/5786, Loss: 0.2450\n","Batch 3800/5786, Loss: 0.0023\n","Batch 3900/5786, Loss: 0.0000\n","Batch 4000/5786, Loss: 0.0003\n","Batch 4100/5786, Loss: 0.0001\n","Batch 4200/5786, Loss: 0.5174\n","Batch 4300/5786, Loss: 0.0023\n","Batch 4400/5786, Loss: 0.0732\n","Batch 4500/5786, Loss: 0.0000\n","Batch 4600/5786, Loss: 0.0000\n","Batch 4700/5786, Loss: 0.0000\n","Batch 4800/5786, Loss: 0.0455\n","Batch 4900/5786, Loss: 0.0493\n","Batch 5000/5786, Loss: 0.0004\n","Batch 5100/5786, Loss: 0.0023\n","Batch 5200/5786, Loss: 0.0149\n","Batch 5300/5786, Loss: 0.0091\n","Batch 5400/5786, Loss: 0.0384\n","Batch 5500/5786, Loss: 0.0000\n","Batch 5600/5786, Loss: 0.0003\n","Batch 5700/5786, Loss: 0.1332\n","📈 Epoch 33/50, Average Loss: 0.0769\n","🎉 New best model! Loss: 0.0769\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 33...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 33 (loss: 0.0769)\n","\n","📊 Epoch 34/50\n","Batch 0/5786, Loss: 0.1169\n","Batch 100/5786, Loss: 0.0001\n","Batch 200/5786, Loss: 0.0000\n","Batch 300/5786, Loss: 0.0878\n","Batch 400/5786, Loss: 0.4366\n","Batch 500/5786, Loss: 0.0249\n","Batch 600/5786, Loss: 0.0000\n","Batch 700/5786, Loss: 0.0022\n","Batch 800/5786, Loss: 0.0020\n","Batch 900/5786, Loss: 0.0000\n","Batch 1000/5786, Loss: 0.0370\n","Batch 1100/5786, Loss: 0.0229\n","Batch 1200/5786, Loss: 0.0001\n","Batch 1300/5786, Loss: 0.0000\n","Batch 1400/5786, Loss: 0.0000\n","Batch 1500/5786, Loss: 0.0011\n","Batch 1600/5786, Loss: 0.0009\n","Batch 1700/5786, Loss: 0.0842\n","Batch 1800/5786, Loss: 0.0000\n","Batch 1900/5786, Loss: 0.0159\n","Batch 2000/5786, Loss: 0.0527\n","Batch 2100/5786, Loss: 0.5765\n","Batch 2200/5786, Loss: 0.0003\n","Batch 2300/5786, Loss: 0.0068\n","Batch 2400/5786, Loss: 0.0004\n","Batch 2500/5786, Loss: 0.0187\n","Batch 2600/5786, Loss: 0.0003\n","Batch 2700/5786, Loss: 0.0120\n","Batch 2800/5786, Loss: 0.0001\n","Batch 2900/5786, Loss: 0.0000\n","Batch 3000/5786, Loss: 0.0000\n","Batch 3100/5786, Loss: 1.1768\n","Batch 3200/5786, Loss: 0.4437\n","Batch 3300/5786, Loss: 0.6250\n","Batch 3400/5786, Loss: 0.0188\n","Batch 3500/5786, Loss: 0.3243\n","Batch 3600/5786, Loss: 0.0253\n","Batch 3700/5786, Loss: 0.0938\n","Batch 3800/5786, Loss: 0.0534\n","Batch 3900/5786, Loss: 0.0000\n","Batch 4000/5786, Loss: 0.0000\n","Batch 4100/5786, Loss: 0.0000\n","Batch 4200/5786, Loss: 0.0012\n","Batch 4300/5786, Loss: 0.0855\n","Batch 4400/5786, Loss: 0.0001\n","Batch 4500/5786, Loss: 0.0000\n","Batch 4600/5786, Loss: 0.0000\n","Batch 4700/5786, Loss: 0.0000\n","Batch 4800/5786, Loss: 0.0615\n","Batch 4900/5786, Loss: 0.1656\n","Batch 5000/5786, Loss: 0.4753\n","Batch 5100/5786, Loss: 0.0057\n","Batch 5200/5786, Loss: 0.0000\n","Batch 5300/5786, Loss: 0.2570\n","Batch 5400/5786, Loss: 0.0000\n","Batch 5500/5786, Loss: 0.2394\n","Batch 5600/5786, Loss: 0.0001\n","Batch 5700/5786, Loss: 1.1535\n","📈 Epoch 34/50, Average Loss: 0.0720\n","🎉 New best model! Loss: 0.0720\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 34...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 34 (loss: 0.0720)\n","\n","📊 Epoch 35/50\n","Batch 0/5786, Loss: 0.3332\n","Batch 100/5786, Loss: 0.0012\n","Batch 200/5786, Loss: 0.0939\n","Batch 300/5786, Loss: 0.0046\n","Batch 400/5786, Loss: 0.6494\n","Batch 500/5786, Loss: 0.0012\n","Batch 600/5786, Loss: 0.0026\n","Batch 700/5786, Loss: 0.0375\n","Batch 800/5786, Loss: 0.0772\n","Batch 900/5786, Loss: 0.0832\n","Batch 1000/5786, Loss: 0.0000\n","Batch 1100/5786, Loss: 0.1048\n","Batch 1200/5786, Loss: 1.2771\n","Batch 1300/5786, Loss: 0.0071\n","Batch 1400/5786, Loss: 0.0056\n","Batch 1500/5786, Loss: 0.0329\n","Batch 1600/5786, Loss: 0.0122\n","Batch 1700/5786, Loss: 0.0153\n","Batch 1800/5786, Loss: 0.0003\n","Batch 1900/5786, Loss: 0.0001\n","Batch 2000/5786, Loss: 0.2512\n","Batch 2100/5786, Loss: 0.0020\n","Batch 2200/5786, Loss: 0.0001\n","Batch 2300/5786, Loss: 0.0000\n","Batch 2400/5786, Loss: 0.3484\n","Batch 2500/5786, Loss: 0.4497\n","Batch 2600/5786, Loss: 0.0000\n","Batch 2700/5786, Loss: 0.0000\n","Batch 2800/5786, Loss: 0.0000\n","Batch 2900/5786, Loss: 0.0561\n","Batch 3000/5786, Loss: 0.0000\n","Batch 3100/5786, Loss: 0.0024\n","Batch 3200/5786, Loss: 0.0016\n","Batch 3300/5786, Loss: 0.0000\n","Batch 3400/5786, Loss: 0.0000\n","Batch 3500/5786, Loss: 0.0000\n","Batch 3600/5786, Loss: 0.0986\n","Batch 3700/5786, Loss: 0.4103\n","Batch 3800/5786, Loss: 0.0069\n","Batch 3900/5786, Loss: 0.0025\n","Batch 4000/5786, Loss: 0.0003\n","Batch 4100/5786, Loss: 0.0007\n","Batch 4200/5786, Loss: 0.5950\n","Batch 4300/5786, Loss: 0.3203\n","Batch 4400/5786, Loss: 0.0076\n","Batch 4500/5786, Loss: 0.0028\n","Batch 4600/5786, Loss: 0.0006\n","Batch 4700/5786, Loss: 0.0026\n","Batch 4800/5786, Loss: 0.0000\n","Batch 4900/5786, Loss: 0.0000\n","Batch 5000/5786, Loss: 0.0024\n","Batch 5100/5786, Loss: 0.0000\n","Batch 5200/5786, Loss: 0.0041\n","Batch 5300/5786, Loss: 0.0002\n","Batch 5400/5786, Loss: 0.0000\n","Batch 5500/5786, Loss: 0.0000\n","Batch 5600/5786, Loss: 0.0151\n","Batch 5700/5786, Loss: 0.0212\n","📈 Epoch 35/50, Average Loss: 0.0667\n","🎉 New best model! Loss: 0.0667\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 35...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 35 (loss: 0.0667)\n","\n","📊 Epoch 36/50\n","Batch 0/5786, Loss: 0.0000\n","Batch 100/5786, Loss: 0.0000\n","Batch 200/5786, Loss: 0.0704\n","Batch 300/5786, Loss: 0.1627\n","Batch 400/5786, Loss: 0.0075\n","Batch 500/5786, Loss: 0.0000\n","Batch 600/5786, Loss: 0.0000\n","Batch 700/5786, Loss: 0.0027\n","Batch 800/5786, Loss: 0.0202\n","Batch 900/5786, Loss: 0.0165\n","Batch 1000/5786, Loss: 0.0000\n","Batch 1100/5786, Loss: 0.0000\n","Batch 1200/5786, Loss: 0.0002\n","Batch 1300/5786, Loss: 0.0001\n","Batch 1400/5786, Loss: 0.0225\n","Batch 1500/5786, Loss: 0.0005\n","Batch 1600/5786, Loss: 0.0019\n","Batch 1700/5786, Loss: 0.0295\n","Batch 1800/5786, Loss: 0.0004\n","Batch 1900/5786, Loss: 0.0001\n","Batch 2000/5786, Loss: 0.0075\n","Batch 2100/5786, Loss: 0.0030\n","Batch 2200/5786, Loss: 0.0030\n","Batch 2300/5786, Loss: 0.0019\n","Batch 2400/5786, Loss: 0.0000\n","Batch 2500/5786, Loss: 0.0010\n","Batch 2600/5786, Loss: 0.0425\n","Batch 2700/5786, Loss: 0.0736\n","Batch 2800/5786, Loss: 0.0002\n","Batch 2900/5786, Loss: 0.0001\n","Batch 3000/5786, Loss: 0.0548\n","Batch 3100/5786, Loss: 0.0000\n","Batch 3200/5786, Loss: 0.0001\n","Batch 3300/5786, Loss: 0.0197\n","Batch 3400/5786, Loss: 0.0002\n","Batch 3500/5786, Loss: 0.0010\n","Batch 3600/5786, Loss: 0.0027\n","Batch 3700/5786, Loss: 0.0000\n","Batch 3800/5786, Loss: 0.0115\n","Batch 3900/5786, Loss: 0.0000\n","Batch 4000/5786, Loss: 0.0135\n","Batch 4100/5786, Loss: 0.2375\n","Batch 4200/5786, Loss: 0.0004\n","Batch 4300/5786, Loss: 0.0013\n","Batch 4400/5786, Loss: 0.1844\n","Batch 4500/5786, Loss: 0.0000\n","Batch 4600/5786, Loss: 0.0001\n","Batch 4700/5786, Loss: 0.0011\n","Batch 4800/5786, Loss: 0.0427\n","Batch 4900/5786, Loss: 0.0003\n","Batch 5000/5786, Loss: 0.0012\n","Batch 5100/5786, Loss: 0.0495\n","Batch 5200/5786, Loss: 0.6484\n","Batch 5300/5786, Loss: 0.2171\n","Batch 5400/5786, Loss: 0.0003\n","Batch 5500/5786, Loss: 0.0001\n","Batch 5600/5786, Loss: 0.0013\n","Batch 5700/5786, Loss: 0.0132\n","📈 Epoch 36/50, Average Loss: 0.0696\n","💾 Training state saved at epoch 36 (loss: 0.0696)\n","\n","📊 Epoch 37/50\n","Batch 0/5786, Loss: 0.0000\n","Batch 100/5786, Loss: 0.0026\n","Batch 200/5786, Loss: 0.0000\n","Batch 300/5786, Loss: 0.0000\n","Batch 400/5786, Loss: 0.1306\n","Batch 500/5786, Loss: 1.2072\n","Batch 600/5786, Loss: 0.0000\n","Batch 700/5786, Loss: 0.0001\n","Batch 800/5786, Loss: 0.0000\n","Batch 900/5786, Loss: 0.0526\n","Batch 1000/5786, Loss: 0.0186\n","Batch 1100/5786, Loss: 0.0000\n","Batch 1200/5786, Loss: 0.0048\n","Batch 1300/5786, Loss: 0.0001\n","Batch 1400/5786, Loss: 0.0001\n","Batch 1500/5786, Loss: 0.1619\n","Batch 1600/5786, Loss: 0.0000\n","Batch 1700/5786, Loss: 0.0000\n","Batch 1800/5786, Loss: 0.0006\n","Batch 1900/5786, Loss: 0.0041\n","Batch 2000/5786, Loss: 0.0000\n","Batch 2100/5786, Loss: 0.0025\n","Batch 2200/5786, Loss: 0.0000\n","Batch 2300/5786, Loss: 0.0003\n","Batch 2400/5786, Loss: 0.0486\n","Batch 2500/5786, Loss: 0.0016\n","Batch 2600/5786, Loss: 0.0002\n","Batch 2700/5786, Loss: 0.0131\n","Batch 2800/5786, Loss: 0.0249\n","Batch 2900/5786, Loss: 0.0798\n","Batch 3000/5786, Loss: 0.0031\n","Batch 3100/5786, Loss: 0.0001\n","Batch 3200/5786, Loss: 0.0149\n","Batch 3300/5786, Loss: 0.0001\n","Batch 3400/5786, Loss: 0.0000\n","Batch 3500/5786, Loss: 0.0000\n","Batch 3600/5786, Loss: 0.0039\n","Batch 3700/5786, Loss: 0.0000\n","Batch 3800/5786, Loss: 0.0199\n","Batch 3900/5786, Loss: 0.0000\n","Batch 4000/5786, Loss: 0.0000\n","Batch 4100/5786, Loss: 0.0303\n","Batch 4200/5786, Loss: 0.2775\n","Batch 4300/5786, Loss: 0.0001\n","Batch 4400/5786, Loss: 0.0016\n","Batch 4500/5786, Loss: 0.0292\n","Batch 4600/5786, Loss: 0.0001\n","Batch 4700/5786, Loss: 0.0002\n","Batch 4800/5786, Loss: 0.0000\n","Batch 4900/5786, Loss: 0.0004\n","Batch 5000/5786, Loss: 0.0003\n","Batch 5100/5786, Loss: 0.0264\n","Batch 5200/5786, Loss: 0.0002\n","Batch 5300/5786, Loss: 0.0089\n","Batch 5400/5786, Loss: 0.4444\n","Batch 5500/5786, Loss: 0.7870\n","Batch 5600/5786, Loss: 0.1317\n","Batch 5700/5786, Loss: 0.0007\n","📈 Epoch 37/50, Average Loss: 0.0659\n","🎉 New best model! Loss: 0.0659\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 37...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 37 (loss: 0.0659)\n","\n","📊 Epoch 38/50\n","Batch 0/5786, Loss: 0.0000\n","Batch 100/5786, Loss: 0.6197\n","Batch 200/5786, Loss: 0.0094\n","Batch 300/5786, Loss: 0.0000\n","Batch 400/5786, Loss: 0.0503\n","Batch 500/5786, Loss: 0.0113\n","Batch 600/5786, Loss: 0.0031\n","Batch 700/5786, Loss: 0.0001\n","Batch 800/5786, Loss: 0.1960\n","Batch 900/5786, Loss: 0.0020\n","Batch 1000/5786, Loss: 0.0021\n","Batch 1100/5786, Loss: 0.0192\n","Batch 1200/5786, Loss: 0.0000\n","Batch 1300/5786, Loss: 0.0443\n","Batch 1400/5786, Loss: 0.0000\n","Batch 1500/5786, Loss: 0.0783\n","Batch 1600/5786, Loss: 0.5355\n","Batch 1700/5786, Loss: 0.2147\n","Batch 1800/5786, Loss: 0.1268\n","Batch 1900/5786, Loss: 0.0207\n","Batch 2000/5786, Loss: 0.3330\n","Batch 2100/5786, Loss: 0.0529\n","Batch 2200/5786, Loss: 0.0000\n","Batch 2300/5786, Loss: 0.0015\n","Batch 2400/5786, Loss: 0.0019\n","Batch 2500/5786, Loss: 0.7695\n","Batch 2600/5786, Loss: 0.0001\n","Batch 2700/5786, Loss: 0.0057\n","Batch 2800/5786, Loss: 0.0000\n","Batch 2900/5786, Loss: 0.0000\n","Batch 3000/5786, Loss: 0.0064\n","Batch 3100/5786, Loss: 0.0000\n","Batch 3200/5786, Loss: 0.0001\n","Batch 3300/5786, Loss: 0.0000\n","Batch 3400/5786, Loss: 0.0002\n","Batch 3500/5786, Loss: 0.0405\n","Batch 3600/5786, Loss: 0.0376\n","Batch 3700/5786, Loss: 0.0132\n","Batch 3800/5786, Loss: 0.0000\n","Batch 3900/5786, Loss: 0.0001\n","Batch 4000/5786, Loss: 0.0000\n","Batch 4100/5786, Loss: 0.0001\n","Batch 4200/5786, Loss: 0.0001\n","Batch 4300/5786, Loss: 0.0000\n","Batch 4400/5786, Loss: 0.0029\n","Batch 4500/5786, Loss: 0.0002\n","Batch 4600/5786, Loss: 0.0004\n","Batch 4700/5786, Loss: 0.0025\n","Batch 4800/5786, Loss: 0.0242\n","Batch 4900/5786, Loss: 0.0679\n","Batch 5000/5786, Loss: 0.0000\n","Batch 5100/5786, Loss: 0.1980\n","Batch 5200/5786, Loss: 0.0612\n","Batch 5300/5786, Loss: 0.0006\n","Batch 5400/5786, Loss: 0.0006\n","Batch 5500/5786, Loss: 0.0000\n","Batch 5600/5786, Loss: 0.4321\n","Batch 5700/5786, Loss: 0.0000\n","📈 Epoch 38/50, Average Loss: 0.0667\n","💾 Training state saved at epoch 38 (loss: 0.0667)\n","\n","📊 Epoch 39/50\n","Batch 0/5786, Loss: 0.0000\n","Batch 100/5786, Loss: 0.0001\n","Batch 200/5786, Loss: 0.0002\n","Batch 300/5786, Loss: 0.0007\n","Batch 400/5786, Loss: 0.0001\n","Batch 500/5786, Loss: 0.0017\n","Batch 600/5786, Loss: 0.0000\n","Batch 700/5786, Loss: 0.0010\n","Batch 800/5786, Loss: 0.0544\n","Batch 900/5786, Loss: 0.0000\n","Batch 1000/5786, Loss: 0.0003\n","Batch 1100/5786, Loss: 0.0022\n","Batch 1200/5786, Loss: 0.0000\n","Batch 1300/5786, Loss: 0.0001\n","Batch 1400/5786, Loss: 0.3237\n","Batch 1500/5786, Loss: 0.0007\n","Batch 1600/5786, Loss: 0.0000\n","Batch 1700/5786, Loss: 0.2179\n","Batch 1800/5786, Loss: 0.1898\n","Batch 1900/5786, Loss: 0.0000\n","Batch 2000/5786, Loss: 0.1172\n","Batch 2100/5786, Loss: 0.3826\n","Batch 2200/5786, Loss: 0.0001\n","Batch 2300/5786, Loss: 0.1250\n","Batch 2400/5786, Loss: 0.0000\n","Batch 2500/5786, Loss: 0.0000\n","Batch 2600/5786, Loss: 0.0000\n","Batch 2700/5786, Loss: 0.0093\n","Batch 2800/5786, Loss: 0.1069\n","Batch 2900/5786, Loss: 0.1370\n","Batch 3000/5786, Loss: 0.0082\n","Batch 3100/5786, Loss: 0.0000\n","Batch 3200/5786, Loss: 0.0000\n","Batch 3300/5786, Loss: 0.0002\n","Batch 3400/5786, Loss: 0.0003\n","Batch 3500/5786, Loss: 0.0270\n","Batch 3600/5786, Loss: 0.0000\n","Batch 3700/5786, Loss: 0.0033\n","Batch 3800/5786, Loss: 0.0003\n","Batch 3900/5786, Loss: 0.0000\n","Batch 4000/5786, Loss: 0.0000\n","Batch 4100/5786, Loss: 0.0000\n","Batch 4200/5786, Loss: 0.0000\n","Batch 4300/5786, Loss: 0.0040\n","Batch 4400/5786, Loss: 0.0053\n","Batch 4500/5786, Loss: 0.0017\n","Batch 4600/5786, Loss: 0.0012\n","Batch 4700/5786, Loss: 0.0001\n","Batch 4800/5786, Loss: 0.2084\n","Batch 4900/5786, Loss: 0.0000\n","Batch 5000/5786, Loss: 0.0162\n","Batch 5100/5786, Loss: 0.0000\n","Batch 5200/5786, Loss: 0.0003\n","Batch 5300/5786, Loss: 0.0635\n","Batch 5400/5786, Loss: 0.0000\n","Batch 5500/5786, Loss: 0.0002\n","Batch 5600/5786, Loss: 0.0001\n","Batch 5700/5786, Loss: 0.0898\n","📈 Epoch 39/50, Average Loss: 0.0630\n","🎉 New best model! Loss: 0.0630\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 39...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 39 (loss: 0.0630)\n","\n","📊 Epoch 40/50\n","Batch 0/5786, Loss: 0.5482\n","Batch 100/5786, Loss: 0.0000\n","Batch 200/5786, Loss: 0.2795\n","Batch 300/5786, Loss: 0.0000\n","Batch 400/5786, Loss: 0.0323\n","Batch 500/5786, Loss: 0.0000\n","Batch 600/5786, Loss: 0.5308\n","Batch 700/5786, Loss: 0.0604\n","Batch 800/5786, Loss: 0.0000\n","Batch 900/5786, Loss: 0.0000\n","Batch 1000/5786, Loss: 0.0105\n","Batch 1100/5786, Loss: 0.0051\n","Batch 1200/5786, Loss: 0.9428\n","Batch 1300/5786, Loss: 0.0021\n","Batch 1400/5786, Loss: 0.1655\n","Batch 1500/5786, Loss: 0.0001\n","Batch 1600/5786, Loss: 0.0977\n","Batch 1700/5786, Loss: 0.0283\n","Batch 1800/5786, Loss: 0.0000\n","Batch 1900/5786, Loss: 0.0230\n","Batch 2000/5786, Loss: 1.0822\n","Batch 2100/5786, Loss: 0.0000\n","Batch 2200/5786, Loss: 0.0020\n","Batch 2300/5786, Loss: 0.0014\n","Batch 2400/5786, Loss: 0.0000\n","Batch 2500/5786, Loss: 0.0250\n","Batch 2600/5786, Loss: 0.0001\n","Batch 2700/5786, Loss: 0.0000\n","Batch 2800/5786, Loss: 0.0015\n","Batch 2900/5786, Loss: 0.0013\n","Batch 3000/5786, Loss: 1.1401\n","Batch 3100/5786, Loss: 0.0034\n","Batch 3200/5786, Loss: 0.0001\n","Batch 3300/5786, Loss: 0.0000\n","Batch 3400/5786, Loss: 0.0001\n","Batch 3500/5786, Loss: 0.6969\n","Batch 3600/5786, Loss: 0.0533\n","Batch 3700/5786, Loss: 0.1637\n","Batch 3800/5786, Loss: 0.0000\n","Batch 3900/5786, Loss: 0.0018\n","Batch 4000/5786, Loss: 0.0000\n","Batch 4100/5786, Loss: 0.0268\n","Batch 4200/5786, Loss: 0.2942\n","Batch 4300/5786, Loss: 0.0000\n","Batch 4400/5786, Loss: 0.1200\n","Batch 4500/5786, Loss: 0.0254\n","Batch 4600/5786, Loss: 0.0007\n","Batch 4700/5786, Loss: 0.0001\n","Batch 4800/5786, Loss: 0.0262\n","Batch 4900/5786, Loss: 0.0016\n","Batch 5000/5786, Loss: 0.0002\n","Batch 5100/5786, Loss: 0.0000\n","Batch 5200/5786, Loss: 0.0003\n","Batch 5300/5786, Loss: 0.0000\n","Batch 5400/5786, Loss: 0.0393\n","Batch 5500/5786, Loss: 0.1254\n","Batch 5600/5786, Loss: 0.0591\n","Batch 5700/5786, Loss: 0.0002\n","📈 Epoch 40/50, Average Loss: 0.0560\n","🎉 New best model! Loss: 0.0560\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 40...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 40 (loss: 0.0560)\n","\n","📊 Epoch 41/50\n","Batch 0/5786, Loss: 0.0018\n","Batch 100/5786, Loss: 0.0000\n","Batch 200/5786, Loss: 0.4951\n","Batch 300/5786, Loss: 0.0003\n","Batch 400/5786, Loss: 0.0000\n","Batch 500/5786, Loss: 0.0003\n","Batch 600/5786, Loss: 0.0000\n","Batch 700/5786, Loss: 0.0030\n","Batch 800/5786, Loss: 0.0027\n","Batch 900/5786, Loss: 0.0068\n","Batch 1000/5786, Loss: 0.0000\n","Batch 1100/5786, Loss: 0.0000\n","Batch 1200/5786, Loss: 0.0008\n","Batch 1300/5786, Loss: 0.0006\n","Batch 1400/5786, Loss: 0.0092\n","Batch 1500/5786, Loss: 0.0776\n","Batch 1600/5786, Loss: 0.0039\n","Batch 1700/5786, Loss: 0.0314\n","Batch 1800/5786, Loss: 0.1186\n","Batch 1900/5786, Loss: 0.0921\n","Batch 2000/5786, Loss: 0.1090\n","Batch 2100/5786, Loss: 0.0016\n","Batch 2200/5786, Loss: 1.0827\n","Batch 2300/5786, Loss: 0.4393\n","Batch 2400/5786, Loss: 0.0001\n","Batch 2500/5786, Loss: 0.0014\n","Batch 2600/5786, Loss: 0.0033\n","Batch 2700/5786, Loss: 0.0002\n","Batch 2800/5786, Loss: 0.0002\n","Batch 2900/5786, Loss: 0.0000\n","Batch 3000/5786, Loss: 0.8103\n","Batch 3100/5786, Loss: 0.0000\n","Batch 3200/5786, Loss: 0.0000\n","Batch 3300/5786, Loss: 0.0000\n","Batch 3400/5786, Loss: 0.0002\n","Batch 3500/5786, Loss: 0.0000\n","Batch 3600/5786, Loss: 0.0007\n","Batch 3700/5786, Loss: 0.6270\n","Batch 3800/5786, Loss: 0.0004\n","Batch 3900/5786, Loss: 0.0020\n","Batch 4000/5786, Loss: 0.0000\n","Batch 4100/5786, Loss: 0.0000\n","Batch 4200/5786, Loss: 0.0572\n","Batch 4300/5786, Loss: 0.0776\n","Batch 4400/5786, Loss: 0.5769\n","Batch 4500/5786, Loss: 0.0004\n","Batch 4600/5786, Loss: 0.0163\n","Batch 4700/5786, Loss: 0.0003\n","Batch 4800/5786, Loss: 1.5574\n","Batch 4900/5786, Loss: 0.0030\n","Batch 5000/5786, Loss: 0.0000\n","Batch 5100/5786, Loss: 0.0655\n","Batch 5200/5786, Loss: 0.0729\n","Batch 5300/5786, Loss: 0.0012\n","Batch 5400/5786, Loss: 0.0046\n","Batch 5500/5786, Loss: 0.0008\n","Batch 5600/5786, Loss: 0.0001\n","Batch 5700/5786, Loss: 0.0000\n","📈 Epoch 41/50, Average Loss: 0.0590\n","💾 Training state saved at epoch 41 (loss: 0.0590)\n","\n","📊 Epoch 42/50\n","Batch 0/5786, Loss: 0.0001\n","Batch 100/5786, Loss: 0.0411\n","Batch 200/5786, Loss: 0.2296\n","Batch 300/5786, Loss: 0.5633\n","Batch 400/5786, Loss: 0.2484\n","Batch 500/5786, Loss: 0.0044\n","Batch 600/5786, Loss: 0.0000\n","Batch 700/5786, Loss: 0.0025\n","Batch 800/5786, Loss: 0.0000\n","Batch 900/5786, Loss: 0.0000\n","Batch 1000/5786, Loss: 0.3346\n","Batch 1100/5786, Loss: 0.4147\n","Batch 1200/5786, Loss: 0.0003\n","Batch 1300/5786, Loss: 0.0003\n","Batch 1400/5786, Loss: 0.0005\n","Batch 1500/5786, Loss: 0.0001\n","Batch 1600/5786, Loss: 0.1134\n","Batch 1700/5786, Loss: 0.0279\n","Batch 1800/5786, Loss: 0.0001\n","Batch 1900/5786, Loss: 0.0043\n","Batch 2000/5786, Loss: 0.0000\n","Batch 2100/5786, Loss: 0.0007\n","Batch 2200/5786, Loss: 0.0000\n","Batch 2300/5786, Loss: 0.8365\n","Batch 2400/5786, Loss: 0.0066\n","Batch 2500/5786, Loss: 0.0000\n","Batch 2600/5786, Loss: 0.1360\n","Batch 2700/5786, Loss: 0.0001\n","Batch 2800/5786, Loss: 1.0921\n","Batch 2900/5786, Loss: 0.0149\n","Batch 3000/5786, Loss: 0.0072\n","Batch 3100/5786, Loss: 0.0554\n","Batch 3200/5786, Loss: 0.0001\n","Batch 3300/5786, Loss: 0.0000\n","Batch 3400/5786, Loss: 0.0000\n","Batch 3500/5786, Loss: 0.0283\n","Batch 3600/5786, Loss: 0.0007\n","Batch 3700/5786, Loss: 0.0011\n","Batch 3800/5786, Loss: 0.0165\n","Batch 3900/5786, Loss: 0.0194\n","Batch 4000/5786, Loss: 0.0000\n","Batch 4100/5786, Loss: 0.0003\n","Batch 4200/5786, Loss: 0.0205\n","Batch 4300/5786, Loss: 0.0000\n","Batch 4400/5786, Loss: 0.0314\n","Batch 4500/5786, Loss: 0.0017\n","Batch 4600/5786, Loss: 0.0005\n","Batch 4700/5786, Loss: 0.0000\n","Batch 4800/5786, Loss: 0.0000\n","Batch 4900/5786, Loss: 0.0601\n","Batch 5000/5786, Loss: 0.1678\n","Batch 5100/5786, Loss: 0.0047\n","Batch 5200/5786, Loss: 0.0002\n","Batch 5300/5786, Loss: 0.1897\n","Batch 5400/5786, Loss: 0.0000\n","Batch 5500/5786, Loss: 0.0007\n","Batch 5600/5786, Loss: 0.0072\n","Batch 5700/5786, Loss: 0.0000\n","📈 Epoch 42/50, Average Loss: 0.0535\n","🎉 New best model! Loss: 0.0535\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 42...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 42 (loss: 0.0535)\n","\n","📊 Epoch 43/50\n","Batch 0/5786, Loss: 0.2440\n","Batch 100/5786, Loss: 0.0089\n","Batch 200/5786, Loss: 0.0000\n","Batch 300/5786, Loss: 0.0000\n","Batch 400/5786, Loss: 0.0001\n","Batch 500/5786, Loss: 0.0210\n","Batch 600/5786, Loss: 0.0169\n","Batch 700/5786, Loss: 0.0138\n","Batch 800/5786, Loss: 0.1355\n","Batch 900/5786, Loss: 0.0000\n","Batch 1000/5786, Loss: 0.0852\n","Batch 1100/5786, Loss: 0.0000\n","Batch 1200/5786, Loss: 0.0000\n","Batch 1300/5786, Loss: 0.0002\n","Batch 1400/5786, Loss: 0.0102\n","Batch 1500/5786, Loss: 0.0517\n","Batch 1600/5786, Loss: 0.0014\n","Batch 1700/5786, Loss: 0.0005\n","Batch 1800/5786, Loss: 0.0930\n","Batch 1900/5786, Loss: 0.0001\n","Batch 2000/5786, Loss: 0.1012\n","Batch 2100/5786, Loss: 0.0000\n","Batch 2200/5786, Loss: 0.0083\n","Batch 2300/5786, Loss: 0.0000\n","Batch 2400/5786, Loss: 0.0002\n","Batch 2500/5786, Loss: 0.6740\n","Batch 2600/5786, Loss: 0.0171\n","Batch 2700/5786, Loss: 0.0000\n","Batch 2800/5786, Loss: 0.0003\n","Batch 2900/5786, Loss: 0.0007\n","Batch 3000/5786, Loss: 0.0007\n","Batch 3100/5786, Loss: 0.0379\n","Batch 3200/5786, Loss: 0.0000\n","Batch 3300/5786, Loss: 0.0597\n","Batch 3400/5786, Loss: 0.0034\n","Batch 3500/5786, Loss: 0.1752\n","Batch 3600/5786, Loss: 0.0600\n","Batch 3700/5786, Loss: 0.0461\n","Batch 3800/5786, Loss: 0.0082\n","Batch 3900/5786, Loss: 0.0000\n","Batch 4000/5786, Loss: 0.0105\n","Batch 4100/5786, Loss: 0.0000\n","Batch 4200/5786, Loss: 0.0000\n","Batch 4300/5786, Loss: 0.0000\n","Batch 4400/5786, Loss: 0.0000\n","Batch 4500/5786, Loss: 0.0000\n","Batch 4600/5786, Loss: 0.0028\n","Batch 4700/5786, Loss: 0.0000\n","Batch 4800/5786, Loss: 0.5428\n","Batch 4900/5786, Loss: 0.0001\n","Batch 5000/5786, Loss: 0.0125\n","Batch 5100/5786, Loss: 0.1932\n","Batch 5200/5786, Loss: 0.0000\n","Batch 5300/5786, Loss: 0.0000\n","Batch 5400/5786, Loss: 0.0001\n","Batch 5500/5786, Loss: 0.0018\n","Batch 5600/5786, Loss: 0.0426\n","Batch 5700/5786, Loss: 0.0115\n","📈 Epoch 43/50, Average Loss: 0.0516\n","🎉 New best model! Loss: 0.0516\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 43...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 43 (loss: 0.0516)\n","\n","📊 Epoch 44/50\n","Batch 0/5786, Loss: 0.0002\n","Batch 100/5786, Loss: 0.0143\n","Batch 200/5786, Loss: 0.0005\n","Batch 300/5786, Loss: 0.8597\n","Batch 400/5786, Loss: 0.0143\n","Batch 500/5786, Loss: 0.3874\n","Batch 600/5786, Loss: 0.0000\n","Batch 700/5786, Loss: 0.0000\n","Batch 800/5786, Loss: 0.0001\n","Batch 900/5786, Loss: 0.5457\n","Batch 1000/5786, Loss: 0.0647\n","Batch 1100/5786, Loss: 0.0058\n","Batch 1200/5786, Loss: 0.0000\n","Batch 1300/5786, Loss: 0.0000\n","Batch 1400/5786, Loss: 0.0079\n","Batch 1500/5786, Loss: 0.0014\n","Batch 1600/5786, Loss: 0.1550\n","Batch 1700/5786, Loss: 0.0000\n","Batch 1800/5786, Loss: 0.0059\n","Batch 1900/5786, Loss: 0.0155\n","Batch 2000/5786, Loss: 0.0000\n","Batch 2100/5786, Loss: 0.0000\n","Batch 2200/5786, Loss: 0.0000\n","Batch 2300/5786, Loss: 0.5660\n","Batch 2400/5786, Loss: 0.0061\n","Batch 2500/5786, Loss: 0.0712\n","Batch 2600/5786, Loss: 0.0000\n","Batch 2700/5786, Loss: 0.0002\n","Batch 2800/5786, Loss: 0.0000\n","Batch 2900/5786, Loss: 0.0001\n","Batch 3000/5786, Loss: 0.0204\n","Batch 3100/5786, Loss: 0.0000\n","Batch 3200/5786, Loss: 0.0021\n","Batch 3300/5786, Loss: 0.0000\n","Batch 3400/5786, Loss: 0.0030\n","Batch 3500/5786, Loss: 0.0021\n","Batch 3600/5786, Loss: 0.1130\n","Batch 3700/5786, Loss: 0.0003\n","Batch 3800/5786, Loss: 0.0000\n","Batch 3900/5786, Loss: 0.0001\n","Batch 4000/5786, Loss: 0.0952\n","Batch 4100/5786, Loss: 0.0000\n","Batch 4200/5786, Loss: 0.0011\n","Batch 4300/5786, Loss: 0.0001\n","Batch 4400/5786, Loss: 0.0004\n","Batch 4500/5786, Loss: 0.0000\n","Batch 4600/5786, Loss: 0.0017\n","Batch 4700/5786, Loss: 0.0092\n","Batch 4800/5786, Loss: 0.0000\n","Batch 4900/5786, Loss: 0.0591\n","Batch 5000/5786, Loss: 0.2603\n","Batch 5100/5786, Loss: 0.6569\n","Batch 5200/5786, Loss: 0.0000\n","Batch 5300/5786, Loss: 0.0291\n","Batch 5400/5786, Loss: 0.0000\n","Batch 5500/5786, Loss: 0.0089\n","Batch 5600/5786, Loss: 0.3576\n","Batch 5700/5786, Loss: 0.0000\n","📈 Epoch 44/50, Average Loss: 0.0517\n","💾 Training state saved at epoch 44 (loss: 0.0517)\n","\n","📊 Epoch 45/50\n","Batch 0/5786, Loss: 0.0002\n","Batch 100/5786, Loss: 0.0004\n","Batch 200/5786, Loss: 0.0000\n","Batch 300/5786, Loss: 1.0093\n","Batch 400/5786, Loss: 0.0041\n","Batch 500/5786, Loss: 0.0000\n","Batch 600/5786, Loss: 0.0024\n","Batch 700/5786, Loss: 0.0181\n","Batch 800/5786, Loss: 0.0000\n","Batch 900/5786, Loss: 0.0532\n","Batch 1000/5786, Loss: 0.0000\n","Batch 1100/5786, Loss: 0.0004\n","Batch 1200/5786, Loss: 0.1540\n","Batch 1300/5786, Loss: 0.0000\n","Batch 1400/5786, Loss: 0.0000\n","Batch 1500/5786, Loss: 0.0005\n","Batch 1600/5786, Loss: 0.0000\n","Batch 1700/5786, Loss: 0.0000\n","Batch 1800/5786, Loss: 0.2478\n","Batch 1900/5786, Loss: 0.0071\n","Batch 2000/5786, Loss: 0.0051\n","Batch 2100/5786, Loss: 0.0000\n","Batch 2200/5786, Loss: 0.0003\n","Batch 2300/5786, Loss: 0.0001\n","Batch 2400/5786, Loss: 0.1434\n","Batch 2500/5786, Loss: 0.0000\n","Batch 2600/5786, Loss: 0.0000\n","Batch 2700/5786, Loss: 0.0017\n","Batch 2800/5786, Loss: 0.0140\n","Batch 2900/5786, Loss: 0.0029\n","Batch 3000/5786, Loss: 0.0007\n","Batch 3100/5786, Loss: 0.0000\n","Batch 3200/5786, Loss: 0.0000\n","Batch 3300/5786, Loss: 0.0023\n","Batch 3400/5786, Loss: 0.0018\n","Batch 3500/5786, Loss: 0.0001\n","Batch 3600/5786, Loss: 0.0016\n","Batch 3700/5786, Loss: 0.0024\n","Batch 3800/5786, Loss: 0.0617\n","Batch 3900/5786, Loss: 0.0000\n","Batch 4000/5786, Loss: 0.0002\n","Batch 4100/5786, Loss: 0.1753\n","Batch 4200/5786, Loss: 0.0006\n","Batch 4300/5786, Loss: 0.0009\n","Batch 4400/5786, Loss: 0.0000\n","Batch 4500/5786, Loss: 0.0006\n","Batch 4600/5786, Loss: 0.0000\n","Batch 4700/5786, Loss: 0.0002\n","Batch 4800/5786, Loss: 0.0003\n","Batch 4900/5786, Loss: 0.2374\n","Batch 5000/5786, Loss: 0.0003\n","Batch 5100/5786, Loss: 0.0000\n","Batch 5200/5786, Loss: 0.4552\n","Batch 5300/5786, Loss: 0.0451\n","Batch 5400/5786, Loss: 0.0000\n","Batch 5500/5786, Loss: 0.0010\n","Batch 5600/5786, Loss: 0.0003\n","Batch 5700/5786, Loss: 0.0069\n","📈 Epoch 45/50, Average Loss: 0.0502\n","🎉 New best model! Loss: 0.0502\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 45...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 45 (loss: 0.0502)\n","\n","📊 Epoch 46/50\n","Batch 0/5786, Loss: 0.0000\n","Batch 100/5786, Loss: 0.0000\n","Batch 200/5786, Loss: 0.0000\n","Batch 300/5786, Loss: 0.0001\n","Batch 400/5786, Loss: 0.0005\n","Batch 500/5786, Loss: 0.0000\n","Batch 600/5786, Loss: 0.1994\n","Batch 700/5786, Loss: 0.5443\n","Batch 800/5786, Loss: 0.0006\n","Batch 900/5786, Loss: 0.0000\n","Batch 1000/5786, Loss: 0.0053\n","Batch 1100/5786, Loss: 0.3097\n","Batch 1200/5786, Loss: 0.0004\n","Batch 1300/5786, Loss: 0.0029\n","Batch 1400/5786, Loss: 0.0436\n","Batch 1500/5786, Loss: 0.2554\n","Batch 1600/5786, Loss: 0.0001\n","Batch 1700/5786, Loss: 0.0114\n","Batch 1800/5786, Loss: 0.0000\n","Batch 1900/5786, Loss: 0.0009\n","Batch 2000/5786, Loss: 0.5792\n","Batch 2100/5786, Loss: 0.0003\n","Batch 2200/5786, Loss: 0.0394\n","Batch 2300/5786, Loss: 0.0024\n","Batch 2400/5786, Loss: 0.0004\n","Batch 2500/5786, Loss: 0.0023\n","Batch 2600/5786, Loss: 0.1777\n","Batch 2700/5786, Loss: 0.0033\n","Batch 2800/5786, Loss: 0.0000\n","Batch 2900/5786, Loss: 0.0148\n","Batch 3000/5786, Loss: 0.0000\n","Batch 3100/5786, Loss: 0.0095\n","Batch 3200/5786, Loss: 0.0002\n","Batch 3300/5786, Loss: 0.0827\n","Batch 3400/5786, Loss: 0.3172\n","Batch 3500/5786, Loss: 0.0006\n","Batch 3600/5786, Loss: 0.1248\n","Batch 3700/5786, Loss: 0.0000\n","Batch 3800/5786, Loss: 0.0000\n","Batch 3900/5786, Loss: 0.0000\n","Batch 4000/5786, Loss: 0.0007\n","Batch 4100/5786, Loss: 0.0000\n","Batch 4200/5786, Loss: 0.0941\n","Batch 4300/5786, Loss: 0.1160\n","Batch 4400/5786, Loss: 0.0434\n","Batch 4500/5786, Loss: 0.2096\n","Batch 4600/5786, Loss: 0.0000\n","Batch 4700/5786, Loss: 0.0777\n","Batch 4800/5786, Loss: 0.0001\n","Batch 4900/5786, Loss: 0.0001\n","Batch 5000/5786, Loss: 0.0068\n","Batch 5100/5786, Loss: 0.0005\n","Batch 5200/5786, Loss: 0.0001\n","Batch 5300/5786, Loss: 0.0000\n","Batch 5400/5786, Loss: 0.0424\n","Batch 5500/5786, Loss: 0.0000\n","Batch 5600/5786, Loss: 0.0157\n","Batch 5700/5786, Loss: 0.0000\n","📈 Epoch 46/50, Average Loss: 0.0455\n","🎉 New best model! Loss: 0.0455\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 46...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 46 (loss: 0.0455)\n","\n","📊 Epoch 47/50\n","Batch 0/5786, Loss: 0.0000\n","Batch 100/5786, Loss: 0.0000\n","Batch 200/5786, Loss: 0.0032\n","Batch 300/5786, Loss: 0.0000\n","Batch 400/5786, Loss: 0.0029\n","Batch 500/5786, Loss: 0.0009\n","Batch 600/5786, Loss: 0.0590\n","Batch 700/5786, Loss: 0.0003\n","Batch 800/5786, Loss: 0.0000\n","Batch 900/5786, Loss: 0.0000\n","Batch 1000/5786, Loss: 0.0008\n","Batch 1100/5786, Loss: 0.0006\n","Batch 1200/5786, Loss: 0.0029\n","Batch 1300/5786, Loss: 0.0001\n","Batch 1400/5786, Loss: 0.0004\n","Batch 1500/5786, Loss: 0.0000\n","Batch 1600/5786, Loss: 0.0014\n","Batch 1700/5786, Loss: 0.0000\n","Batch 1800/5786, Loss: 0.0028\n","Batch 1900/5786, Loss: 0.0030\n","Batch 2000/5786, Loss: 0.0000\n","Batch 2100/5786, Loss: 0.0000\n","Batch 2200/5786, Loss: 0.0000\n","Batch 2300/5786, Loss: 0.0001\n","Batch 2400/5786, Loss: 0.0002\n","Batch 2500/5786, Loss: 0.0010\n","Batch 2600/5786, Loss: 0.0000\n","Batch 2700/5786, Loss: 0.0000\n","Batch 2800/5786, Loss: 0.0002\n","Batch 2900/5786, Loss: 0.0000\n","Batch 3000/5786, Loss: 0.0044\n","Batch 3100/5786, Loss: 0.0000\n","Batch 3200/5786, Loss: 0.0000\n","Batch 3300/5786, Loss: 0.0286\n","Batch 3400/5786, Loss: 0.8284\n","Batch 3500/5786, Loss: 0.0698\n","Batch 3600/5786, Loss: 0.0000\n","Batch 3700/5786, Loss: 0.0006\n","Batch 3800/5786, Loss: 0.0000\n","Batch 3900/5786, Loss: 0.0013\n","Batch 4000/5786, Loss: 0.0021\n","Batch 4100/5786, Loss: 0.0000\n","Batch 4200/5786, Loss: 0.0005\n","Batch 4300/5786, Loss: 0.0025\n","Batch 4400/5786, Loss: 0.0000\n","Batch 4500/5786, Loss: 0.0107\n","Batch 4600/5786, Loss: 0.0000\n","Batch 4700/5786, Loss: 0.0001\n","Batch 4800/5786, Loss: 0.0000\n","Batch 4900/5786, Loss: 0.0017\n","Batch 5000/5786, Loss: 0.0000\n","Batch 5100/5786, Loss: 0.0000\n","Batch 5200/5786, Loss: 0.0000\n","Batch 5300/5786, Loss: 0.0000\n","Batch 5400/5786, Loss: 0.0294\n","Batch 5500/5786, Loss: 0.2780\n","Batch 5600/5786, Loss: 0.0286\n","Batch 5700/5786, Loss: 0.0411\n","📈 Epoch 47/50, Average Loss: 0.0431\n","🎉 New best model! Loss: 0.0431\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 47...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 47 (loss: 0.0431)\n","\n","📊 Epoch 48/50\n","Batch 0/5786, Loss: 0.0000\n","Batch 100/5786, Loss: 0.0000\n","Batch 200/5786, Loss: 0.0018\n","Batch 300/5786, Loss: 0.0292\n","Batch 400/5786, Loss: 0.0207\n","Batch 500/5786, Loss: 0.0000\n","Batch 600/5786, Loss: 0.0000\n","Batch 700/5786, Loss: 0.0001\n","Batch 800/5786, Loss: 0.0044\n","Batch 900/5786, Loss: 0.0000\n","Batch 1000/5786, Loss: 0.0044\n","Batch 1100/5786, Loss: 0.4675\n","Batch 1200/5786, Loss: 0.9931\n","Batch 1300/5786, Loss: 0.2283\n","Batch 1400/5786, Loss: 0.0038\n","Batch 1500/5786, Loss: 0.0010\n","Batch 1600/5786, Loss: 0.0081\n","Batch 1700/5786, Loss: 0.0017\n","Batch 1800/5786, Loss: 0.0109\n","Batch 1900/5786, Loss: 0.0002\n","Batch 2000/5786, Loss: 0.2569\n","Batch 2100/5786, Loss: 0.0051\n","Batch 2200/5786, Loss: 0.0000\n","Batch 2300/5786, Loss: 0.0211\n","Batch 2400/5786, Loss: 0.0000\n","Batch 2500/5786, Loss: 0.0000\n","Batch 2600/5786, Loss: 0.0001\n","Batch 2700/5786, Loss: 0.0003\n","Batch 2800/5786, Loss: 0.0000\n","Batch 2900/5786, Loss: 0.0021\n","Batch 3000/5786, Loss: 0.1293\n","Batch 3100/5786, Loss: 0.0007\n","Batch 3200/5786, Loss: 0.0008\n","Batch 3300/5786, Loss: 0.0000\n","Batch 3400/5786, Loss: 0.0010\n","Batch 3500/5786, Loss: 0.0000\n","Batch 3600/5786, Loss: 0.0078\n","Batch 3700/5786, Loss: 0.0001\n","Batch 3800/5786, Loss: 0.0000\n","Batch 3900/5786, Loss: 0.0003\n","Batch 4000/5786, Loss: 0.2597\n","Batch 4100/5786, Loss: 0.0000\n","Batch 4200/5786, Loss: 0.0000\n","Batch 4300/5786, Loss: 0.0000\n","Batch 4400/5786, Loss: 0.0020\n","Batch 4500/5786, Loss: 0.0620\n","Batch 4600/5786, Loss: 0.1050\n","Batch 4700/5786, Loss: 0.3542\n","Batch 4800/5786, Loss: 0.0144\n","Batch 4900/5786, Loss: 0.0003\n","Batch 5000/5786, Loss: 0.0003\n","Batch 5100/5786, Loss: 1.1446\n","Batch 5200/5786, Loss: 0.0000\n","Batch 5300/5786, Loss: 0.0000\n","Batch 5400/5786, Loss: 0.0002\n","Batch 5500/5786, Loss: 0.0000\n","Batch 5600/5786, Loss: 0.0010\n","Batch 5700/5786, Loss: 0.0005\n","📈 Epoch 48/50, Average Loss: 0.0423\n","🎉 New best model! Loss: 0.0423\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 48...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 48 (loss: 0.0423)\n","\n","📊 Epoch 49/50\n","Batch 0/5786, Loss: 0.0000\n","Batch 100/5786, Loss: 0.0052\n","Batch 200/5786, Loss: 0.0035\n","Batch 300/5786, Loss: 0.0000\n","Batch 400/5786, Loss: 0.0447\n","Batch 500/5786, Loss: 0.0000\n","Batch 600/5786, Loss: 0.0002\n","Batch 700/5786, Loss: 0.0009\n","Batch 800/5786, Loss: 0.0000\n","Batch 900/5786, Loss: 0.0000\n","Batch 1000/5786, Loss: 0.0000\n","Batch 1100/5786, Loss: 0.0012\n","Batch 1200/5786, Loss: 0.0000\n","Batch 1300/5786, Loss: 0.0000\n","Batch 1400/5786, Loss: 0.8190\n","Batch 1500/5786, Loss: 0.0000\n","Batch 1600/5786, Loss: 0.0000\n","Batch 1700/5786, Loss: 0.1194\n","Batch 1800/5786, Loss: 0.0000\n","Batch 1900/5786, Loss: 0.0009\n","Batch 2000/5786, Loss: 0.0001\n","Batch 2100/5786, Loss: 0.0014\n","Batch 2200/5786, Loss: 0.2133\n","Batch 2300/5786, Loss: 0.0000\n","Batch 2400/5786, Loss: 0.0032\n","Batch 2500/5786, Loss: 0.0156\n","Batch 2600/5786, Loss: 0.6870\n","Batch 2700/5786, Loss: 0.0020\n","Batch 2800/5786, Loss: 0.0013\n","Batch 2900/5786, Loss: 0.6541\n","Batch 3000/5786, Loss: 0.0077\n","Batch 3100/5786, Loss: 0.0003\n","Batch 3200/5786, Loss: 0.0001\n","Batch 3300/5786, Loss: 0.0002\n","Batch 3400/5786, Loss: 0.0000\n","Batch 3500/5786, Loss: 0.0022\n","Batch 3600/5786, Loss: 0.1238\n","Batch 3700/5786, Loss: 0.2603\n","Batch 3800/5786, Loss: 0.1415\n","Batch 3900/5786, Loss: 0.0342\n","Batch 4000/5786, Loss: 0.0001\n","Batch 4100/5786, Loss: 0.1495\n","Batch 4200/5786, Loss: 0.0003\n","Batch 4300/5786, Loss: 0.0446\n","Batch 4400/5786, Loss: 0.0069\n","Batch 4500/5786, Loss: 0.0008\n","Batch 4600/5786, Loss: 0.0000\n","Batch 4700/5786, Loss: 0.0026\n","Batch 4800/5786, Loss: 0.2788\n","Batch 4900/5786, Loss: 0.0439\n","Batch 5000/5786, Loss: 0.0006\n","Batch 5100/5786, Loss: 0.0000\n","Batch 5200/5786, Loss: 0.0000\n","Batch 5300/5786, Loss: 0.0089\n","Batch 5400/5786, Loss: 0.0000\n","Batch 5500/5786, Loss: 0.0001\n","Batch 5600/5786, Loss: 0.0000\n","Batch 5700/5786, Loss: 0.1871\n","📈 Epoch 49/50, Average Loss: 0.0425\n","💾 Training state saved at epoch 49 (loss: 0.0425)\n","\n","📊 Epoch 50/50\n","Batch 0/5786, Loss: 0.0003\n","Batch 100/5786, Loss: 0.0000\n","Batch 200/5786, Loss: 0.0554\n","Batch 300/5786, Loss: 0.0135\n","Batch 400/5786, Loss: 0.0000\n","Batch 500/5786, Loss: 0.0000\n","Batch 600/5786, Loss: 0.0040\n","Batch 700/5786, Loss: 0.0006\n","Batch 800/5786, Loss: 0.0078\n","Batch 900/5786, Loss: 0.0000\n","Batch 1000/5786, Loss: 0.0035\n","Batch 1100/5786, Loss: 0.0006\n","Batch 1200/5786, Loss: 0.0000\n","Batch 1300/5786, Loss: 0.0063\n","Batch 1400/5786, Loss: 0.0000\n","Batch 1500/5786, Loss: 0.0000\n","Batch 1600/5786, Loss: 0.0007\n","Batch 1700/5786, Loss: 0.0333\n","Batch 1800/5786, Loss: 0.0000\n","Batch 1900/5786, Loss: 0.0000\n","Batch 2000/5786, Loss: 0.0212\n","Batch 2100/5786, Loss: 0.0094\n","Batch 2200/5786, Loss: 0.0795\n","Batch 2300/5786, Loss: 0.0338\n","Batch 2400/5786, Loss: 0.0032\n","Batch 2500/5786, Loss: 0.0000\n","Batch 2600/5786, Loss: 0.0000\n","Batch 2700/5786, Loss: 0.0000\n","Batch 2800/5786, Loss: 0.0002\n","Batch 2900/5786, Loss: 0.0003\n","Batch 3000/5786, Loss: 0.0000\n","Batch 3100/5786, Loss: 0.0001\n","Batch 3200/5786, Loss: 0.0121\n","Batch 3300/5786, Loss: 0.3404\n","Batch 3400/5786, Loss: 0.0000\n","Batch 3500/5786, Loss: 0.0000\n","Batch 3600/5786, Loss: 0.0000\n","Batch 3700/5786, Loss: 0.0000\n","Batch 3800/5786, Loss: 0.0023\n","Batch 3900/5786, Loss: 0.0003\n","Batch 4000/5786, Loss: 0.0009\n","Batch 4100/5786, Loss: 0.1170\n","Batch 4200/5786, Loss: 0.0000\n","Batch 4300/5786, Loss: 0.0000\n","Batch 4400/5786, Loss: 0.0012\n","Batch 4500/5786, Loss: 0.0001\n","Batch 4600/5786, Loss: 0.0001\n","Batch 4700/5786, Loss: 0.0008\n","Batch 4800/5786, Loss: 0.0002\n","Batch 4900/5786, Loss: 0.0000\n","Batch 5000/5786, Loss: 0.0561\n","Batch 5100/5786, Loss: 0.1205\n","Batch 5200/5786, Loss: 0.0002\n","Batch 5300/5786, Loss: 0.0002\n","Batch 5400/5786, Loss: 0.0000\n","Batch 5500/5786, Loss: 0.0003\n","Batch 5600/5786, Loss: 0.0000\n","Batch 5700/5786, Loss: 0.0005\n","📈 Epoch 50/50, Average Loss: 0.0376\n","🎉 New best model! Loss: 0.0376\n","✅ Best model saved to /content/drive/MyDrive/MuseGuard/best_unified_model.pth\n","🔄 Building and saving reference base at epoch 50...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","💾 Training state saved at epoch 50 (loss: 0.0376)\n","\n","🎊 Training completed!\n","📊 Total epochs: 50\n","🏆 Best loss: 0.0376\n","💾 Final model saved to /content/drive/MyDrive/MuseGuard/final_unified_plagiarism_model.pth\n","🔄 Building final reference base...\n","Built reference index with 323 pieces\n","Reference base saved to /content/drive/MyDrive/MuseGuard/music_reference_base.pkl\n","✅ Reference base built and saved with 323 pieces\n","\n","🎉 === Training completed successfully! ===\n","📁 All files saved to: /content/drive/MyDrive/MuseGuard\n","📄 Files available:\n","- final_unified_plagiarism_model.pth (final trained model)\n","- best_unified_model.pth (best performing model)\n","- latest_unified_model.pth (latest model state)\n","- music_reference_base.pkl (reference database)\n","- file_validation_cache.pkl (validation cache for future runs)\n","- checkpoint_epoch_X.pth (checkpoint files)\n","\n","💡 Benefits of live saving:\n","  ✅ No data loss from internet interruptions\n","  ✅ Can resume training from any point\n","  ✅ Reference base updated during training\n","  ✅ Multiple backup checkpoints available\n","  ✅ Next run will be even faster thanks to cache!\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}